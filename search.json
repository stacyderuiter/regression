[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GLMs and Friends",
    "section": "",
    "text": "Preface\nThis is a collection of notes on multiple regression models, generalized linear models, mixed-effects models, generalized estimating equations, and generalized additive models. They were originally developed for teaching the courses STAT 245 and DATA 545 at Calvin University.\nThey are a work in progress. If you have questions, please reach out."
  },
  {
    "objectID": "lm1.html#data",
    "href": "lm1.html#data",
    "title": "1  Linear Regression",
    "section": "1.1 Data",
    "text": "1.1 Data\nWe will consider a small dataset from an article by J.S. Martin and colleagues, titled Facial width-to-height ratio is associated with agonistic and affiliative dominance in bonobos (Pan paniscus)\nNotes: variable fWHR is the facial width-height ratio and AssR is the Assertiveness score of affiliative dominance. normDS is another dominance score.\nWhat figures should we consider, to get a sense of the dataset and think about how we might model fWHR?\n\n\nRows: 117\nColumns: 8\n$ Name   &lt;chr&gt; \"Zuani\", \"Zuani\", \"Zorba\", \"Zorba\", \"Zorba\", \"Zomi\", \"Zomi\", \"Z…\n$ Group  &lt;chr&gt; \"Apenheul\", \"Apenheul\", \"Wilhelma\", \"Wilhelma\", \"Wilhelma\", \"Fr…\n$ Sex    &lt;chr&gt; \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Female\",…\n$ Age    &lt;dbl&gt; 22, 22, 34, 34, 34, 15, 15, 14, 14, 14, 18, 18, 18, 18, 18, 12,…\n$ fWHR   &lt;dbl&gt; 1.475052, 1.321814, 1.581446, 1.479237, 1.390086, 1.340909, 1.2…\n$ AssR   &lt;dbl&gt; 5.36, 5.36, 2.36, 2.36, 2.36, 3.92, 3.92, 4.74, 4.74, 4.74, 2.6…\n$ normDS &lt;dbl&gt; 1.430, 1.430, 2.341, 2.341, 2.341, 3.087, 3.087, 3.035, 3.035, …\n$ weight &lt;dbl&gt; 24.0, 24.0, NA, NA, NA, NA, NA, 41.6, 41.6, 41.6, 38.0, 38.0, 3…"
  },
  {
    "objectID": "lm1.html#preliminary-considerations",
    "href": "lm1.html#preliminary-considerations",
    "title": "1  Linear Regression",
    "section": "1.2 Preliminary Considerations",
    "text": "1.2 Preliminary Considerations\nJust as we imagine before we start coding to create graphics, we ought to think before we start fitting models.\nTraditional ways of interpreting statistical results are premised on the idea that you made a plan, got some data, fitted the model you planned, and want to draw conclusions.\nIf, instead, you got data, scrutinized the data, fitted lots of different models, and now want to report results from the one that fitted best…well, generally things tend to go wrong. This is especially true if you use the data to lead you from a more complex to a simpler model. As Harrell (2015) points out in section 4.3,\n\nUncertainty underestimated (overconfidence: standard errors and confidence intervals too small; \\(R^2\\) too big)\nSpurious relationships look important and slope estimates are biased high\nIf testing hypotheses, p-values too small\n\nHow can we avoid these problems? Some more insight will come when we consider model assessment and selection in future sections. For now, we need to remember:\n\nFitting and interpreting one well-considered, sensible model is prefereable to trying many things and then trying to choose among them later."
  },
  {
    "objectID": "lm1.html#response-and-predictors",
    "href": "lm1.html#response-and-predictors",
    "title": "1  Linear Regression",
    "section": "1.3 Response and Predictors",
    "text": "1.3 Response and Predictors\nA regression model is our attempt to quantify how a response variable of interest changes when a set of predictor variables change.\nSo, to begin, we need to identify our (one) response variable – the thing we are most interested in measuring or predicting or describing or understanding.\nThen, we need to identify a set of predictor variables that we expect to be associated with changes in the response. (If we are planning an experiment, they should be variables we can collect data on; if working with data already collected, they must be in or derived from the data available.)\nHow do we choose which predictors to include, and how many?\nFirst, rely on experts and previous experience. If you know the context of the problem well, you have a good sense of the predictors that will be of interest. If you don’t, then you should consult experts (or published work on the topic).\nThere are also practical limits on the number of predictors you can reasonably consider, given a dataset.\n\n1.3.1 Sample size and predictors\nOne important consideration, when planning a regression model, is: How many predictors can I reasonably include?\nIt depends on the size of the dataset: it takes several observations to get a good estimate of any statistics, so it makes sense that fitting a model with lots of predictors will require a bigger dataset. And if you try to fit too many, the chances of overfitting increase. Overfitting is when you model noise as well as signal, capturing in your model apparent relationships that actually exist only in the current dataset, not in reality.\nFor linear regression, Harrell (2015, Chapter 4.6) offers a rule of thumb: the number of parameters being estimated, \\(p\\), should be less than \\(\\frac{n}{10}\\) or \\(\\frac{n}{20}\\). To give just one standard rule of thumb, we should aim for \\(p &lt; \\frac{n}{15}\\). \\(n\\) is the sample size (number of rows in the dataset)."
  },
  {
    "objectID": "lm1.html#simple-linear-regression-residuals-least-squares",
    "href": "lm1.html#simple-linear-regression-residuals-least-squares",
    "title": "1  Linear Regression",
    "section": "1.4 Simple linear regression, Residuals & Least squares",
    "text": "1.4 Simple linear regression, Residuals & Least squares\nFirst, let’s review and consider a simple (one-predictor) linear regression model. Fit the model\n\nslr &lt;- lm(fWHR ~ AssR, data=bonobos)\n\nExtract the slope and intercept values:\n\ncoef(slr)\n\n(Intercept)        AssR \n 1.30685287  0.02918242 \n\n\nAdd the regression line to the plot:\n\ngf_point(fWHR ~ AssR, data=bonobos) |&gt; \n  gf_lm()\n\n\n\nsummary(slr)\n\n\nCall:\nlm(formula = fWHR ~ AssR, data = bonobos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31320 -0.11369 -0.01242  0.09008  0.49241 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.30685    0.06283  20.801   &lt;2e-16 ***\nAssR         0.02918    0.01420   2.055   0.0421 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1689 on 115 degrees of freedom\nMultiple R-squared:  0.03542,   Adjusted R-squared:  0.02704 \nF-statistic: 4.223 on 1 and 115 DF,  p-value: 0.04213\n\n\n\n1.4.1 Using lm() to fit a linear regression in R\nWe use function lm() with a formula of the form y ~ x (and an input data = _____).\nTo view the result, we ask for a summary().\n\nslr &lt;- lm(fWHR ~ AssR, data = bonobos)\nsummary(slr)\n\n\nCall:\nlm(formula = fWHR ~ AssR, data = bonobos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31320 -0.11369 -0.01242  0.09008  0.49241 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.30685    0.06283  20.801   &lt;2e-16 ***\nAssR         0.02918    0.01420   2.055   0.0421 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1689 on 115 degrees of freedom\nMultiple R-squared:  0.03542,   Adjusted R-squared:  0.02704 \nF-statistic: 4.223 on 1 and 115 DF,  p-value: 0.04213\n\n\nFrom this traditional on-screen output, we can fetch the information and estimates we need.\nWe can also “clean up” the output into a data table using broom::tidy():\n\nbroom::tidy(slr)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   1.31      0.0628     20.8  8.83e-41\n2 AssR          0.0292    0.0142      2.06 4.21e- 2\n\n\n\n\n1.4.2 Equation of the fitted regression line\nIn the process above, we chose a response variable \\(y\\), a predictor \\(x\\), and estimated the slope and intercept of the line \\(\\beta_0\\) and \\(\\beta_1\\).\n\\[ y = \\beta_0 + \\beta_1 x\\]\nBut the data don’t lie perfectly on the line, and we have too include that in our model, too. There is some error \\(\\epsilon\\) in our model for the data, so we should keep track of it.\n\\[ y = \\beta_0 + \\beta_1 x + \\epsilon\\]\nBut \\(\\epsilon\\) is not one number to be estimated – the amount of error is different for every data point! How can we express this mathematically?\n\n\n1.4.3 Regression residuals = “errors”\nFirst, we need a way to measure how far the model line is from the data, for every observation in the dataset. The residual for the ith observation is the observed response variable value from the data (\\(y_i\\)) minus the predicted response value on the regression line (\\(\\hat{y}_i\\)). This will be the response variable value for the point on the line whose predictor (\\(x\\) value is the same as for the observed data point). These “residuals” are shown in red on the figure. (The numeric value of each residual is the length of its red line segment.)\n\n\n\n\n\nHow can we summarize this information? Let’s look at the distribution of the residuals:\n\ngf_histogram(~resid(slr), bins = 15)\n\n\n\n\nHmmm…looks pretty unimodal, and symmetric (except for a bit of right skew). It almost has that familiar bell shape…like a Normal distribution. This is a hint.\nIn linear regression, we model residuals with a normal distribution, with mean zero (the line should go through the “middle” of the points, and averaging over them all, our residual should be 0).\nThe standard deviation of the residuals depends on how far away the points are, on average, from the line. This value, the residual standard deviation \\(\\sigma\\), must be estimated in each case.\nR lm() summary output calls it “residual standard error” and lists it near the bottom of the summary. (This is a somewhat inaccurate name but we’re stuck with it.)\nSo now we can give a complete model equation. In general,\n\\[ y = \\beta_0 + \\beta_1 x + \\epsilon,\\] where\n\\[ \\epsilon \\sim N(0, \\sigma)\\]\nOr, in our specific case with the Bonobos,\n\\[ y = 1.31 + 0.029 x + \\epsilon, \\text{ where } \\epsilon \\sim N(0, 0.169)\\]"
  },
  {
    "objectID": "lm1.html#multiple-regression",
    "href": "lm1.html#multiple-regression",
    "title": "1  Linear Regression",
    "section": "1.5 Multiple regression",
    "text": "1.5 Multiple regression\nRarely does our response variable really depend on only one predictor. Can we expand our formulation to include more predictors? In R, it’s super easy:\n\nmlr &lt;- lm(fWHR ~ AssR + weight, data=bonobos)\ncoef(mlr)\n\n(Intercept)        AssR      weight \n0.944790930 0.039888045 0.008644299 \n\n\n\n\n\n\n\n\n\n\n\n1.5.1 Choosing predictors\nNotice that in this chapter, we are building up from a simpler model to a more complex one with more predictors. This is to reveal to you the mathematical machinery that lets us specify and fit the more complex models.\nBut this is not how to build a model in practice. Recall, we agreed to think carefully and choose a reasonable set of predictors to use at the outset, doing as much of the work as possible before even looking at the dataset. Our \\(p &lt; \\frac{n}{15}\\) rule gives a rough limit to the maximum number of parameters we can estimate. If we do otherwise, we have to be cautious and know all our conclusions are on much shakier ground.\n\n\n1.5.2 Estimation\nBut…how did R come up with those estimates of the slope and intercept (and residual standard deviation) parameters?\nIn the simple linear regression case, it’s easy to visualize:\n\n\n\n\n\nThe best line is the one that makes the residuals the smallest (by going right through the middle of the points). OK, let’s be careful: some residuals are positive and some are negative, and we want the line that minimizes their magnitude. The traditional approach is to choose the slope that minimizes the sum of the squared residuals,\n\\[ SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nHow does this work when we have multiple predictors? It’s harder to draw, but just as easy to compute \\(\\hat{y}\\) and thus the observed residuals \\(e_i\\).\n\n\n1.5.3 Computing Predictions\nUse the regression equation to compute predicted values for the three data points below:\n\n\n# A tibble: 4 × 3\n   fWHR  AssR weight\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  1.88  4.74   41.6\n2  1.80  5.38   50.6\n3  1.59  3.97   NA  \n4  1.55  4.87   38.5"
  },
  {
    "objectID": "lm1.html#predictors-with-two-categories",
    "href": "lm1.html#predictors-with-two-categories",
    "title": "1  Linear Regression",
    "section": "1.6 Predictors with two categories",
    "text": "1.6 Predictors with two categories\n\n\n\n\n\n\nmlr2 &lt;- lm(fWHR ~ AssR + weight + Sex, data = bonobos)\ncoef(mlr2)\n\n(Intercept)        AssR      weight     SexMale \n1.065420976 0.058435841 0.002257142 0.128484275 \n\n\nHow does the model incorporate this covariate mathematically?\n   \n\n1.6.1 Predictors with more categories\n\ngf_boxplot(fWHR ~ Group, data = bonobos)\n\n\n\nmlr3 &lt;- lm(fWHR ~ AssR + weight + Sex + Group, data = bonobos)\ncoef(mlr3)\n\n      (Intercept)              AssR            weight           SexMale \n      1.007734691       0.064361973       0.003458979       0.124854271 \n   GroupFrankfurt GroupPlanckendael     GroupTwycross     GroupWilhelma \n      0.037426358      -0.008464572      -0.112907589       0.011186724 \n   GroupWuppertal \n     -0.004364826 \n\n\nHow does the model incorporate this covariate mathematically?"
  },
  {
    "objectID": "lm1.html#returning-to-the-r-model-summary",
    "href": "lm1.html#returning-to-the-r-model-summary",
    "title": "1  Linear Regression",
    "section": "1.7 Returning to the R Model Summary",
    "text": "1.7 Returning to the R Model Summary\nThere are several bits of information you should be able to extract from the summary() output R produces on a fitted linear regression model:\n\n\\(\\beta\\)s, Coefficient Estimates\n\\(\\sigma\\), labeled “residual standard error”    \n\\(R^2\\) (adjusted)\n\n   \n\nmlr3 &lt;- lm(fWHR ~ AssR + weight + Sex + Group, data = bonobos)\nsummary(mlr3)\n\n\nCall:\nlm(formula = fWHR ~ AssR + weight + Sex + Group, data = bonobos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.38288 -0.09488 -0.02642  0.07196  0.48464 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.007735   0.217585   4.631 2.05e-05 ***\nAssR               0.064362   0.021158   3.042   0.0035 ** \nweight             0.003459   0.005547   0.624   0.5353    \nSexMale            0.124854   0.059278   2.106   0.0394 *  \nGroupFrankfurt     0.037426   0.074892   0.500   0.6191    \nGroupPlanckendael -0.008465   0.075407  -0.112   0.9110    \nGroupTwycross     -0.112908   0.074779  -1.510   0.1364    \nGroupWilhelma      0.011187   0.085538   0.131   0.8964    \nGroupWuppertal    -0.004365   0.071292  -0.061   0.9514    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1691 on 59 degrees of freedom\n  (49 observations deleted due to missingness)\nMultiple R-squared:  0.2517,    Adjusted R-squared:  0.1502 \nF-statistic:  2.48 on 8 and 59 DF,  p-value: 0.02167"
  },
  {
    "objectID": "lm1.html#predictions-from-the-model",
    "href": "lm1.html#predictions-from-the-model",
    "title": "1  Linear Regression",
    "section": "1.8 Predictions from the model",
    "text": "1.8 Predictions from the model\n\n1.8.1 By Hand\nThe equation for the fitted model above is:\n\\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3I_{Male} + \\beta_4I_{Frankfurt} + \\beta_5I_{Planckendael} + \\beta_6I_{Twycross} + \\beta_7I_{Wilhelma} + \\beta_7I_{Wuppertal} + \\epsilon\\]\nwhere\n\n\\(y =\\)\n\\(\\beta_0=\\)\n\n   \n\n\\(x_1=\\)\n\\(x_2=\\)\n\\(\\beta_1, \\beta_2, \\beta_3 ...\\) are:\n\\(I_{Male} =\\)\n\\(I_{Frankfurt} =\\)\n\\(I_{Planckendael} =\\) , etc.\n\\(\\epsilon=\\)\n\n\n1.8.1.1 Comprehension check:\nWhat is the expected fWHR (according to this model) for a 30 kg female bonobo at the Wilhelma zoo?\n   \n\n\n\n1.8.2 Prediction Plots in R\nWe can ask R to compute predictions for all the data points in the real dataset.\n\nbonobos &lt;- bonobos |&gt; \n  mutate(preds = predict(mlr3))\n\nError in `mutate()`:\nℹ In argument: `preds = predict(mlr3)`.\nCaused by error:\n! `preds` must be size 117 or 1, not 68.\n\n\nWait, what? This error is because the lm() function removes rows containing missing values from the dataset, so it computes only 68 residuals (for the complete cases in the data). This doesn’t match the 117 rows in the original data. We can solve the problem by omitting rows with missing values first. To be safe, we first select only the variables we need, so we don’t omit rows based on missing values in unused variables.\n\nb2 &lt;- bonobos |&gt;\n  dplyr::select(fWHR, weight, AssR, Sex, Group) |&gt;\n  na.omit() |&gt;\n  mutate(preds = predict(mlr3))\n\nWe have a full set of predictions!\nBut if we plot these predictions on a scatter plot of fWHR as a function of AssR, we do not get a straight line, because the predictions are also impacted by varying values of weight, Sex, and Group:\n\ngf_point(fWHR ~ AssR, data = b2) |&gt;\n  gf_line(preds ~ AssR, data=b2)\n\n\n\n\nBut…we would really like a straight line that helps us visualize the meaning of the \\(\\beta\\) (slope coefficient) for AssR. We can make predictions for a hypothetical dataset, in which AssR varies over a reasonable range, but the other predictors stay constant. This lets us see how AssR (and only AssR) affects the response, without contributions from other predictors. In choosing the values to include in hypothetical dataset, we often choose to hold variables constant at their most common or median values, but not blindly: also, avoid impossible or implausible variable combinations (for example, specifying that a person lives in the state of Michigan but the city of Chicago, or that they are a 5-year-old person with 4 children). In this case, to match the figures in the published paper, we are also going to vary the Sex - but generally you’d only allow one predictor to vary.\n\nfake_data &lt;- expand.grid(AssR = seq(from=1.8, to=5.7, by=0.05),\n                         weight = 38.5,\n                         Sex = c('Female', 'Male'),\n                         Group = 'Wuppertal')\n\nfake_data &lt;- fake_data |&gt; \n  mutate(preds = predict(mlr3, newdata = fake_data))\ngf_line(preds ~ AssR, color = ~Sex, data=fake_data) |&gt; gf_labs(y='Predicted\\nfWHR')\n\n\n\n\n\n1.8.2.1 Comprehension checks:\n\nShould we overlay prediction-plot line(s) on the data scatter plot?\nHow do you think the plot would look if we changed the constant predictor values?\nWhat is missing from this picture?\n\n\n\n1.8.2.2 Shortcut\n\nlibrary(s245)\npred_plot(mlr3, 'AssR')"
  },
  {
    "objectID": "lm1.html#why-are-we-doing-this-again",
    "href": "lm1.html#why-are-we-doing-this-again",
    "title": "1  Linear Regression",
    "section": "1.9 Why are we doing this again?",
    "text": "1.9 Why are we doing this again?\nWhy make prediction plots?"
  },
  {
    "objectID": "lm1.html#shortcut-method---with-uncertainty",
    "href": "lm1.html#shortcut-method---with-uncertainty",
    "title": "1  Linear Regression",
    "section": "1.10 Shortcut Method - With Uncertainty",
    "text": "1.10 Shortcut Method - With Uncertainty\nWe saw before that pred_plot() makes it very easy for us to generate prediction plots showing what a (multiple regression) model says about the relationship between the response and one of the predictors:\n\nlibrary(s245)\npred_plot(mlr3, 'AssR') |&gt;\n  gf_labs(y = 'Predicted fWHR')\n\n\n\n\nNote the custom axis label - otherwise you get a long, unwieldy default “Predictions from fitted model”\n\nlibrary(s245)\npred_plot(mlr3, 'Group') |&gt;\n  gf_labs(y = 'Predicted fWHR')\n\n\n\n\nThey look nice! But they should raise two questions:\n\nUncertainty:\nFixed values:\n\n\nget_fixed(bonobos)  |&gt; \n  pander::pander()\n\nQuitting from lines 548-550 [unnamed-chunk-18] (lm1.rmarkdown) Error in if (class(data[, v]) == \"character\") ...: ! the condition has length &gt; 1 Backtrace: 1. pander::pander(get_fixed(bonobos)) 2. s245::get_fixed(bonobos)\n\n\n\n1.10.1 Anatomy of a Confidence Interval\n\npred_plot(mlr3, 'Sex') |&gt;\n  gf_labs(y = 'Predicted fWHR')"
  },
  {
    "objectID": "lm1.html#diy-method",
    "href": "lm1.html#diy-method",
    "title": "1  Linear Regression",
    "section": "1.11 DIY Method",
    "text": "1.11 DIY Method\n\n1.11.1 Creating a hypothetical dataset\nWe would like to create a hypothetical dataset where one predictor variable varies, and all the rest stay fixed. Let’s choose AssR. We use expand.grid():\n\nfake_data &lt;- expand.grid(AssR = seq(from=1.8, to=5.7, by=0.05),\n                         weight = 40,\n                         Sex = 'Female',\n                         Group = 'Twycross')\nglimpse(fake_data)\n\nRows: 79\nColumns: 4\n$ AssR   &lt;dbl&gt; 1.80, 1.85, 1.90, 1.95, 2.00, 2.05, 2.10, 2.15, 2.20, 2.25, 2.3…\n$ weight &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ Sex    &lt;fct&gt; Female, Female, Female, Female, Female, Female, Female, Female,…\n$ Group  &lt;fct&gt; Twycross, Twycross, Twycross, Twycross, Twycross, Twycross, Twy…\n\n\nNow, make predictions for our fake data.\n\npreds &lt;- predict(mlr3, newdata = fake_data, se.fit = TRUE)\nfake_data &lt;- fake_data |&gt;\n  mutate(fitted = preds$fit,\n         se.fit = preds$se.fit)\nglimpse(fake_data)\n\nRows: 79\nColumns: 6\n$ AssR   &lt;dbl&gt; 1.80, 1.85, 1.90, 1.95, 2.00, 2.05, 2.10, 2.15, 2.20, 2.25, 2.3…\n$ weight &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ Sex    &lt;fct&gt; Female, Female, Female, Female, Female, Female, Female, Female,…\n$ Group  &lt;fct&gt; Twycross, Twycross, Twycross, Twycross, Twycross, Twycross, Twy…\n$ fitted &lt;dbl&gt; 1.149038, 1.152256, 1.155474, 1.158692, 1.161910, 1.165128, 1.1…\n$ se.fit &lt;dbl&gt; 0.08347207, 0.08267088, 0.08187552, 0.08108616, 0.08030298, 0.0…\n\n\nHow do we go from standard errors to confidence intervals? We can either do this before plotting, or while plotting. To do it before and add the results to the hypothetical dataset:\n\nfake_data &lt;- fake_data |&gt;\n  mutate(CI_lower = fitted - 1.96*se.fit,\n         CI_upper = fitted + 1.96*se.fit)\nglimpse(fake_data)\n\nRows: 79\nColumns: 8\n$ AssR     &lt;dbl&gt; 1.80, 1.85, 1.90, 1.95, 2.00, 2.05, 2.10, 2.15, 2.20, 2.25, 2…\n$ weight   &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 4…\n$ Sex      &lt;fct&gt; Female, Female, Female, Female, Female, Female, Female, Femal…\n$ Group    &lt;fct&gt; Twycross, Twycross, Twycross, Twycross, Twycross, Twycross, T…\n$ fitted   &lt;dbl&gt; 1.149038, 1.152256, 1.155474, 1.158692, 1.161910, 1.165128, 1…\n$ se.fit   &lt;dbl&gt; 0.08347207, 0.08267088, 0.08187552, 0.08108616, 0.08030298, 0…\n$ CI_lower &lt;dbl&gt; 0.9854326, 0.9902210, 0.9949980, 0.9997632, 1.0045164, 1.0092…\n$ CI_upper &lt;dbl&gt; 1.312643, 1.314291, 1.315950, 1.317621, 1.319304, 1.321000, 1…\n\n\n\n\n1.11.2 Making the plot\nNow, we just need to plot!\n\ngf_line(fitted ~ AssR, data=fake_data) |&gt; \n  gf_labs(y='Predicted\\nfWHR') |&gt;\n  gf_ribbon(CI_lower + CI_upper ~ AssR, data = fake_data)\n\n\n\n\nIf we wanted to figure out the CI bounds while plotting, we could calculate them on the fly like this:\n\ngf_line(fitted ~ AssR, data=fake_data) |&gt; \n  gf_labs(y='Predicted\\nfWHR') |&gt;\n  gf_ribbon((fitted - 1.96*se.fit ) + (fitted + 1.96*se.fit) ~ AssR,\n            data = fake_data)\n\n(which will look just the same).\n\n\n1.11.3 Categorical predictors\nWhat will be different if the predictor of interest is categorical?\n\nhypothetical data:\nplot:\n\n\nfake_sex_data &lt;- expand.grid(AssR = 4.51,\n                         weight = 40,\n                         Sex = c('Male', 'Female'),\n                         Group = 'Twycross')\npreds &lt;- predict(mlr3, newdata = fake_sex_data, se.fit = TRUE)\nfake_sex_data &lt;- fake_sex_data |&gt;\n  mutate(fitted = preds$fit,\n         se.fit = preds$se.fit)\ngf_point(fitted ~ Sex, data=fake_sex_data) |&gt; \n  gf_labs(y='Predicted fWHR') |&gt;\n  gf_errorbar((fitted - 1.96*se.fit ) + (fitted + 1.96*se.fit) ~ Sex, \n            data = fake_sex_data)"
  },
  {
    "objectID": "IC-based-selection.html#data-and-model",
    "href": "IC-based-selection.html#data-and-model",
    "title": "2  Model Selection Using Information Criteria",
    "section": "2.1 Data and Model",
    "text": "2.1 Data and Model\nToday we will recreate part of the analysis from Vertebrate community composition and diversity declines along a defaunation gradient radiating from rural villages in Gabon, by Sally Koerner and colleagues. They investigated the relationship between rural villages, hunting, and wildlife in Gabon. They asked how monkey abundance depends on distance from villages, village size, and vegetation characteristics. They shared their data at Dryad.org and we can read it in and fit a regression model like this:\n\ndefaun &lt;- read.csv('http://sldr.netlify.com/data/koerner_gabon_defaunation.csv')\n\n\nape_mod &lt;- lm(RA_Apes ~ Veg_DBH + Veg_Canopy + Veg_Understory +\n                   Veg_Rich + Veg_Stems + Veg_liana +\n                   LandUse + Distance + NumHouseholds, data = defaun)\nsummary(ape_mod)\n\n\nCall:\nlm(formula = RA_Apes ~ Veg_DBH + Veg_Canopy + Veg_Understory + \n    Veg_Rich + Veg_Stems + Veg_liana + LandUse + Distance + NumHouseholds, \n    data = defaun)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9857 -0.9419 -0.0360  0.8239  6.3832 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)     5.752517  13.372210   0.430   0.6741  \nVeg_DBH        -0.093171   0.073114  -1.274   0.2249  \nVeg_Canopy      0.670094   2.062545   0.325   0.7504  \nVeg_Understory -1.691235   2.071299  -0.817   0.4289  \nVeg_Rich        0.361960   0.480362   0.754   0.4646  \nVeg_Stems      -0.097211   0.169073  -0.575   0.5751  \nVeg_liana      -0.158505   0.253031  -0.626   0.5419  \nLandUseNeither  1.696755   2.058937   0.824   0.4247  \nLandUsePark    -2.947189   2.222710  -1.326   0.2077  \nDistance        0.302626   0.119865   2.525   0.0254 *\nNumHouseholds  -0.002107   0.043458  -0.048   0.9621  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.725 on 13 degrees of freedom\nMultiple R-squared:  0.5439,    Adjusted R-squared:  0.1931 \nF-statistic: 1.551 on 10 and 13 DF,  p-value: 0.2262\n\nas.numeric(logLik(ape_mod))\n\n[1] -50.75799"
  },
  {
    "objectID": "IC-based-selection.html#calculations",
    "href": "IC-based-selection.html#calculations",
    "title": "2  Model Selection Using Information Criteria",
    "section": "2.2 Calculations",
    "text": "2.2 Calculations\n\nInformation criteria allow us to balance the conflicting goals of having a model that fits the data as well as possible (which pushes us toward models with more predictors) and parsimony (choosing the simplest model, with the fewest predictors, that works for the data and research question). The basic idea is that we minimize the quantity \\(-(2LogLikelihood - penalty) = -2LogLikelihood + penalty\\)\nAIC is computed according to \\(-2LogLikelihood +2k\\), where \\(k\\) is the number of coefficients being estimated (don’t forget \\(\\sigma\\)!) Smaller AIC is better.\nBIC is computed according to \\(-2LogLikelihood + ln(n)k\\), where \\(n\\) is the number of observations (rows) in the dataset and \\(k\\) is the number of coefficients being estimated. Smaller BIC is better.\nVerify that the BIC for this model is 139.65."
  },
  {
    "objectID": "IC-based-selection.html#decisions-with-ics",
    "href": "IC-based-selection.html#decisions-with-ics",
    "title": "2  Model Selection Using Information Criteria",
    "section": "2.3 Decisions with ICs",
    "text": "2.3 Decisions with ICs\nThe following rules of thumb (not laws, just common rules of thumb) may help you make decisions with ICs:\n\nA model with lower IC by at least 3 units is notably better.\nIf two or more models have ICs within 3 IC units of each other, there is not a lot of difference between them. Here, we usually choose the model with fewest predictors.\nIn some cases, if the research question is to measure the influence of some particular predictor on the response, but the IC does not strongly support including that predictor in the best model (IC difference less than 3), you might want to keep it in anyway and then discuss the situation honestly, for example, “AIC does not provide strong support for including predictor x in the best model, but the model including predictor x indicates that as x increases the response decreases slightly. More research would be needed…”"
  },
  {
    "objectID": "IC-based-selection.html#all-possible-subsets-selection",
    "href": "IC-based-selection.html#all-possible-subsets-selection",
    "title": "2  Model Selection Using Information Criteria",
    "section": "2.4 All-possible-subsets Selection",
    "text": "2.4 All-possible-subsets Selection\nThe model we just fitted is our full model, with all predictors of potential interest included. How can we use information criteria to choose the best model from possible models with subsets of the predictors?\nWe can use the dredge() function from the MuMIn package to get and display ICs for all these models.\nBefore using dredge, we need to make sure our dataset has no missing values, and also set the “na.action” input for our model (can be done in call to lm(..., na.action = 'na.fail') also).\n\nlibrary(MuMIn)\nape_mod &lt;- ape_mod |&gt; update(na.action = 'na.fail')\nape_dredge &lt;- dredge(ape_mod, rank='BIC')\n\nFixed term is \"(Intercept)\"\n\npander::pander(head(ape_dredge, 7))\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\n \n(Intercept)\nDistance\nLandUse\nNumHouseholds\nVeg_Canopy\n\n\n\n\n258\n8.753\n0.195\nNA\nNA\nNA\n\n\n2\n-0.6912\n0.2303\nNA\nNA\nNA\n\n\n274\n11.44\n0.1848\nNA\nNA\nNA\n\n\n322\n11.9\n0.2033\nNA\nNA\nNA\n\n\n290\n9.805\n0.1884\nNA\nNA\nNA\n\n\n386\n9.49\n0.1976\nNA\nNA\nNA\n\n\n266\n7.783\n0.1896\nNA\nNA\n0.2771\n\n\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\n\n \nVeg_DBH\nVeg_liana\nVeg_Rich\nVeg_Stems\nVeg_Understory\ndf\n\n\n\n\n258\nNA\nNA\nNA\nNA\n-2.988\n4\n\n\n2\nNA\nNA\nNA\nNA\nNA\n3\n\n\n274\n-0.04551\nNA\nNA\nNA\n-3.144\n5\n\n\n322\nNA\nNA\n-0.1939\nNA\n-3.11\n5\n\n\n290\nNA\n-0.09802\nNA\nNA\n-2.952\n5\n\n\n386\nNA\nNA\nNA\n-0.03113\n-2.904\n5\n\n\n266\nNA\nNA\nNA\nNA\n-2.964\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nlogLik\nBIC\ndelta\nweight\n\n\n\n\n258\n-53.9\n120.5\n0\n0.3284\n\n\n2\n-55.8\n121.1\n0.6241\n0.2404\n\n\n274\n-53.38\n122.7\n2.146\n0.1123\n\n\n322\n-53.55\n123\n2.491\n0.09449\n\n\n290\n-53.67\n123.2\n2.727\n0.08399\n\n\n386\n-53.82\n123.5\n3.03\n0.0722\n\n\n266\n-53.88\n123.7\n3.144\n0.0682\n\n\n\n\n\n\nWhat is the best model according to BIC, for this dataset?"
  },
  {
    "objectID": "IC-based-selection.html#which-ic-should-i-use",
    "href": "IC-based-selection.html#which-ic-should-i-use",
    "title": "2  Model Selection Using Information Criteria",
    "section": "2.5 Which IC should I use?",
    "text": "2.5 Which IC should I use?\nAIC and BIC may give different best models, especially if the dataset is large. You may want to just choose one to use a priori (before making calculations). You might prefer BIC if you want to err on the “conservative” side, as it is more likely to select a “smaller” model with fewer predictors. This is because of its larger penalty."
  },
  {
    "objectID": "IC-based-selection.html#quantities-derived-from-aic",
    "href": "IC-based-selection.html#quantities-derived-from-aic",
    "title": "2  Model Selection Using Information Criteria",
    "section": "2.6 Quantities derived from AIC",
    "text": "2.6 Quantities derived from AIC\n\n\\(\\Delta AIC\\) is the AIC for a given model, minus the AIC of the best one in the dataset. (Same for \\(\\Delta BIC\\))\nAkaike weights are values (ranging from 0-1) that measure the weight of evidence suggesting that a model is the best one (given that there is one best one in the set)"
  },
  {
    "objectID": "IC-based-selection.html#important-caution",
    "href": "IC-based-selection.html#important-caution",
    "title": "2  Model Selection Using Information Criteria",
    "section": "2.7 Important Caution",
    "text": "2.7 Important Caution\nVery important: IC can ONLY be compared for models with the same response variable, and the exact same rows of data."
  },
  {
    "objectID": "likelihood.html#data",
    "href": "likelihood.html#data",
    "title": "3  Likelihood",
    "section": "3.1 Data",
    "text": "3.1 Data\nToday’s dataset was collected in Senegal in 2015-2016 in a survey carried out by UNICEF, of 5440 households in the urban area of Dakar, Senegal. Among these households, information was collected about 4453 children under 5 years old, including their \n\ngf_dhistogram(~AN3, data=wt, binwidth=1) |&gt;\n  gf_labs(x='Weight (kg)', y='Probability\\nDensity') |&gt;\n  gf_fitdistr(dist='dnorm', size=1.3) |&gt;\n  gf_refine(scale_x_continuous(breaks=seq(from=0, to=30, by=2)))"
  },
  {
    "objectID": "likelihood.html#review---the-normal-probability-density-function-pdf",
    "href": "likelihood.html#review---the-normal-probability-density-function-pdf",
    "title": "3  Likelihood",
    "section": "3.2 Review - the Normal probability density function (PDF)",
    "text": "3.2 Review - the Normal probability density function (PDF)\n\\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\]"
  },
  {
    "objectID": "likelihood.html#a-simple-model",
    "href": "likelihood.html#a-simple-model",
    "title": "3  Likelihood",
    "section": "3.3 A simple model",
    "text": "3.3 A simple model\nThe distribution of weights looks quite unimodal and symmetric, so we will model it with a normal distribution with mean 11.8 and standard deviation 3.53 (N( \\(\\mu=\\) 11.8, \\(\\sigma=\\) 3.53), black line)."
  },
  {
    "objectID": "likelihood.html#using-the-model-to-make-predictions",
    "href": "likelihood.html#using-the-model-to-make-predictions",
    "title": "3  Likelihood",
    "section": "3.4 Using the Model to Make Predictions",
    "text": "3.4 Using the Model to Make Predictions\nIf you had to predict the weight of one child from this population, what weight would you guess? \nIs it more likely for a child in Dakar to weigh 10kg, or 20kg? How much more likely?  \nWhat is the probability of a child in Dakar weighing 11.5 kg?"
  },
  {
    "objectID": "likelihood.html#likelihood-to-the-rescue",
    "href": "likelihood.html#likelihood-to-the-rescue",
    "title": "3  Likelihood",
    "section": "3.5 Likelihood to the Rescue!",
    "text": "3.5 Likelihood to the Rescue!\nWhich is more likely: three children who weigh 11, 8.2, and 13kg, or three who weigh 10, 12.5 and 15 kg?\nHow did you:\n\nFind the likelihood of each observation?\n\n \n\nCombine the likelihoods of a set of three observations?\n\n \nWhat did you have to assume about the set of observations?"
  },
  {
    "objectID": "likelihood.html#how-does-this-relate-to-linear-regression",
    "href": "likelihood.html#how-does-this-relate-to-linear-regression",
    "title": "3  Likelihood",
    "section": "3.6 How does this relate to linear regression?",
    "text": "3.6 How does this relate to linear regression?\nWhat if we think of this situation as a linear regression problem (with no predictors)?\n\nlm_version &lt;- lm(AN3 ~ 1, data = wt)\nsummary(lm_version)\n\n\nCall:\nlm(formula = AN3 ~ 1, data = wt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.8964 -2.3964  0.1036  2.4036 18.9036 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.79644    0.05435   217.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.529 on 4216 degrees of freedom\n  (235 observations deleted due to missingness)\n\n\n\n3.6.1 Model Equation:"
  },
  {
    "objectID": "likelihood.html#likelihood-of-a-dataset-given-a-model",
    "href": "likelihood.html#likelihood-of-a-dataset-given-a-model",
    "title": "3  Likelihood",
    "section": "3.7 Likelihood of a dataset, given a model",
    "text": "3.7 Likelihood of a dataset, given a model\nFinally, now, we can understand what we were computing when we did\n\nlogLik(lm_version)\n\n'log Lik.' -11301.19 (df=2)\n\n\nFor our chosen regression model, we know that the residuals should have a normal distribution with mean 0 and standard deviation \\(\\sigma\\) (estimated Residual Standard Error from R summary() output).\nFor each data point in the dataset, for a given regression model, we can compute a model prediction.\nWe can subtract the prediction from the observed response-variable values to get the residuals.\nWe can compute the likelihood (\\(L\\)) of this set of residuals by finding the likelihood of each individual residual \\(e_i\\) in a \\(N(0, \\sigma)\\) distribution.\nTo get the likelihood of the full dataset given the model, we use the fact that the residuals are independent (they better be, because that was one of the conditions of of linear regression model) – we can multiply the likelihoods of all the individual residuals together to get the joint likelihood of the full set.\nThat is the “likelihood” that is used in the AIC and BIC calculations we considered earlier."
  },
  {
    "objectID": "pdf-pmf.html#beyond-normal",
    "href": "pdf-pmf.html#beyond-normal",
    "title": "4  PDFs and PMFs",
    "section": "4.1 Beyond Normal",
    "text": "4.1 Beyond Normal\nIn our exploration of likelihoods, we did a little bit of work with the Normal probability density function. Here, we will state some characteristics of the normal distribution slightly more formally, and then we will get familiar with some other probability distributions (a.k.a. “the normal distribution’s wierd friends”)."
  },
  {
    "objectID": "pdf-pmf.html#types-of-probability-distributions",
    "href": "pdf-pmf.html#types-of-probability-distributions",
    "title": "4  PDFs and PMFs",
    "section": "4.2 Types of probability distributions",
    "text": "4.2 Types of probability distributions\n\n4.2.1 Continuous distributions\nThe probability distribution of a continuous variable (one that can take on continuous real values, at least within a certain range) is called a probability density function or PDF for short.\nPDFs are functions of one continuous variable (we’ll call it \\(x\\)) that have two properties in common:\n\nThe total area under the curve is 1 (\\(\\int_{-\\infty}^{\\infty} f(x)dx = 1\\))\nThe values of the function are non-negative (0, or a positive real number) for all real \\(x\\) (\\(f(x) \\geq 0 \\forall x \\in \\mathbb{R}\\))\n\nFor PDFs, the function values (\\(y\\)-axis values) are probability densities, useful for computing likelihoods; probabilities are given by finding areas under the curve.\n\n\n4.2.2 Discrete Distributions\nWe use categorical as well as quantitative variables in our regression models, so it will prove useful to have some discrete probability distributions as well as continuous ones. Discrete distributions associate each possible value of a discrete variable with its probability of occurence.\nA discrete probability distribution is characterized by a probability mass function or PMF for short (this is the discrete equivalent of a PDF).\nA PMF is a function of one discrete variable (we’ll call it \\(x\\)) that have two properties in common:\n\nThe sum of all the function’s values is 1 (\\(\\sum_{x \\in S} f(x) =1\\), where \\(S\\) is the set of all possible values \\(x\\) can have)\nAll values of the function are between 0 and 1 inclusive (they are probabilities) (\\(f(x) \\in [0,1] \\forall x \\in S\\))"
  },
  {
    "objectID": "pdf-pmf.html#relevant-features-of-distributions",
    "href": "pdf-pmf.html#relevant-features-of-distributions",
    "title": "4  PDFs and PMFs",
    "section": "4.3 Relevant Features of Distributions",
    "text": "4.3 Relevant Features of Distributions\nFor this course, the most important features to note about each probability distribuiton will be:\n\nThe type of distribution: discrete or continuous?\nThe support of the distribution: what range of possible values can the random variable \\(X\\) take on?\nThe parameters of the distribution. Changing the parameters tunes the center and shape of the distribution, so these are what we need to estimate to fit a particular kind of distribution to a specific dataset. (For example, the parameters of the normal distribution are the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\).)\nThe shape of the distribution: what shapes can the function take one? (For example, the normal distribution is always unimodal and symmetric.)\nThe PDF or PMF of the distribution. What mathematical expression controls the shape of the distribution?\nWe will also note a few examples of variables that might be well modelled by each distribution."
  },
  {
    "objectID": "pdf-pmf.html#examples-of-continuous-distributions",
    "href": "pdf-pmf.html#examples-of-continuous-distributions",
    "title": "4  PDFs and PMFs",
    "section": "4.4 Examples of Continuous Distributions",
    "text": "4.4 Examples of Continuous Distributions\n\n4.4.1 Normal\n\n4.4.1.0.1 Type\nContinuous\n\n\n4.4.1.0.2 Support\nAll real numbers\n\n\n4.4.1.0.3 Parameters\n\n\\(\\mu\\), the mean, which can take on any real value\n\\(\\sigma\\), the standard deviation, which can take on any positive real value\n\n\n\n4.4.1.0.4 Shapes\nThe shape is always unimodal and symmetric.\n\n\n\n\n\n\n\n4.4.1.0.5 PDF or PMF\nThe normal distribution has PDF:\n\\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n4.4.1.0.6 Examples\nA normal distribution might be a good fit for data on childrens’ weights in kg, or for the duration of visits at a zoo, or…\n\n\n\n4.4.2 Gamma\n\n4.4.2.0.1 Type\nContinuous\n\n\n4.4.2.0.2 Support\npositive real numbers\n\n\n4.4.2.0.3 Parameters\nThere are two alternate but equivalent parameterizations for the gamma distribution.\nThe first option: \\(\\alpha\\) (shape) and \\(\\beta\\) (rate), where \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\)\nThe second option: \\(k\\) (shape) and \\(\\theta\\) (scale), where \\(k &gt;0\\) and \\(\\theta&gt;0\\).\nConverting between the two parameterizations: \\(\\alpha = k\\) and \\(\\beta = \\frac{1}{\\theta}\\).\n\n\n4.4.2.0.4 Shapes\nThe gamma distribution can take on a unimodal, symmetric shape or a unimodal shape with any amount of right skew (up to an exponential distribution shape).\n\n\n\n\n\nNote: you don’t need to be familiar with exactly how the different values of parameters influence the shape of the gamma distribution PDF, so the curves here are not labelled with parameter values.\n\n\n4.4.2.0.5 PDF or PMF\nThe gamma distribution has PDF:\n\\[ f(x) = \\frac{1}{\\Gamma(k) \\theta^k}x^{(k-1)}e^{\\frac{-x}{\\theta}}\\] Where \\(\\Gamma\\) is the Gamma function (look up the definition if you choose), or equivalently,\n\\[ f(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{(\\alpha-1)}e^{-\\beta x}\\]\n\n\n4.4.2.0.6 Examples\nGamma distributions are often used to model things like wind speed or duration of an event (any quantity that might have right skew and is never negative).\n\n\n\n4.4.3 Beta\n\n4.4.3.0.1 Type\nContinuous\n\n\n4.4.3.0.2 Support\nReal numbers between 0 and 1 ([0,1])\n\n\n4.4.3.0.3 Parameters\n\\(\\alpha\\) (shape 1) and \\(\\beta\\) (shape 2), both of which must be \\(&gt;1\\).\n\n\n4.4.3.0.4 Shapes\nThis distribution can take on almost any shape, for example:\n\ngf_dist('beta', params = c(shape1=5, shape2=1)) |&gt;\n  gf_dist('beta', params = c(shape1=1, shape2=3), color=colrs[1]) |&gt;\n  gf_dist('beta', params = c(shape1=2, shape2=2), color=colrs[2]) |&gt;\n  gf_dist('beta', params = c(shape1=2, shape2=5), color=colrs[3]) |&gt;\n  gf_dist('beta', params = c(shape1=0.5, shape2=0.5), color=colrs[4]) |&gt;\n  gf_labs(x='Possible Values of Variable', y='Probability Density') |&gt;\n  gf_lims(y=c(0,2.5))\n\n\n\n\n\n\n4.4.3.0.5 PDF or PMF\nThe PDF is:\n\\[ f(x) = \\frac{x^{(\\alpha -1)}(1-x)^{(\\beta-1)}}{ B(\\alpha, \\beta)}\\] Where \\(B\\) is the Beta function (again, feel free to look up the definition if you are interested).\n\n\n4.4.3.0.6 Examples\nBeta distributions could be used for any variable that takes on values between 0-1, for example, baseball players’ batting averages, or test scores (as proportions)."
  },
  {
    "objectID": "pdf-pmf.html#examples-of-discrete-distributions",
    "href": "pdf-pmf.html#examples-of-discrete-distributions",
    "title": "4  PDFs and PMFs",
    "section": "4.5 Examples of Discrete Distributions",
    "text": "4.5 Examples of Discrete Distributions\n\n4.5.1 Binomial\n\n4.5.1.0.1 Type\nDiscrete\n\n\n4.5.1.0.2 Support\nYou can think about the support of this distribution two ways. Technically, the support is \\(k= 0, 1,2,3,\\dots\\): 0 and positive integers, interpreted as the number of “successes” in \\(n\\) binomial trials. Binomial trials are independent observations of a process that has two possible outcomes, “success” or “failure”, with set probabilities of each occurring. (And probabilities of success and failure must sum to 1.)\nSo, for each individual binomial trial, the possible outcomes are 0 and 1, often interpreted as TRUE and FALSE or “success” and “failure”.\nFor our purposes, this distribution will be useful for modelling response variables that are categorical with two possible values (and the \\(n\\) trials will be the \\(n\\) rows in our dataset).\n\n\n4.5.1.0.3 Parameters\nParameters are \\(n\\), the number of independent trials, and \\(p\\), the probability of success in each trial.\n\n\n4.5.1.0.4 Shapes\nThe figure below shows the shape of binomial distributions with fixed \\(n=100\\) and varying \\(p\\):\n\ngf_dist('binom', params = c(size=100, prob=0.1), color=colrs[1]) |&gt;\n  gf_dist('binom', params = c(size=100, prob=0.25), color=colrs[2]) |&gt;\n  gf_dist('binom', params = c(size=100, prob=0.5), color=colrs[3]) |&gt;\n  gf_dist('binom', params = c(size=100, prob=0.75), color=colrs[4]) |&gt;\n  gf_dist('binom', params = c(size=100, prob=0.99), color=colrs[5]) |&gt;\n  gf_labs(x='k, the Number of Successes in 100 Trials', y='Probability')\n\n\n\n\nThe \\(p\\) used were 0.1, 0.25, 0.5, 0.75, and 0.99. Can you tell which is which?\n\n\n4.5.1.0.5 PDF or PMF\nThe PMF for the binomial distribution is: \\[ P(X=k \\vert n,p) = {n \\choose k} p^k (1-p)^{n-k}\\]\nWhere \\(k\\) is the number of successes observed in \\(n\\) trials (you can think of \\(k\\) as our “x-axis variable” for this PMF).\n\n\n4.5.1.0.6 Examples\nWe might use this distribution to model any categorical variable with two possible values, like Age (if possible values are “adult” and “child”) or health status (“has disease” or “does not have disease”). We’ll think of each observation in the dataset as one of the \\(n\\) indpendent trials, with one of two possible outcomes for each trial.\n\n\n\n4.5.2 Poisson\n\n4.5.2.0.1 Type\nDiscrete\n\n\n4.5.2.0.2 Support\nThe support is 0 and positive integers (i.e., this distribution works well for count data).\n\n\n4.5.2.0.3 Parameters\nThe Poisson distribution has one parameter, \\(\\lambda\\) (the event rate per unit time) which must be greater than 0.\n\n\n4.5.2.0.4 Shapes\nThe distribution can take on unimodal shapes with varying amounts of right skew (from none, to an exponential shape).\n\ncolrs &lt;- RColorBrewer::brewer.pal(8, 'Set2')\ngf_dist('pois', params=c(lambda=0.5), color=colrs[1]) |&gt;\n  gf_dist('pois', params=c(lambda=3), color=colrs[2]) |&gt;\n  gf_dist('pois', params=c(lambda=6), color=colrs[3]) |&gt;\n  gf_dist('pois', params=c(lambda=15), color=colrs[4]) |&gt;\n  gf_labs(x='k (number of events)', y='Probability')\n\n\n\n\n\n\n4.5.2.0.5 PDF or PMF\nThe Poisson PMF is:\n\\[P(X=k \\vert \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\]\n\n\n4.5.2.0.6 Examples\nThe Poisson distribution might be used to model any response variable that is comprised of counts, for example, the number of birds sighted in a bird survey, or the number of people admitted to an emergency room each hour.\n\n\n\n4.5.3 Negative Binomial\nThere are two versions or “types” of this distribution, cleverly known as NB1 (type 1) and NB2 (type 2). NB1 has “constant overdispersion” – the variance of the distribution is greater than the mean according to a constant ratio. NB2 has “variable overdispersion” – the variance is a quadratic function of the mean. The NB2 is the one that corresponds directly to conceptualization in terms of binomial trials (with the PMF giving the probability of observing \\(y\\) failures before the \\(r\\)th success). Hardin and Hilbe 2007 describe the negative binomial this way: “Instead of counts entering uniformly, we see counts entering with a specific gamma-distributed shape.”\n\n4.5.3.0.1 Type\nDiscrete\n\n\n4.5.3.0.2 Support\nThe support is 0 and positive integers (i.e., this distribution works well for count data). It also has a derivation in terms of binomial trials, but in our regression models, we will only use it with count data.\n\n\n4.5.3.0.3 Parameters\nA common parameterization of the negative binomial (online and in actuarial science) has parameters \\(p\\), the probability of success on each binomial trial, and \\(r\\), the number of failures observed. The PMF then gives the probability of observing \\(k\\) failures before the \\(r\\)th success in a series of Bernoulli trials.\nHere, we will use an alternate parameterization. The other common way to parameterize and derive the NB is as a Poisson-gamma mixture – a modified version of a Poisson distribution. In this scheme, the parameters of the distribution are \\(\\mu\\) and \\(\\alpha\\).\n\n\n4.5.3.0.4 Shapes\nThese distributions can take on unimodal shapes with varying amounts of right skew. In NB2 (type 2) distributions the variance (spread) is larger relative to the mean.\nNB1:\n\nlibrary(gamlss.dist)\ncolrs &lt;- RColorBrewer::brewer.pal(8, 'Set2')\ngf_dist('NBI', params=c(mu=1, sigma=0.5), color=colrs[1]) |&gt;\n  gf_dist('NBI', params=c(mu=1, sigma=0.5), color=colrs[2]) |&gt;\n  gf_dist('NBI', params=c(mu=4, sigma=1), color=colrs[3]) |&gt;\n  gf_dist('NBI', params=c(mu=15, sigma=4), color=colrs[4]) |&gt;\n  gf_lims(x=c(0,20)) |&gt;\n  gf_labs(x='k (number of events)', y='Probability')\n\n\n\n\nNB2:\n\ncolrs &lt;- RColorBrewer::brewer.pal(8, 'Set2')\ngf_dist('NBII', params=c(mu=1, sigma=0.5), color=colrs[1]) |&gt;\n  gf_dist('NBII', params=c(mu=1, sigma=0.5), color=colrs[2]) |&gt;\n  gf_dist('NBII', params=c(mu=4, sigma=1), color=colrs[3]) |&gt;\n  gf_dist('NBII', params=c(mu=15, sigma=4), color=colrs[4]) |&gt;\n  gf_lims(x=c(0,20)) |&gt;\n  gf_labs(x='k (number of events)', y='Probability')\n\n\n\n\n\n\n4.5.3.0.5 PDF or PMF\nDetails of the parameterizations and likelihood and fitting of NB1 and NB2 distributions can be found in Hardin and Hilbe 2007, if you are interested.\nThe PMF for the NB1, where the variance is a constant multiple of the mean, is:\n\\[ f(x \\vert \\mu, \\alpha) = \\frac{\\Gamma(x + \\mu)}{\\Gamma(\\mu)\\Gamma(x+1)}(\\frac{1}{1+\\alpha})^\\mu(\\frac{\\alpha}{1+\\alpha})^x \\]\nWhere \\(\\Gamma\\) is a Gamma function. Note that if \\(\\alpha\\) = 0 this becomes a Poisson distribution, so the Poisson is a special case of the NB1.\nThe PMF for the NB2, where the variance is a quadratic function of the mean, is:\n\\[ f(x \\vert \\mu, \\alpha) = \\frac{\\Gamma(x + \\frac{1}{\\alpha})}{\\Gamma(\\frac{1}{\\alpha})\\Gamma(x+1)}(\\frac{1}{1+\\alpha \\mu})^{\\frac{1}{\\alpha}}(1 - \\frac{1}{1+\\alpha \\mu})^x \\]\n\n\n4.5.3.0.6 Examples\nNB distributions are good models for overdispersed count data, where (in the regression context) the residual variance is not equal to the expected (predicted) value. (Note that if you are reading this before learning about regression models for count data, you may not understand this sentence yet…don’t worry, it will make sense when you return later!) Some examples might include sightings data on numbers of animals seen on wildlife surveys, or the number of items bought per order at an online retailer.\n\n\n\n4.5.4 A Mixture Distribution: Tweedie\nThe Tweedie family of distributions is a very large one - depending on the values of the different parameters, the PMF/PDF can be written in many different ways, and it can take on many different shapes. The description below is a simplified one, geared toward the types of Tweedie distributions we are likely to try to use in regression models in this course – mainly the “compound Poisson-gamma” type.\nSome extra resources for which you will not be held responsible in this course:\n\nYou can find an accessible description and example of this kind of distribution at: http://www.notenoughthoughts.net/posts/modeling-activity.html.\nThe following site may also be useful in regard to using the Tweedie in regression models: http://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_genmod_details28.htm\n\n\n4.5.4.0.1 Type\nThese distributions are both continuous and discrete - a kind of mix of a Poisson distribution and gamma distribution(s).\n\n\n4.5.4.0.2 Support\nThe support is non-negative real numbers (greater than or equal to 0).\n\n\n4.5.4.0.3 Parameters\n(Note: there are multiple different ways to parameterize these distributions.) We will use:\n\n\\(p\\), the index or power parameter, which can be 0 (resulting in a normal distribution), 1 (resulting in a Poisson distribution), \\(1 &lt; p &lt; 2\\) (a compound Poisson-gamma distribution – what we will mainly use), 2 (a gamma distribution), 3 (an inverse Gaussian distribution), \\(&lt;3\\) (a positive stable distribution), or \\(\\infty\\) (an extreme positive stable distribuiton). For the case where \\(1 &lt; p &lt; 2\\), and the distribution is a compound of a Poisson and a gamma, then \\(p = \\frac{k+2}{k+1}\\) where \\(k\\) is the parameter of the gamma distribution. When \\(1&lt;p&lt;2\\), \\(p\\) closer to 1 means thant the Poisson distribution (the mass at 0) gets more “weight” in the compound distribution, and values of \\(p\\) closer to 2 mean that the gamma distribution gets more “weight.”\n\\(\\mu\\). For the case where \\(1 &lt; p &lt; 2\\), and the distribution is a compound of a Poisson and a gamma, then \\(\\mu = \\lambda k \\theta\\) where \\(\\lambda\\) is the parameter of the Poisson distribution and \\(k\\) and \\(\\theta\\) are the parameters of the gamma.\n\\(\\phi\\) For the case where \\(1 &lt; p &lt; 2\\), and the distribution is a compound of a Poisson and a gamma, then \\(\\phi = \\frac{\\lambda^{(1-p)} (k \\theta) ^{(2-p)}}{2-p}\\) where \\(\\lambda\\) is the parameter of the Poisson distribution and \\(k\\) and \\(\\theta\\) are the parameters of the gamma.\n\n\n\n4.5.4.0.4 Shapes\nA compound Poisson-gamma Tweedie distribution can take on varying shapes; the main characteristic of interest for us is that it can have a mass at 0, then a unimodal or multimodal distribution with a long right tail (lots of right skew).\n\nlibrary(tweedie)\ntex &lt;- data.frame(x=seq(from=0, by=0.1, to=50)) |&gt;\n  mutate(dens.a = dtweedie(x, xi = 1.1, mu=4, phi=2)) |&gt;\n  mutate(dens.b = dtweedie(x, xi = 1.3, mu=4, phi=0.5)) |&gt;\n  mutate(dens.c = dtweedie(x, xi = 1.5, mu=4, phi=0.3)) |&gt;\n  mutate(dens.d = dtweedie(x, xi = 1.8, mu=4, phi=1)) |&gt;\n  mutate(dens.e = dtweedie(x, xi = 1.5, mu=4, phi=5))\n  \n\ngf_line(dens.a ~ x, data=tex, color=colrs[1]) |&gt;\n  gf_line(dens.b ~x, data=tex, color=colrs[2]) |&gt;\n  gf_line(dens.c ~x, data=tex, color=colrs[3]) |&gt;\n  gf_line(dens.d ~x, data=tex, color=colrs[4]) |&gt;\n  gf_line(dens.e ~x, data=tex, color=colrs[5])\n\n\n\n\n\n\n4.5.4.0.5 PDF or PMF\nA Tweedie distribution with \\(p&gt;1\\) has the form:\n\\[ f(X \\vert \\mu, \\phi, p) = a(x, \\phi)e^{\\frac{1}{\\phi}(\\frac{x\\mu^{(1-p)}}{(1-p)} - \\kappa(\\mu,p))}\\]\nwhere \\(\\kappa(\\mu,p) = \\frac{\\mu^{(2-p)}}{(2-p)}\\) if \\(p\\neq2\\), and if \\(p=2\\), \\(\\kappa(\\mu,p) = log(\\mu)\\); but \\(a(x,\\phi)\\) is a function that does not have an analytical expression. This expression is from SAS documentation at https://support.sas.com/rnd/app/stat/examples/tweedie/tweedie.pdf.\nAlternately (and more simply(?)), a Tweedie distribution with \\(1 &lt; p &lt; 2\\) is a compound of a Poisson distribution with parameter \\(\\lambda\\) and a gamma distribution with parameters \\(k\\) and \\(\\theta\\). For example:\n“Suppose that airplanes arrive at an airport following a Poisson process, and the number of passengers in each airplane follows a certain gamma distribution. Then, the number of passengers arriving at the airport follows a compound Poisson gamma process \\[ Y = \\sum_{i=1}^{N} D_i\\] where \\(N\\) is the Poisson process that the airplanes follow, and \\(D_i\\) is the gamma distribution that the passengers follow.” (Thanks to D. Mao, http://math.uchicago.edu/~may/REU2013/REUPapers/Mao.pdf for this example.)\n\n\n4.5.4.0.6 Examples\nThe Tweedie distributions may be useful for “zero-inflated” data, where there is a class of observations for which the observed value of the variable is always zero, and another class for which the variable takes on positive continuous values. For example, this might model the number of birds present per unit area (when the study area includes places of unsuitable habitat where none are ever found), or perhaps the quantity of alcohol consumed per week by different people (some of whom may drink varying amounts, and others of whom may never drink at all)."
  },
  {
    "objectID": "count-data-regression.html#data-source",
    "href": "count-data-regression.html#data-source",
    "title": "5  Regression for Count Data",
    "section": "5.1 Data Source",
    "text": "5.1 Data Source\nThe dataset used here is beevisits, from:\nFelicity Muth, Jacob S. Francis, and Anne S. Leonard. 2017. Bees use the taste of pollen to determine which flowers to visit. Biology Letters 12(7): DOI: 10.1098/rsbl.2016.0356. http://rsbl.royalsocietypublishing.org/content/roybiolett/12/7/20160356.full.pdf.\n\nknitr::include_graphics('images/BeeExperiment.png')"
  },
  {
    "objectID": "count-data-regression.html#a-bad-idea-multiple-linear-regression-model",
    "href": "count-data-regression.html#a-bad-idea-multiple-linear-regression-model",
    "title": "5  Regression for Count Data",
    "section": "5.2 A bad idea: multiple linear regression model",
    "text": "5.2 A bad idea: multiple linear regression model\n\n\nRows: 270 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): colony, beename, treatment, target colour, flower\ndbl (1): novisits\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nsummary(beevisits)\n\n    colony            beename              treatment  target colour     \n Length:270         Length:270         quinine  :90   Length:270        \n Class :character   Class :character   cellulose:90   Class :character  \n Mode  :character   Mode  :character   sucrose  :90   Mode  :character  \n                                                                        \n                                                                        \n                                                                        \n    novisits         flower         \n Min.   : 0.000   Length:270        \n 1st Qu.: 1.000   Class :character  \n Median : 3.000   Mode  :character  \n Mean   : 4.437                     \n 3rd Qu.: 7.000                     \n Max.   :21.000                     \n\n\n\nbee.lm &lt;- lm(novisits ~ flower + treatment +\n               colony, data=beevisits)\nsummary(bee.lm)\n\n\nCall:\nlm(formula = novisits ~ flower + treatment + colony, data = beevisits)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5668 -2.1113 -0.2847  1.5665 11.5767 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          2.1113     0.4440   4.755 3.27e-06 ***\nflowernovel         -1.6778     0.4386  -3.825 0.000163 ***\nflowertarget         5.4556     0.4386  12.438  &lt; 2e-16 ***\ntreatmentcellulose   2.6883     0.4389   6.124 3.30e-09 ***\ntreatmentsucrose     2.7240     0.4415   6.170 2.56e-09 ***\ncolonyX             -0.8318     0.4491  -1.852 0.065110 .  \ncolonyY             -1.8493     0.4254  -4.347 1.97e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.942 on 263 degrees of freedom\nMultiple R-squared:  0.5763,    Adjusted R-squared:  0.5667 \nF-statistic: 59.63 on 6 and 263 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "count-data-regression.html#problems-with-the-linear-model",
    "href": "count-data-regression.html#problems-with-the-linear-model",
    "title": "5  Regression for Count Data",
    "section": "5.3 Problems with the linear model",
    "text": "5.3 Problems with the linear model\nWhat problems do we have with this model (its appropriateness, or goodness of fit to the data)?\n\nNon-constant error variance\nNon-normality of residuals\nSome predicted values are less than 0 – it’s impossible that a bee could visit a flower a negative number of times!"
  },
  {
    "objectID": "count-data-regression.html#poisson-regression",
    "href": "count-data-regression.html#poisson-regression",
    "title": "5  Regression for Count Data",
    "section": "5.4 Poisson Regression",
    "text": "5.4 Poisson Regression\nDetailed notes on the model equation were filled in here in class\n\n5.4.1 Fitting the Model\n\nprm &lt;- glm(novisits ~ flower + treatment +\n               colony, data=beevisits,\n           family=poisson(link='log')) \nsummary(prm)\n\n\nCall:\nglm(formula = novisits ~ flower + treatment + colony, family = poisson(link = \"log\"), \n    data = beevisits)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         0.79055    0.08695   9.092  &lt; 2e-16 ***\nflowernovel        -0.75072    0.10442  -7.189 6.51e-13 ***\nflowertarget        0.99945    0.06916  14.451  &lt; 2e-16 ***\ntreatmentcellulose  0.69817    0.07910   8.827  &lt; 2e-16 ***\ntreatmentsucrose    0.70953    0.07967   8.906  &lt; 2e-16 ***\ncolonyX            -0.17521    0.07175  -2.442   0.0146 *  \ncolonyY            -0.43724    0.07311  -5.981 2.22e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1236.33  on 269  degrees of freedom\nResidual deviance:  542.05  on 263  degrees of freedom\nAIC: 1260.6\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n5.4.2 Conditions\nWhat conditions must hold for this model to be appropriate?\n\nResponse variable (y) contains count data\nLinearity: \\(log(\\lambda_{i})\\) is a linear function of the covariates \\(x_1\\), \\(x_2\\), … \\(x_n\\). (Later we will consider how to check this when some covariates are quantitative…in brief: plot log(rate) as a function of each covariate and look for (no non-)linear trend.)\nMean = Variance\nIndependence (of residuals)\nThere is not a condition specifying a PDF that the residuals should follow.\n\n\n\n5.4.3 Model Assessment\nWhich conditions can we check already?\n\nacf(resid(prm, type='response'), main='bee.lm Residuals')\n\n\n\n\nWhat does type='response' mean?\nThis means that the residuals are reported on the same scale as the response variable (that is, in units of counts, rather than log(counts)).\n\nbeevisits &lt;- beevisits |&gt;\n  mutate(pm.resids = resid(prm, type='response'),\n         pm.fitted = fitted(prm))\n\ngf_point(pm.resids ~ pm.fitted, data=beevisits) |&gt;\n  gf_labs(x='Fitted Values', y='Residuals')\n\n\n\n\nThis trumpet pattern is what we expect! Why?\nIf the variance equals the mean, then as the mean (fitted value) goes up, then the variance (spread of residuals) will also be larger.\nBut how can we decide if it’s the right amount of increase in variance with increase in fitted value? We can compute Pearson residuals, which are scaled by the expected variance. The Pearson residuals should have approximately constant variance as a function of fitted values, and a standard deviation of about 1.\n\nbeevisits &lt;- beevisits |&gt;\n  mutate(pm.pearson.resids = resid(prm, type='pearson'))\n\ngf_point( pm.pearson.resids ~ pm.fitted, data=beevisits) |&gt;\n  gf_labs(x='Fitted Values', y='Pearson Residuals')\n\n\n\n\nA more complex solution: we could divide the fitted values into bins (choice of bin size is somewhat arbitrary; generally, you want them as small as possible, but still containing enough observations per bin to get good mean and variance estimates). In each bin, compute the mean and the variance of the residuals. Plot these means vs these variances and see if the slope is 1 (and intercept 0).\nFirst, split the fitted values into 5 bins:\n\nbeevisits &lt;- beevisits |&gt;\n  mutate(fitted.bins = cut(fitted(prm), breaks=5))\n\nhead(beevisits)\n\n# A tibble: 6 × 12\n  colony beename   treatment `target colour` novisits flower lm.resids lm.fitted\n  &lt;chr&gt;  &lt;chr&gt;     &lt;fct&gt;     &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 Y      giantbee… sucrose   yellow                12 target     3.56       8.44\n2 W      o51Wsucr… sucrose   blue                   8 target    -2.29      10.3 \n3 W      o54Wquin… quinine   blue                  13 target     5.43       7.57\n4 W      o58Wcell… cellulose yellow                 9 target    -1.26      10.3 \n5 W      o60Wcell… cellulose blue                  11 target     0.745     10.3 \n6 W      o63Wquin… quinine   blue                   5 target    -2.57       7.57\n# ℹ 4 more variables: pm.resids &lt;dbl&gt;, pm.fitted &lt;dbl&gt;,\n#   pm.pearson.resids &lt;dbl&gt;, fitted.bins &lt;fct&gt;\n\n\nNext, compute the means and variances in each bin, view the result, and plot:\n\nbinned.bees &lt;- beevisits |&gt;\n  group_by(fitted.bins) |&gt;\n  summarise(mean = mean(pm.fitted, na.rm=TRUE),\n            var = var(pm.resids))\n\nbinned.bees\n\n# A tibble: 5 × 3\n  fitted.bins   mean   var\n  &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 (0.661,2.97]  1.77  3.34\n2 (2.97,5.27]   4.28 10.6 \n3 (5.27,7.57]   5.99 21.3 \n4 (7.57,9.88]   7.83 12.8 \n5 (9.88,12.2]  11.5  13.5 \n\ngf_point(var ~ mean, data=binned.bees) |&gt;\n  gf_labs(x='Mean Fitted Value', \n          y='Variance of Residuals') |&gt;\n   gf_abline(slope=1, intercept=0)\n\n\n\n\nThat’s a bit of a pain, and still a judgment call at the end.\nHow else can we check the Mean = Variance condition?\n\n\n5.4.4 Checking for overdispersion using overdispersion factor\nWe can estimate the overdispersion factor. This satisfies\n\\[ \\text{residual variance} = \\text{overdispersion factor} * \\text{mean}\\]\nSo if it’s a lot larger than 1, we have a problem. The function overdisp_fun() in package s245 computes an estimate:\n\nlibrary(s245)\noverdisp_fun(prm)\n\n[1] 2.047036\n\n\nHere, it’s about 2: not too terrible, but still, residual variance is double what it should be according to our model. (Usually if the overdispersion is larger than 2 we will prefer a different model that better accounts for the fact that \\(\\text{mean} \\neq \\text{variance}\\).)"
  },
  {
    "objectID": "count-data-regression.html#accounting-for-overdispersion-negative-binomial-models",
    "href": "count-data-regression.html#accounting-for-overdispersion-negative-binomial-models",
    "title": "5  Regression for Count Data",
    "section": "5.5 Accounting for overdispersion: Negative Binomial Models",
    "text": "5.5 Accounting for overdispersion: Negative Binomial Models\nFinally, we could simply fit a model that allows for a more permissive mean-variance relationship (where the variance can be larger or smaller than the mean, by different amounts), and see if it is a better fit to the data according it our IC. The negative binomial distributions (type I and II) do this:\n\nlibrary(glmmTMB)\nnbm1 &lt;- glmmTMB(novisits ~ flower + treatment +\n               colony, data=beevisits,\n           family=nbinom1(link='log'))\n\nnbm2 &lt;- glmmTMB(novisits ~ flower + treatment +\n               colony, data=beevisits,\n           family=nbinom2(link='log'))\n\nsummary(nbm1)\n\n Family: nbinom1  ( log )\nFormula:          novisits ~ flower + treatment + colony\nData: beevisits\n\n     AIC      BIC   logLik deviance df.resid \n  1182.6   1211.4   -583.3   1166.6      262 \n\n\nDispersion parameter for nbinom1 family (): 1.07 \n\nConditional model:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         0.71730    0.12287   5.838 5.28e-09 ***\nflowernovel        -0.74144    0.14503  -5.112 3.18e-07 ***\nflowertarget        1.03838    0.09810  10.585  &lt; 2e-16 ***\ntreatmentcellulose  0.71563    0.11167   6.408 1.47e-10 ***\ntreatmentsucrose    0.74317    0.11149   6.666 2.63e-11 ***\ncolonyX            -0.11496    0.09974  -1.153 0.249038    \ncolonyY            -0.38387    0.10155  -3.780 0.000157 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(nbm2)\n\n Family: nbinom2  ( log )\nFormula:          novisits ~ flower + treatment + colony\nData: beevisits\n\n     AIC      BIC   logLik deviance df.resid \n  1207.3   1236.1   -595.6   1191.3      262 \n\n\nDispersion parameter for nbinom2 family (): 4.69 \n\nConditional model:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         0.73536    0.11775   6.245 4.24e-10 ***\nflowernovel        -0.72548    0.12693  -5.716 1.09e-08 ***\nflowertarget        1.04379    0.09983  10.456  &lt; 2e-16 ***\ntreatmentcellulose  0.76192    0.11229   6.785 1.16e-11 ***\ntreatmentsucrose    0.76936    0.11424   6.734 1.65e-11 ***\ncolonyX            -0.16805    0.10838  -1.551    0.121    \ncolonyY            -0.51527    0.10761  -4.788 1.68e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can use AIC or BIC-based model selection to decide which model fits best, between Poisson, NB1, and NB2 models.\n\nAIC(prm, nbm1, nbm2)\n\n     df      AIC\nprm   7 1260.621\nnbm1  8 1182.575\nnbm2  8 1207.286\n\n\nWe have a clear winner: NB1!"
  },
  {
    "objectID": "count-data-regression.html#accounting-for-overdispersion-quasi-poisson-model",
    "href": "count-data-regression.html#accounting-for-overdispersion-quasi-poisson-model",
    "title": "5  Regression for Count Data",
    "section": "5.6 Accounting for overdispersion: quasi-Poisson Model",
    "text": "5.6 Accounting for overdispersion: quasi-Poisson Model\nOr yet another option…in our class, we will not use quasi-Poisson models as much, as they are fitted via quasi-likelihood and this complicates model selection.\n\nqprm &lt;- glm(novisits ~ flower + treatment +\n               colony, data=beevisits,\n           family=quasipoisson(link='log'))\nsummary(qprm)\n\n\nCall:\nglm(formula = novisits ~ flower + treatment + colony, family = quasipoisson(link = \"log\"), \n    data = beevisits)\n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.79055    0.12440   6.355 9.14e-10 ***\nflowernovel        -0.75072    0.14940  -5.025 9.32e-07 ***\nflowertarget        0.99945    0.09896  10.100  &lt; 2e-16 ***\ntreatmentcellulose  0.69817    0.11317   6.169 2.58e-09 ***\ntreatmentsucrose    0.70953    0.11399   6.225 1.90e-09 ***\ncolonyX            -0.17521    0.10266  -1.707   0.0891 .  \ncolonyY            -0.43724    0.10460  -4.180 3.97e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 2.047071)\n\n    Null deviance: 1236.33  on 269  degrees of freedom\nResidual deviance:  542.05  on 263  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe should not compare AIC with QAIC to compare two models, so to decide between Poisson and quasi-Poisson (if you ever use it) you must rely on the model assessment plots and the overdispersion factor to decide which is better."
  },
  {
    "objectID": "count-data-regression.html#model-selection-with-dredge-and-qaic-bic",
    "href": "count-data-regression.html#model-selection-with-dredge-and-qaic-bic",
    "title": "5  Regression for Count Data",
    "section": "5.7 Model selection with dredge() and (Q)AIC, BIC",
    "text": "5.7 Model selection with dredge() and (Q)AIC, BIC\nPoisson and negative binomial models are fitted via maximum likelihood, so AIC or BIC may be used for model selection.\nHow can we use model selection criteria in a case where the likelihood can’t be computed exactly?\nThe quasi (log) likelihood is an approximation to the likelihood. It has some properties in common with a (log) likelihood, but is not a likelihood; we resort to using it in cases where the (log) likelihood can not even be evaluated.\nWith some code/mathematical gymnastics, we can use principles of quasi-likelihood to estimate QAIC (quasi-AIC, or AIC based on quasi-likelihood) in R for model selection.\n\n5.7.1 Review: all subsets selection with dredge()\nWhat if, instead of comparing two (or a few) specific models, we want to compare all possible models that contain some combination of a set of candidate covariates? The package MuMIn contains a function, dredge(), that takes as input a “full” model (one containing all the candidate covariates). It then computes AIC (or other model selection criteria) for all possible models containing subsets of the covariate and reports the results in a ranked table. For example, for our Poisson regression model, we could do:\n\nlibrary(MuMIn)\n#have to make sure na.action is 'na.fail' for input model\nprm &lt;- update(prm, na.action='na.fail')\ndredge(prm, rank='AIC')\n\nFixed term is \"(Intercept)\"\n\n\nGlobal model call: glm(formula = novisits ~ flower + treatment + colony, family = poisson(link = \"log\"), \n    data = beevisits, na.action = \"na.fail\")\n---\nModel selection table \n  (Intrc) colny flowr trtmn df   logLik    AIC  delta weight\n8  0.7905     +     +     +  7 -623.310 1260.6   0.00      1\n7  0.6428           +     +  5 -642.317 1294.6  34.01      0\n4  1.3100     +     +        5 -677.205 1364.4 103.79      0\n3  1.1560           +        3 -695.120 1396.2 135.62      0\n6  1.1240     +           +  5 -898.637 1807.3 546.65      0\n5  0.9767                 +  3 -917.644 1841.3 580.67      0\n2  1.6440     +              3 -952.532 1911.1 650.44      0\n1  1.4900                    1 -970.446 1942.9 682.27      0\nModels ranked by AIC(x) \n\ndredge(prm, rank='BIC')\n\nFixed term is \"(Intercept)\"\n\n\nGlobal model call: glm(formula = novisits ~ flower + treatment + colony, family = poisson(link = \"log\"), \n    data = beevisits, na.action = \"na.fail\")\n---\nModel selection table \n  (Intrc) colny flowr trtmn df   logLik    BIC  delta weight\n8  0.7905     +     +     +  7 -623.310 1285.8   0.00      1\n7  0.6428           +     +  5 -642.317 1312.6  26.82      0\n4  1.3100     +     +        5 -677.205 1382.4  96.59      0\n3  1.1560           +        3 -695.120 1407.0 121.23      0\n6  1.1240     +           +  5 -898.637 1825.3 539.46      0\n5  0.9767                 +  3 -917.644 1852.1 566.27      0\n2  1.6440     +              3 -952.532 1921.9 636.05      0\n1  1.4900                    1 -970.446 1946.5 660.68      0\nModels ranked by BIC(x) \n\n\n\n\n5.7.2 Review: IC “weights”\nNote the last two columns of the dredge output: the “delta” (or \\(\\delta\\)) AIC or BIC values and the “weights”. The \\(\\delta\\)s are obtained by simply subtracting the best model’s IC value from that of each other model.\nWe already mentioned a rule of thumb: that the \\(\\delta IC\\) should be at least 3 or to provide reasonable evidence that one model is really better than the other. Another way of measuring the differences between models is to use model weights. Theoretically, these measure the relative likelihoods of different models; you can think of them as giving the probability that a given model is the best-fitting one in the set of models examined, according to the IC being used.\nModel weights are computed simply according to: \\[ e^\\frac{-\\delta IC}{2}\\]\nAnd they sum to one for all models in a dredge().\n\n\n5.7.3 Extending dredge() to quasi-Likelihood\nWith (rather a lot of) effort to define some custom functions, we can do the same thing for a quasi-Poisson model using quasi-AIC. Note: This idea is based on notes by Ben Bolker provided with the R package bbmle.\n\nlibrary(MuMIn)\n\n# modify a glm() output object so that\n# it contains a quasipoisson fit but the \n# AIC (likelihood) from the equivalent regular Poisson model\nx.quasipoisson &lt;- function(...) {\nres &lt;- quasipoisson(...)\nres$aic &lt;- poisson(...)$aic\nres\n}\n\n# function to extract the overdispersion parameter\n# from a quasi model\ndfun &lt;- function(object) {\nwith(object,sum((weights * residuals^2)[weights &gt; 0])/df.residual)\n}\n\n# function that modifies MuMIn::dredge() \n# for use with quasi GLM\nqdredge &lt;- function(model, family='x.quasipoisson', na.action=na.fail, chat = dfun(model), rank='QAIC', ...){\n  model2 &lt;- update(model, family=family, na.action=na.action)\n  (dt &lt;- dredge(model2, rank=rank, chat=chat, ...))\n}\n\n#do \"dredge\" for model selection\nqdredge(qprm)\n\nFixed term is \"(Intercept)\"\n\n\nGlobal model call: glm(formula = novisits ~ flower + treatment + colony, family = family, \n    data = beevisits, na.action = na.action)\n---\nModel selection table \n  (Intrc) colny flowr trtmn df   logLik  rank  delta weight\n8  0.7905     +     +     +  8 -622.310 626.0   0.00  0.999\n7  0.6428           +     +  6 -641.317 640.6  14.57  0.001\n4  1.3100     +     +        6 -676.205 674.7  48.66  0.000\n3  1.1560           +        4 -694.120 688.2  62.16  0.000\n6  1.1240     +           +  6 -897.637 891.0 265.00  0.000\n5  0.9767                 +  4 -916.644 905.6 279.57  0.000\n2  1.6440     +              4 -951.532 939.7 313.65  0.000\n1  1.4900                    2 -969.446 953.2 327.15  0.000\nModels ranked by rank(x, chat = 2.04707063774377) \n\n\nNote: the qdredge() function is also provided for you in the package s245. So if you do want to use this, all you need to do is use s245::qdredge() instead of dredge()."
  },
  {
    "objectID": "count-data-regression.html#offsets",
    "href": "count-data-regression.html#offsets",
    "title": "5  Regression for Count Data",
    "section": "5.8 Offsets",
    "text": "5.8 Offsets\nIn our model, the response variable was the number of flowers visited by a bee, and each bee was in the same experimental setting for the same amount of time, so there was no need to account for effort or time spent in each case.\nThis is not always true: consider, for example:\n\nDolphin surveys\nBird counts\nConsider the schools and crime example from homework as well\n\nIn this case, it would be natural to adjust for effort. The intuitive way to do it would be to use counts per unit effort as the response variable:\n\\[ log(\\frac{\\lambda_i}{effort}) = \\beta_0 + \\dots \\]\nBut notice that this is equivalent to including \\(log(effort)\\) as an ``offset” on the right hand side of the regression equation:\n\\[ log(\\lambda_i) = \\beta_0 + \\dots + log(effort)\\]\nThis is how we specify models with offsets in R:\n\noffset.mod &lt;- glm(counts ~ predictor1 + predictor2 +\n                    offset(log(effort)), family=poisson)\n\nNote: if you use dredge() with a model with an offset, be sure to specify the offset as a “fixed” term, i.e. a term that must be included in all models:\n\ndredge(offset.mod, fixed = 'offset(log(effort))')"
  },
  {
    "objectID": "count-data-regression.html#prediction-plots",
    "href": "count-data-regression.html#prediction-plots",
    "title": "5  Regression for Count Data",
    "section": "5.9 Prediction Plots",
    "text": "5.9 Prediction Plots\nOnce you have a “best” model, how do you interpret it?\nWhen we went to great lengths to make prediction plots for a linear regression model so we could “see” the slope of the predicted relationship, you may have wondered: why bother? I can just look at the slope estimate and get the same insight!\nNow, with a more complicated model equation with a link function, it’s not so easy. Now, we will really appreciate those prediction plots!\nWith the link function and the Poisson distribution, it is more challenging to interpret the coefficients of the model directly. The easiest way to understand the effects of different predictors is to look at plots of model predictions. However, as always we don’t want to plot fitted(model) as a function of each predictor in a model with more than one predictor; predictors other than the one we are interested in will influence the predictions, introducing extra variation. Instead, we will construct a new (fake) dataset to make predictions for, in which all predictors but the one we are interested in are held constant. For example, using our quasi-Poisson model and looking at the effect of colony (predicted values with 95% CIs):\n\nnewdata &lt;- data.frame(colony=c('W', 'X', 'Y'),\n                      flower='familiar',\n                      target.colour='blue',\n                      treatment='cellulose')\n\npred = predict(qprm, newdata=newdata,\n                 type='response',\n                 se.fit=TRUE)\n\nnewdata &lt;- newdata |&gt;\n  mutate(preds = pred$fit,\n         CIlow = pred$fit - 1.96*pred$se.fit,\n         CIup = pred$fit + 1.96*pred$se.fit)\n\ngf_point(preds ~ colony, data=newdata) |&gt;\n  gf_labs(x='Colony', y='Predicted\\nN. Visits') |&gt;\n  gf_errorbar(CIlow + CIup ~ colony, data=newdata)\n\n\n\n\nIf we had a quantitative predictor instead, the process would be similar, except when we define newdata, we would have to choose a range and granularity for which to make predictions. For example, imagine if bee length in mm was one of our predictors, and we wanted predictions for lengths between 0.5 and 3 mm (with a value every 0.05mm). We might begin:\n\nnewdata &lt;- expand.grid(bee.length = seq(from=0.05, by=0.05, to=3),\n                       colony=c('W'),\n                       flower='familiar',\n                      target.colour='blue',\n                      treatment='cellulose')\nhead(newdata)\n\n  bee.length colony   flower target.colour treatment\n1       0.05      W familiar          blue cellulose\n2       0.10      W familiar          blue cellulose\n3       0.15      W familiar          blue cellulose\n4       0.20      W familiar          blue cellulose\n5       0.25      W familiar          blue cellulose\n6       0.30      W familiar          blue cellulose\n\n\nThen, when we make the plot, we would need to use gf\\_ribbon() instead of gf\\_errorbar() to show the CI.\nWe can also use pred_plot(fitted_model, 'variable_name') to make these plots with a lot less code (and if you need to know the values at which other predictors are fixed, use get_fixed(dataset))."
  },
  {
    "objectID": "model-averaging.html#data-school-survey-on-crime-and-safety",
    "href": "model-averaging.html#data-school-survey-on-crime-and-safety",
    "title": "6  Model Averaging",
    "section": "6.1 Data: School Survey on Crime and Safety",
    "text": "6.1 Data: School Survey on Crime and Safety\nThe data for this example are from a survey of U.S. schools, the 2000 School Survey on Crime and Safety. There is information about the study at\nhttp://catalog.data.gov/dataset/2000-school-survey-on-crime-and-safety,\nwhich says the study “is a cross-sectional survey of the nation’s public schools designed to provide estimates of school crime, discipline, disorder, programs and policies. SSOCS is administered to public primary, middle, high, and combined school principals in the spring of even-numbered school years…Public schools were sampled in the spring of 2000 to participate in the study.”\nThe dataset you will use is available online at:\nhttp://sldr.netlify.com/data/sscrime.csv\nIt contains a number of variables:\n\nVisitorCheckIn: Whether visitors to the school must check in to gain entry to the school.\nLockedGates: Whether there are locked gates at the entry to the school.\nMetalDetectors: Whether there is a metal detector at the entrance to the school.\nDrugSniffDog: Whether a drug-sniffing dog is randomly brought into the school to carry out inspections.\nDrugTesting: Whether any drug testing of students occurs.\nUniformsRequired:Whether students are required to wear uniforms.\nDressCode: Whether a strict dress code is enforced.\nLockers: Whether students have lockers.\nStudentIDBadges: Whether students are required to wear ID badges.\nStaffIDBadges: Whether teachers and other staff are required to wear ID badges.\nSecurityCameras: Whether there are security cameras on the premises.\nOfficialRiotPlan: Whether the school has a written plan in place for how to deal with a riot or large-scale fight.\nViolenceReductionProgram: Whether the school has a Violence Reduction Program in place.\nSecurity: Whether security officers are present on the premises.\nTrainingHours: Average amount of time (in hours) that teachers and staff have devoted to training related to violence reduction.\nAttacksWithoutWeapon: Number of attacks that have occurred at the school, not involving a weapon.\nThefts: Number of thefts.\nVandalism: Number of incidents of vandalism.\nViolentIncidentsTotal: Number of violent incidents of all types that have occurred at the school.\nEnrollment: Number of students enrolled in the school (categorical)\nNEnrollment: Number of students enrolled in the school (numeric)\nSurveyRespondent: The identity of the person who filled out the survey.\nLocation: Whether the location of the school is Urban, Rural, etc.\n\n\nssc &lt;- read.csv('http://sldr.netlify.com/data/sscrime.csv')"
  },
  {
    "objectID": "model-averaging.html#modelling-number-of-violent-incidents-per-school",
    "href": "model-averaging.html#modelling-number-of-violent-incidents-per-school",
    "title": "6  Model Averaging",
    "section": "6.2 Modelling number of violent incidents per school",
    "text": "6.2 Modelling number of violent incidents per school\nWe will fit a model for the number of violent incidents total as a function of a number of predictors. This is count data and we will fit a negative binomial regression model:\n\nlibrary(glmmTMB)\nschool.nb2 &lt;- glmmTMB(ViolentIncidentsTotal ~ TrainingHours + Location + \n                        SecurityCameras + DressCode + UniformsRequired + \n                        NEnrollment, data=ssc,\n                family=nbinom2(link='log'),\n                na.action = 'na.fail')\n\nI will use AIC and the dredge() function to compare all possible subsets of my saturated model and figure out which variables should be included in the best model. I chose AIC in this case because it is perhaps more widely used than BIC (that’s not a good reason unless you really have no better one, but there you have it) and because with the relatively small sample size here, I don’t feel a particular need to use BIC for its larger penalty term.\n\nlibrary(MuMIn)\n#do \"dredge\" for model selection\nmod.sel &lt;- dredge(school.nb2, rank='AIC')\nhead(mod.sel, 8)\n\nGlobal model call: glmmTMB(formula = ViolentIncidentsTotal ~ TrainingHours + Location + \n    SecurityCameras + DressCode + UniformsRequired + NEnrollment, \n    data = ssc, family = nbinom2(link = \"log\"), na.action = \"na.fail\", \n    ziformula = ~0, dispformula = ~1)\n---\nModel selection table \n   cnd((Int)) dsp((Int)) cnd(DrC) cnd(Lct)  cnd(NEn) cnd(ScC)  cnd(TrH)\n4       3.610          +        +        +                             \n36      3.585          +        +        +                             \n12      3.635          +        +        +                  +          \n8       3.570          +        +        + 2.537e-05                   \n20      3.608          +        +        +                    0.0008916\n44      3.610          +        +        +                  +          \n40      3.529          +        +        + 3.410e-05                   \n16      3.588          +        +        + 3.140e-05        +          \n   cnd(UnR) df    logLik    AIC delta weight\n4            6 -1745.380 3502.8  0.00  0.287\n36        +  7 -1745.017 3504.0  1.27  0.152\n12           7 -1745.103 3504.2  1.45  0.139\n8            7 -1745.280 3504.6  1.80  0.117\n20           7 -1745.380 3504.8  2.00  0.106\n44        +  8 -1744.734 3505.5  2.71  0.074\n40        +  8 -1744.840 3505.7  2.92  0.067\n16           8 -1744.953 3505.9  3.15  0.060\nModels ranked by AIC(x) \n\n\nBecause the first 7 or so models all have AIC scores within 3 units of each other, it is hard to choose one best model here. In this situation, one way to choose is to pick the model that includes the smallest number of predictors, and still acheives an AIC that is among the best. Another option would be to use model averaging."
  },
  {
    "objectID": "model-averaging.html#model-averaging",
    "href": "model-averaging.html#model-averaging",
    "title": "6  Model Averaging",
    "section": "6.3 Model Averaging",
    "text": "6.3 Model Averaging\nWhat if we wanted to use model averaging to find the best model, instead? We might choose this route because there are several models that all have AIC that are close to each other and thus fit the data approximately equally well. So we might choose to make predictions (and compute coefficients) that are the average of all the models (weighted by IC weights).\nNotes of caution:\n\nIf the model is not a linear regression (if there is a link function for instance) then it’s important to get predictions by averaging the predictions from the different models, not by making predictions using the model-averaged coefficients. The code below is careful to do this.\nModel averaging is used pretty widely but is also controversial (like most model selection methods, in fact!) For example, see: [https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/14-1639.1] and [https://drewtyre.rbind.io/post/rebutting_cade/].\n\nTo do model averaging, we use package MuMIn (function model.avg).\n\n6.3.1 Getting the Averaged Model\nThe following code gets the average model. If we did the default (fit=FALSE), it would be a bit faster, but we would then not be able to get predictions from the model.\n\nmod.sel2 &lt;- dredge(school.nb2)\nave.model &lt;- MuMIn::model.avg(mod.sel2, fit=TRUE)\nsummary(ave.model)\n\n\nCall:\nmodel.avg(object = get.models(object = mod.sel2, subset = NA))\n\nComponent model call: \nglmmTMB(formula = ViolentIncidentsTotal ~ &lt;64 unique rhs&gt;, data = ssc, \n     family = nbinom2(link = \"log\"), ziformula = ~0, dispformula = ~1, \n     na.action = na.fail)\n\nComponent models: \n       df   logLik    AICc delta weight\n12      6 -1745.38 3502.98  0.00   0.24\n126     7 -1745.02 3504.33  1.35   0.12\n124     7 -1745.10 3504.51  1.52   0.11\n123     7 -1745.28 3504.86  1.88   0.09\n125     7 -1745.38 3505.06  2.08   0.08\n1246    8 -1744.73 3505.86  2.87   0.06\n1236    8 -1744.84 3506.07  3.08   0.05\n1234    8 -1744.95 3506.29  3.31   0.05\n1256    8 -1745.02 3506.42  3.44   0.04\n1245    8 -1745.10 3506.59  3.60   0.04\n1235    8 -1745.28 3506.95  3.96   0.03\n12346   9 -1744.49 3507.46  4.48   0.03\n12456   9 -1744.73 3507.95  4.96   0.02\n12356   9 -1744.84 3508.17  5.18   0.02\n12345   9 -1744.95 3508.38  5.40   0.02\n123456 10 -1744.48 3509.56  6.58   0.01\n2       5 -1753.09 3516.33 13.35   0.00\n26      6 -1752.29 3516.80 13.82   0.00\n24      6 -1752.60 3517.42 14.44   0.00\n246     7 -1751.79 3517.87 14.89   0.00\n25      6 -1753.09 3518.40 15.41   0.00\n23      6 -1753.09 3518.40 15.41   0.00\n236     7 -1752.26 3518.82 15.84   0.00\n256     7 -1752.29 3518.87 15.89   0.00\n234     7 -1752.59 3519.48 16.49   0.00\n245     7 -1752.59 3519.49 16.50   0.00\n2346    8 -1751.72 3519.82 16.83   0.00\n2456    8 -1751.78 3519.96 16.97   0.00\n235     7 -1753.09 3520.47 17.49   0.00\n2356    8 -1752.26 3520.90 17.92   0.00\n2345    8 -1752.59 3521.56 18.57   0.00\n23456   9 -1751.71 3521.91 18.92   0.00\n136     5 -1756.63 3523.42 20.43   0.00\n1346    6 -1756.08 3524.39 21.41   0.00\n13      4 -1758.25 3524.60 21.62   0.00\n16      4 -1758.59 3525.28 22.29   0.00\n1356    6 -1756.61 3525.44 22.45   0.00\n134     5 -1757.74 3525.65 22.66   0.00\n1       3 -1759.81 3525.68 22.69   0.00\n13456   7 -1756.08 3526.46 23.48   0.00\n135     5 -1758.21 3526.59 23.60   0.00\n146     5 -1758.30 3526.76 23.78   0.00\n14      4 -1759.53 3527.16 24.17   0.00\n156     5 -1758.54 3527.24 24.26   0.00\n15      4 -1759.75 3527.61 24.62   0.00\n1345    6 -1757.73 3527.69 24.71   0.00\n1456    6 -1758.28 3528.79 25.80   0.00\n145     5 -1759.50 3529.15 26.17   0.00\n36      4 -1765.82 3539.76 36.77   0.00\n346     5 -1764.96 3540.08 37.10   0.00\n6       3 -1767.12 3540.31 37.33   0.00\n46      4 -1766.54 3541.19 38.20   0.00\n356     5 -1765.77 3541.70 38.71   0.00\n3456    6 -1764.95 3542.12 39.14   0.00\n56      4 -1767.04 3542.18 39.20   0.00\n(Null)  2 -1769.34 3542.72 39.74   0.00\n3       3 -1768.50 3543.05 40.07   0.00\n456     5 -1766.50 3543.17 40.18   0.00\n34      4 -1767.71 3543.52 40.54   0.00\n4       3 -1768.77 3543.61 40.63   0.00\n5       3 -1769.24 3544.54 41.55   0.00\n35      4 -1768.41 3544.93 41.95   0.00\n345     5 -1767.68 3545.51 42.53   0.00\n45      4 -1768.72 3545.54 42.55   0.00\n\nTerm codes: \n       cond(DressCode)         cond(Location)      cond(NEnrollment) \n                     1                      2                      3 \n cond(SecurityCameras)    cond(TrainingHours) cond(UniformsRequired) \n                     4                      5                      6 \n\nModel-averaged coefficients:  \n(full average) \n                             Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)\ncond((Int))                 3.596e+00  1.316e-01   1.320e-01  27.252  &lt; 2e-16\ncond(DressCodeyes)          3.765e-01  9.608e-02   9.639e-02   3.906 9.39e-05\ncond(LocationRural)        -5.597e-01  1.328e-01   1.332e-01   4.201 2.65e-05\ncond(LocationTown)         -5.874e-01  1.543e-01   1.548e-01   3.795 0.000148\ncond(LocationUrban Fringe) -1.024e-01  1.179e-01   1.183e-01   0.866 0.386523\ncond(UniformsRequiredyes)   5.685e-02  1.373e-01   1.376e-01   0.413 0.679432\ncond(SecurityCamerasyes)   -2.384e-02  6.451e-02   6.466e-02   0.369 0.712383\ncond(NEnrollment)           8.826e-06  2.812e-05   2.818e-05   0.313 0.754163\ncond(TrainingHours)        -2.156e-04  2.509e-02   2.517e-02   0.009 0.993166\n                              \ncond((Int))                ***\ncond(DressCodeyes)         ***\ncond(LocationRural)        ***\ncond(LocationTown)         ***\ncond(LocationUrban Fringe)    \ncond(UniformsRequiredyes)     \ncond(SecurityCamerasyes)      \ncond(NEnrollment)             \ncond(TrainingHours)           \n \n(conditional average) \n                             Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)\ncond((Int))                 3.596e+00  1.316e-01   1.320e-01  27.252  &lt; 2e-16\ncond(DressCodeyes)          3.771e-01  9.499e-02   9.530e-02   3.956 7.61e-05\ncond(LocationRural)        -5.598e-01  1.328e-01   1.332e-01   4.203 2.63e-05\ncond(LocationTown)         -5.874e-01  1.542e-01   1.547e-01   3.796 0.000147\ncond(LocationUrban Fringe) -1.024e-01  1.179e-01   1.183e-01   0.866 0.386505\ncond(UniformsRequiredyes)   1.661e-01  1.921e-01   1.927e-01   0.862 0.388948\ncond(SecurityCamerasyes)   -7.409e-02  9.597e-02   9.628e-02   0.770 0.441588\ncond(NEnrollment)           3.049e-05  4.550e-05   4.565e-05   0.668 0.504120\ncond(TrainingHours)        -8.265e-04  4.912e-02   4.928e-02   0.017 0.986617\n                              \ncond((Int))                ***\ncond(DressCodeyes)         ***\ncond(LocationRural)        ***\ncond(LocationTown)         ***\ncond(LocationUrban Fringe)    \ncond(UniformsRequiredyes)     \ncond(SecurityCamerasyes)      \ncond(NEnrollment)             \ncond(TrainingHours)           \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you are trying to get model-averaged coefficients from the summary output above, be sure to look for the “full average” ones and not the “conditional average” (which only includes models where the predictor was included, i.e., where the coefficient was not 0).\n\n\n6.3.2 Getting Predictions from the Averaged Model\n\nma.preds &lt;- predict(ave.model, se.fit=TRUE,\n                    type = 'response', \n                    backtransform = FALSE)\n\nThe resulting predictions are a list with entries fit and se.fit just like we are used to. (So you could make predictions with a newdata data set and use them for prediction plots, for example. Be careful – your “new” dataset now has to include values for all candidate predictors in the full model.)\nComparing with the predictions from our previous “best” model:\n\nbest.school.nb2 &lt;- glmmTMB(ViolentIncidentsTotal ~ DressCode + Location,\n                           data=ssc,family=nbinom2(link='log'))\n\n\n# pred_plot(ave.model, 'DressCode', ylab = 'N. Incidents',\n#             data = ssc, color = 'red')\n# \n# pred_plot(best.school.nb2, 'DressCode', ylab = 'N. Incidents',) \n\nSo they are pretty comparable, but a little different (the differences may be bigger the more there are different models with similar IC results contributing to the average model – when one model carries almost all the weight, then the “single best” model and the model-averaging model will give almost the same results). It also makes sense that there will be a bit more uncertainty in the average model."
  },
  {
    "objectID": "interactions.html#example-quantitative-categorical-interaction",
    "href": "interactions.html#example-quantitative-categorical-interaction",
    "title": "7  Interactions",
    "section": "7.1 Example: Quantitative-Categorical Interaction",
    "text": "7.1 Example: Quantitative-Categorical Interaction\n\ngf_point(eval ~ beauty, color = ~female, shape = ~female, data = teach_beauty) |&gt;\n  gf_lm()\n\n\n\n\n\n\n\n\nEval may go up as beauty increases, but the slope of the relationship is different for females and non-females. This is an interaction between beauty and female."
  },
  {
    "objectID": "interactions.html#categorical-categorical-interaction-example",
    "href": "interactions.html#categorical-categorical-interaction-example",
    "title": "7  Interactions",
    "section": "7.2 Categorical-Categorical Interaction Example",
    "text": "7.2 Categorical-Categorical Interaction Example\n\ngf_boxplot(eval ~ formal | female, data = teach_beauty)\n\n\n\n\n\n\n\n\nPerhaps Informal Dress affects eval scores, but really only for non-females – for females, formal dress doesn’t make a difference either way.\nThe effect of formal dress is different depending on the value of female. This is an interaction between formal and female."
  },
  {
    "objectID": "interactions.html#quant-quant-interactions",
    "href": "interactions.html#quant-quant-interactions",
    "title": "7  Interactions",
    "section": "7.3 Quant-Quant interactions?",
    "text": "7.3 Quant-Quant interactions?\nYes, these are possible, but very hard to visualize and conceptualize. Basically, it would mean that the slope of the line for one predictor changes gradually as the value of a second variable changes."
  },
  {
    "objectID": "interactions.html#r-code",
    "href": "interactions.html#r-code",
    "title": "7  Interactions",
    "section": "7.4 R code",
    "text": "7.4 R code\nIf you want to include an interaction term in a model in R, use a * rather than a + between the predictors that (may) interact. For example, based on our exploration above, we might try:\n\nbeauty_mod &lt;- lm(eval ~ beauty*female +\n                   formal*female, \n                 data = teach_beauty, \n                 na.action = 'na.fail')\nsummary(beauty_mod)\n\n\nCall:\nlm(formula = eval ~ beauty * female + formal * female, data = teach_beauty, \n    na.action = \"na.fail\")\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83418 -0.36763  0.04966  0.39789  1.07161 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                            3.84839    0.10813  35.591   &lt;2e-16 ***\nbeauty                                 0.09021    0.04744   1.902   0.0578 .  \nfemalenot female                       0.27615    0.13130   2.103   0.0360 *  \nformalInformal Dress                   0.05751    0.11574   0.497   0.6195    \nbeauty:femalenot female                0.10841    0.06452   1.680   0.0936 .  \nfemalenot female:formalInformal Dress -0.08378    0.14276  -0.587   0.5576    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5371 on 457 degrees of freedom\nMultiple R-squared:  0.07326,   Adjusted R-squared:  0.06312 \nF-statistic: 7.226 on 5 and 457 DF,  p-value: 1.587e-06\n\n\nNotice the additional indicator variables in the coefficient table/model equation. Now we need to adjust the effects of the beauty predictor depending on the values of formal and female, which interact with it.\nWe can use IC-based model selection to determine whether including these interactions in a model is important or not.\n\nlibrary(MuMIn)\ndredge(beauty_mod, rank = 'AIC')\n\nGlobal model call: lm(formula = eval ~ beauty * female + formal * female, data = teach_beauty, \n    na.action = \"na.fail\")\n---\nModel selection table \n   (Int)     bty fml frm bty:fml fml:frm df   logLik   AIC delta weight\n12 3.899 0.08762   +           +          5 -366.309 742.6  0.00  0.414\n4  3.897 0.14860   +                      4 -367.868 743.7  1.12  0.236\n16 3.896 0.08773   +   +       +          6 -366.308 744.6  2.00  0.152\n8  3.897 0.14860   +   +                  5 -367.868 745.7  3.12  0.087\n32 3.848 0.09021   +   +       +       +  7 -366.134 746.3  3.65  0.067\n24 3.833 0.14880   +   +               +  6 -367.560 747.1  4.50  0.044\n2  4.010 0.13300                          3 -375.323 756.6 14.03  0.000\n6  4.032 0.13170       +                  4 -375.249 758.5 15.88  0.000\n3  3.901           +                      3 -378.503 763.0 20.39  0.000\n7  3.932           +   +                  4 -378.367 764.7 22.12  0.000\n23 3.872           +   +               +  5 -378.103 766.2 23.59  0.000\n1  3.998                                  2 -383.747 771.5 28.88  0.000\n5  4.044               +                  3 -383.431 772.9 30.24  0.000\nModels ranked by AIC(x) \n\n\nIn the case of the particular model we fitted, the “best” model starting from this full model is actually one without interactions. If you want to explore the dataset further, you will find that actually a model where age, beauty AND female interact fits much better…"
  },
  {
    "objectID": "interactions.html#cautionary-note",
    "href": "interactions.html#cautionary-note",
    "title": "7  Interactions",
    "section": "7.5 Cautionary note",
    "text": "7.5 Cautionary note\nIf you include an interaction in a regression model, you must also include the corresponding “fixed effects” – this means if you have an indicator variable/slope term for an interaction in your model, you must also have the indicator variables/slopes corresponding to the individual predictors. Our fitting functions (lm(), glm(), glmmTMB(), etc.) are smart enough to ensure this for you. So is dredge(). (It would take effort to mess this up in R.)"
  },
  {
    "objectID": "binary-regression.html#data-source",
    "href": "binary-regression.html#data-source",
    "title": "8  Binary Regression",
    "section": "8.1 Data Source",
    "text": "8.1 Data Source\nThe dataset used here is on Alaskan wood frogs, detailing some physical characteristics, habitat characteristics, and the number of developmental and other abnormalities found in the frogs. It was originally obtained from:[http://datadryad.org/resource/doi:10.5061/dryad.sq72d].\nThe data file can be accessed online at: [http://sldr.netlify.com/data/FrogAbnormalities.csv]\n\n\n  FrogID TotalLength TailLength    Stage Year RoadDistance RoadType Abnormal\n1     25    19.19701 31.8285559 Stage 43 2011           22   Gravel       No\n2     29    20.99035  0.1151022 Stage 46 2012          387   Gravel       No\n3     29    20.50364  0.1055217 Stage 46 2011          781    Paved       No\n  SkeletalAbnormality EyeAbnormality orig.id\n1                  No             No    3450\n2                  No             No    2276\n3                  No             No     158"
  },
  {
    "objectID": "binary-regression.html#logistic-regression",
    "href": "binary-regression.html#logistic-regression",
    "title": "8  Binary Regression",
    "section": "8.2 Logistic Regression",
    "text": "8.2 Logistic Regression\nRecall, for linear regression we fitted a model for continuous (numeric) response variable \\(y\\) according to:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...\\beta_k x_k + \\epsilon \\]\nwhere \\(x\\)s are the \\(k\\) predictor variables, \\(\\beta\\)s are the parameters to be estimated by the model, and \\(\\epsilon \\sim N(0,\\sigma)\\) are the model residuals.\nWhen our response variable was a count variable, we modified our equation to:\n\\[log(\\lambda_i) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...\\beta_k x_k + \\epsilon_{link} \\]\npositing that \\(y_i \\sim Pois(\\lambda_i)\\) for Poisson regression; similarly for quasiPoisson or negative binomial regression, we just replaced that Poisson distribution with a quasiPoisson or a negative binomial distribution.\nWhat if our response variable is logical – a categorical variable with just two possible values? We will designate one of the two values a “success,” and then we want to predict the probability of success as a function of some set of predictors. What will our model equation look like in this case?\n\\[ logit(p_i) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...\\beta_k x_k + \\epsilon_{link}\\]\nwhere the logit function is \\(logit(x) = log(\\frac{x}{1-x})\\). This function maps probabilities to positive and negative real numbers, effectively “spreading them out” from the range 0-1 to the full range of real numbers. How does this equation relate back to our desired response variable? Well, \\(i\\)th observation of the response variable is assumed to follow a binomial distribution with probability \\(p_i\\) (\\(y_i \\sim Binom(n_i, p_i)\\)). (\\(n_i\\) depends on the setup of the data – often n=1 for each row of the dataset, as here where each row is one frog. We can think of each frog as one binomial trial, with success/failure meaning abnormality/normality of the frog.)"
  },
  {
    "objectID": "binary-regression.html#checking-the-data-setup",
    "href": "binary-regression.html#checking-the-data-setup",
    "title": "8  Binary Regression",
    "section": "8.3 Checking the data setup",
    "text": "8.3 Checking the data setup\nWe would like to model the proportion frogs with abnormalities as a function of a set of covariates. The variable Abnormal has values “Yes” and “No”. In R, if we use this (factor) variable as our response, how will R determine which level (value of the variable) is a “success”?\nR uses the FIRST variable value as “failure” and the second as “success” – this makes sense if you imagine coding 0 for failure and 1 for success (and then sorting in numeric/alphabetical order). If you have a categorical variable with informative values, you will need to make sure that the “base” (first) level is the one you want to equate with “failure”.\n\nlevels(frogs$Abnormal)\n\nNULL\n\n\nIf you do need to rearrange the levels, one way to do it is to use the forcats::fct_relevel() function. Example:\n\n#ref will be the FIRST level after releveling\nfrogs &lt;- frogs |&gt; \n  mutate(Abnormal = forcats::fct_relevel(Abnormal, 'No'))\nfrogs |&gt; pull(Abnormal) |&gt; levels()\n\n[1] \"No\"  \"Yes\""
  },
  {
    "objectID": "binary-regression.html#fitting-a-saturated-model",
    "href": "binary-regression.html#fitting-a-saturated-model",
    "title": "8  Binary Regression",
    "section": "8.4 Fitting a saturated model",
    "text": "8.4 Fitting a saturated model\nLet’s try fitting a model for Abnormalities as a function of Stage, Year, RoadType, and RoadDistance. Why do you think these variables and not others were chosen?\nPerhaps it makes sense that type and distance to road are proxies for urbanization, and frogs may do better in more pristine habitats. It also seems likely that there would be differences over time. There may also be differences by Stage, if frogs with severe abnormalities have trouble even surviving to the later/older stages.\n\nfrog.logr &lt;- glm(Abnormal ~ Stage + factor(Year) + RoadType + RoadDistance,\n                 data=frogs, family=binomial(link='logit'))\nsummary(frog.logr)\n\n\nCall:\nglm(formula = Abnormal ~ Stage + factor(Year) + RoadType + RoadDistance, \n    family = binomial(link = \"logit\"), data = frogs)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -0.3509350  0.4198798  -0.836 0.403268    \nStageStage 43     0.1129785  0.1136966   0.994 0.320376    \nStageStage 44    -0.0987133  0.1290558  -0.765 0.444338    \nStageStage 45    -0.4616044  0.1301337  -3.547 0.000389 ***\nStageStage 46    -0.5954533  0.1451580  -4.102 4.09e-05 ***\nfactor(Year)2001 -1.4231818  0.8393630  -1.696 0.089971 .  \nfactor(Year)2002 -1.3820162  1.1210725  -1.233 0.217664    \nfactor(Year)2003 -0.9744573  0.5216890  -1.868 0.061778 .  \nfactor(Year)2004 -1.4345710  0.4272057  -3.358 0.000785 ***\nfactor(Year)2005 -1.1185067  0.4214799  -2.654 0.007960 ** \nfactor(Year)2006 -1.1232649  0.4229028  -2.656 0.007905 ** \nfactor(Year)2010 -0.4985424  0.4299638  -1.159 0.246253    \nfactor(Year)2011 -1.1674055  0.4163277  -2.804 0.005046 ** \nfactor(Year)2012 -1.0488208  0.4157733  -2.523 0.011650 *  \nRoadTypePaved    -0.1689634  0.0842935  -2.004 0.045020 *  \nRoadDistance      0.0005653  0.0002471   2.287 0.022176 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4787.5  on 5370  degrees of freedom\nResidual deviance: 4701.4  on 5355  degrees of freedom\nAIC: 4733.4\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "binary-regression.html#link-functions",
    "href": "binary-regression.html#link-functions",
    "title": "8  Binary Regression",
    "section": "8.5 Link Functions",
    "text": "8.5 Link Functions\nHere, we have used the logit link function, which is the most common. However, there are other functions that translate proportions to real numbers, and are sometimes used in regression for binary data. Two common options are:\n\nProbit regression: link=‘probit’\nComplementary log-log regression: link=‘cloglog’\n\nThere are not closed-form expressions for the the probit and complementary log-log functions that are easy to write down, so that is why the exact functions are not given here. As shown below, the shapes of these three functions are very similar. So it may come as no big surprise that frequently they provide similar goodness of fit to data (according to IC). If that is the case, choose logit (which makes some of the interpretation of results easier).\n\n\n\n\n\nNote: figure is from [http://data.princeton.edu/wws509/notes]."
  },
  {
    "objectID": "binary-regression.html#conditions",
    "href": "binary-regression.html#conditions",
    "title": "8  Binary Regression",
    "section": "8.6 Conditions",
    "text": "8.6 Conditions\nUnder what conditions is a logistic regression model appropriate?\n\nResponse variable is logical – you can characterize it as the outcome of a binomial trial (or a set of independent binomial trials). Some response variables can be expressed as proportions, but can not be well modelled with binomial regression. For example, you might take one-minute recordings in the woods and measure the proportion of each minute during which bird song was audible. The data will look like proportions, but you can’t think of them as binomial trials and should not model them with binomial regression (what is a “trial” here, and what is a “success”? Make sure you can answer those questions before using binomial regression.)\nLinearity: logit(p) should have a linear relationship with each predictor variable. (A bit hard to check - see solutions to HW8 for an example of how it can be done.)\nIndependence: Same as usual.\nMean-variance relationship: The Pearson or Deviance residuals will decrease as a function of fitted value, and should have approximately constant variance as a function of fitted value. But a residuals vs fitted plot is of almost no use to us – the examples later on show how you can expect it to look, and if it deviates from the expected appearance, try to figure out why and what is going on; but if it looks as expected, you can say “there is no evidence in this figure of a violation of the conditions of binary regression.”\nNO distributional assumptions about residuals."
  },
  {
    "objectID": "binary-regression.html#model-assessment-plots",
    "href": "binary-regression.html#model-assessment-plots",
    "title": "8  Binary Regression",
    "section": "8.7 Model Assessment Plots",
    "text": "8.7 Model Assessment Plots\n\n\n\n\n\n\ngf_point(resid(frog.logr, type='pearson') ~ fitted(frog.logr)) |&gt;\n  gf_labs(title='frog.logr', \n          y=' Pearson\\nResiduals',x='Fitted Values')\n\ngf_point(resid(frog.logr, type='deviance') ~ fitted(frog.logr)) |&gt;\n  gf_labs(title='frog.logr', \n          y=' Deviance\\nResiduals',x='Fitted Values')\n\ngf_point(resid(frog.logr, type='response') ~ fitted(frog.logr)) |&gt;\n  gf_labs(title='frog.logr', \n          y=' Raw Response\\nResiduals',x='Fitted Values')\n\ngf_histogram(~resid(frog.logr, type='pearson'), bins=15) |&gt;\n  gf_labs(title='frog.logr', \n          x='Residuals', y='Count')\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two “lines” in the residuals vs fitted plots correspond with the two possible values of the response variable in the data.\nAnd remember - there is not a strict distributional assumption about the residuals (in other words, they don’t have to follow, say, a normal distribution), so we don’t really have to make a histogram of them. The one here is shown just to help you remember that you don’t have to check it, and if you do, it will look “strange” (bimodal like this) yet it is nothing to worry about."
  },
  {
    "objectID": "binary-regression.html#odds-ratios",
    "href": "binary-regression.html#odds-ratios",
    "title": "8  Binary Regression",
    "section": "8.8 Odds Ratios",
    "text": "8.8 Odds Ratios\nThe odds (or odds ratio) is \\(\\frac{p}{1-p}\\) – the ratio of success to failure. So if P(success) = 0.75, then the odds will be \\(\\frac{0.75}{0.25}\\) = 3 or “three to one” – you will usually succeed three times for every failure.\nRemember, the logit function was \\(logit(x) = log(\\frac{p}{1-p})\\)? In other words, the logit is the log of the odds ratio. This means that the coefficients of a binary regression model with logit link function have special interpretations in terms of odds ratios.\nLet’s consider a simplified version of our model (just to make it easier to write out the model equation):\n\nsimple &lt;- glm(Abnormal ~ Stage, data = frogs, family=binomial(link='logit'))\ncoef(simple)\n\n  (Intercept) StageStage 43 StageStage 44 StageStage 45 StageStage 46 \n   -1.4618893     0.1001146    -0.1094112    -0.4527238    -0.5119712 \n\n\nSo our model equation is:\n\\[ logit(p_i) = -1.46 + 0.10I_{s43} - 0.11I_{s44} - 0.45I_{s45} - 0.51I_{s46}\\]\nAccording to this model, the log-odds (logit(p)) for a Stage 42 frog is -1.46, so the odds of being Abnormal for a Stage 42 frog are \\(e^{-1.46} = 0.23\\).\nThe log-odds for a Stage 46 frog are -1.46 - 0.51 = -1.97, so the odds of it being Abnormal are \\(e^{-1.97} = 0.14\\).\nThe change in odds going from Stage 42 to 46 is then \\(\\frac{0.14}{0.23} = 0.61\\) – the odds of a Stage 42 frog being abnormal are nearly double those of a Stage 46 frog.\nNotice – we didn’t actually have to compute all that to find the 0.6 value!\nWe know that for Stage 46\n\\[ log(\\frac{p}{1-p}) = -1.46 -0.51\\] so\n\\[ \\frac{p}{1-p} = e^{-1.46 - 0.51} = e^{-1.46}e^{-0.51}\\]\nAnd \\(e^{-1.46}\\) is the odds for Stage 42…aha! So, \\(e^{-0.51} = 0.60\\) is the multiplier on the odds ratio to go from stage 42 to 46. And in general, \\(e^{\\beta}\\) is the multiplier on the odds ratio for a one-unit change in the predictor variable for which \\(\\beta\\) is the model coefficient."
  },
  {
    "objectID": "binary-regression.html#model-selection",
    "href": "binary-regression.html#model-selection",
    "title": "8  Binary Regression",
    "section": "8.9 Model Selection",
    "text": "8.9 Model Selection\nAs usual:\n\nlibrary(MuMIn)\nfrog.logr &lt;- update(frog.logr, na.action='na.fail')\nmod.sel &lt;- dredge(frog.logr, rank='AIC')\n\nFixed term is \"(Intercept)\"\n\nhead(mod.sel,5)\n\nGlobal model call: glm(formula = Abnormal ~ Stage + factor(Year) + RoadType + RoadDistance, \n    family = binomial(link = \"logit\"), data = frogs, na.action = \"na.fail\")\n---\nModel selection table \n     (Int) fct(Yer)       RdD RdT Stg df    logLik    AIC delta weight\n16 -0.3509        + 0.0005653   +   + 16 -2350.719 4733.4  0.00  0.590\n12 -0.3426        + 0.0005161       + 15 -2352.740 4735.5  2.04  0.213\n14 -0.3749        +             +   + 15 -2353.232 4736.5  3.03  0.130\n10 -0.3653        +                 + 14 -2354.889 4737.8  4.34  0.067\n15 -1.4510          0.0004694   +   +  7 -2368.278 4750.6 17.12  0.000\nModels ranked by AIC(x) \n\nafm &lt;- model.avg(mod.sel, fit=TRUE)\ncoef(afm)\n\n     (Intercept) factor(Year)2001 factor(Year)2002 factor(Year)2003 \n   -0.3535582013    -1.4237089763    -1.3808976121    -0.9748675931 \nfactor(Year)2004 factor(Year)2005 factor(Year)2006 factor(Year)2010 \n   -1.4331851168    -1.1210426509    -1.1258664109    -0.5125316516 \nfactor(Year)2011 factor(Year)2012     RoadDistance    RoadTypePaved \n   -1.1735417137    -1.0548360421     0.0005522245    -0.1658639112 \n   StageStage 43    StageStage 44    StageStage 45    StageStage 46 \n    0.1093083878    -0.1003833764    -0.4577677670    -0.5886749698"
  },
  {
    "objectID": "binary-regression.html#prediction-plots",
    "href": "binary-regression.html#prediction-plots",
    "title": "8  Binary Regression",
    "section": "8.10 Prediction Plots",
    "text": "8.10 Prediction Plots\nShown here are example prediction plots for Stage and RoadDistance. First, check out a summary table for the variables in the model to help determine fixed values.\n\nfrogs$Year &lt;- factor(frogs$Year)\nsummary(frogs[,c('RoadDistance', 'RoadType', 'Stage',\n                 'Year')])\n\n  RoadDistance      RoadType            Stage                Year     \n Min.   :  3.00   Length:5371        Length:5371        2012   :1561  \n 1st Qu.: 15.00   Class :character   Class :character   2011   :1433  \n Median : 31.00   Mode  :character   Mode  :character   2005   : 683  \n Mean   : 99.38                                         2006   : 647  \n 3rd Qu.: 83.00                                         2004   : 605  \n Max.   :781.00                                         2010   : 300  \n                                                        (Other): 142  \n\n\n\nlibrary(s245)\n# pred_plot(afm, 'Stage', data = frogs)\n\n\n# pred_plot(afm, 'RoadDistance', data = frogs)\n\nHow does this compare to the raw data?\n\ntally(~Abnormal|Stage, data=frogs, format='prop')\n\n        Stage\nAbnormal  Stage 42  Stage 43  Stage 44  Stage 45  Stage 46\n     No  0.8118215 0.7960480 0.8279689 0.8715365 0.8780252\n     Yes 0.1881785 0.2039520 0.1720311 0.1284635 0.1219748\n\nbins &lt;- cut(frogs$RoadDistance,breaks=c(0, 25, 50, 100, 250,800))\nprop(~Abnormal=='Yes'|bins, data=frogs)\n\n   prop_TRUE.(0,25]   prop_TRUE.(25,50]  prop_TRUE.(50,100] prop_TRUE.(100,250] \n          0.1516080           0.1925424           0.1379747           0.1694215 \nprop_TRUE.(250,800] \n          0.1613876 \n\n\nBut…remember, in the raw data, other predictors may also be influencing the patterns that you see in the data. In addition, we can look at the width of the confidence bands on the model estimates, and look at the model selection results to get an idea of whether this predictor is really important in the model or not. This is just an example to get you thinking about what prediction plots are showing you and what you can do with them!"
  },
  {
    "objectID": "binary-regression2.html#data",
    "href": "binary-regression2.html#data",
    "title": "9  Binary regression: Data with more than one trial per row",
    "section": "9.1 Data",
    "text": "9.1 Data\nThe dataset used here is a reality-based simulated EIA dataset on duck sightings before and after windfarm installation (impact). Hourly, observers did up to 200 scans of the study area, and for each scan recorded whether duck(s) were seen (a success) or not (a failure).\nThe data file can be accessed online at:\\ \n\n\n  successes trials day month impact\n1       172    200   7     1      0\n2       190    200  13     1      0\n3       191    200  27     1      0"
  },
  {
    "objectID": "binary-regression2.html#checking-the-data-setup",
    "href": "binary-regression2.html#checking-the-data-setup",
    "title": "9  Binary regression: Data with more than one trial per row",
    "section": "9.2 Checking the data setup",
    "text": "9.2 Checking the data setup\nWe would like to model the proportion scans with ducks sighted as a function of a set of covariates. Each row of our dataset gives us the number of successes in some number of trials (and also gives the corresponding values of the covariates for ALL those trials). We can also use this kind of summary data with a logistic regression; we will just need to add a column for the number of failures:\n\npd &lt;- pd |&gt; mutate(failures = trials-successes)\n#or same thing in base R\npd$failures = pd$trials - pd$successes"
  },
  {
    "objectID": "binary-regression2.html#fitting-a-saturated-model",
    "href": "binary-regression2.html#fitting-a-saturated-model",
    "title": "9  Binary regression: Data with more than one trial per row",
    "section": "9.3 Fitting a saturated model",
    "text": "9.3 Fitting a saturated model\nLet’s try fitting a model for proportion sightings as a function of day, month, and impact.\nWe need a response “variable” that is really 2 variables bound together: a column with the “successes” and a column with the “failures”. These don’t have to be literally called successes and failures – you can use whatever variable names you like – but the first one of the two should be successes (the thing you want to compute the proportion for) and the second failures.\n\nduck.logr &lt;- glm( cbind(successes, failures) ~ day + month + impact, \n                  family=binomial(link='logit'),\n                  data=pd)\nsummary(duck.logr)\n\n\nCall:\nglm(formula = cbind(successes, failures) ~ day + month + impact, \n    family = binomial(link = \"logit\"), data = pd)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.770286   0.077096  35.933  &lt; 2e-16 ***\nday         -0.011727   0.003564  -3.290    0.001 ** \nmonth       -0.091546   0.007819 -11.708  &lt; 2e-16 ***\nimpact      -0.222642   0.048407  -4.599 4.24e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 488.19  on 71  degrees of freedom\nResidual deviance: 244.96  on 68  degrees of freedom\nAIC: 607.85\n\nNumber of Fisher Scoring iterations: 4\n\n#or maybe...\nduck.logr2 &lt;- glm( cbind(successes, failures) ~ day + factor(month) + impact, \n                   family=binomial(link='logit'), \n                   data=pd)\nsummary(duck.logr2)\n\n\nCall:\nglm(formula = cbind(successes, failures) ~ day + factor(month) + \n    impact, family = binomial(link = \"logit\"), data = pd)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      2.618402   0.123703  21.167  &lt; 2e-16 ***\nday             -0.011735   0.004058  -2.892 0.003829 ** \nfactor(month)2  -0.297341   0.135091  -2.201 0.027734 *  \nfactor(month)3  -0.014290   0.141766  -0.101 0.919712    \nfactor(month)4  -0.287869   0.135761  -2.120 0.033971 *  \nfactor(month)5  -0.258362   0.141490  -1.826 0.067850 .  \nfactor(month)6  -0.404561   0.132914  -3.044 0.002336 ** \nfactor(month)7  -0.141972   0.139208  -1.020 0.307799    \nfactor(month)8  -0.452317   0.132294  -3.419 0.000628 ***\nfactor(month)9  -0.632855   0.128257  -4.934 8.05e-07 ***\nfactor(month)10 -0.665992   0.129788  -5.131 2.88e-07 ***\nfactor(month)11 -1.136006   0.123841  -9.173  &lt; 2e-16 ***\nfactor(month)12 -0.949485   0.127997  -7.418 1.19e-13 ***\nimpact          -0.223631   0.048515  -4.609 4.04e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 488.19  on 71  degrees of freedom\nResidual deviance: 197.84  on 58  degrees of freedom\nAIC: 580.73\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "binary-regression2.html#checking-linearity",
    "href": "binary-regression2.html#checking-linearity",
    "title": "9  Binary regression: Data with more than one trial per row",
    "section": "9.4 Checking linearity",
    "text": "9.4 Checking linearity\nWhat should be linear here? Well, logit(p) (where p is the probability of success, for a given set of predictor-variable values) should be a linear function of the predictors. We can actually check this graphically now that we have multiple trials per row of data! (But remember that the effects of other, unplotted predictors may also be influencing the plot that you see…)\nHere, we need to decide: Do we see a linear pattern (or no pattern)? For the month and day data here, we might also consider whetehr it would make more sense to fit either of them as a categorical covariate rather than numeric.\n\ngf_point(logit(successes/trials) ~ day, data=pd) |&gt;\n  gf_labs(y='logit(p)')\ngf_point(logit(successes/trials) ~ month, data=pd)|&gt;\n  gf_labs(y='logit(p)')"
  },
  {
    "objectID": "binary-regression2.html#model-assessment",
    "href": "binary-regression2.html#model-assessment",
    "title": "9  Binary regression: Data with more than one trial per row",
    "section": "9.5 Model Assessment",
    "text": "9.5 Model Assessment\nWith data set up as proportions (many trials with the number of successes and failures in each row, rather than one row per trial), model assessment plots are a bit more useful. Specifically, we can check the Pearson residuals vs. fitted plot for constant variance as a function of fitted value, to confirm that the mean-variance relationship matches what we expect.\nSince the Pearson residuals are already adjusted for the expected variance, we should see approximately constant spread, with values ranging from about -2 to 2 (and not more than a few larger than \\(\\pm\\) 3).\n\nacf(resid(duck.logr2, type='pearson'))\ngf_point(resid(duck.logr2, type='pearson') ~ fitted(duck.logr2))"
  },
  {
    "objectID": "binary-regression2.html#model-selection",
    "href": "binary-regression2.html#model-selection",
    "title": "9  Binary regression: Data with more than one trial per row",
    "section": "9.6 Model Selection",
    "text": "9.6 Model Selection\nWe can do model selection as usual. Here, it looks like the best model is the saturated (full) model.\n\nlibrary(MuMIn)\nduck.logr2 &lt;- update(duck.logr2, na.action='na.fail')\ndredge(duck.logr2)\n\nFixed term is \"(Intercept)\"\n\n\nGlobal model call: glm(formula = cbind(successes, failures) ~ day + factor(month) + \n    impact, family = binomial(link = \"logit\"), data = pd, na.action = \"na.fail\")\n---\nModel selection table \n  (Int)      day fct(mnt)     imp df   logLik  AICc  delta weight\n8 2.618 -0.01173        + -0.2236 14 -276.365 588.1   0.00  0.934\n7 2.430                 + -0.2235 13 -280.560 593.4   5.30  0.066\n4 2.501 -0.01172        +         13 -287.029 606.3  18.24  0.000\n3 2.314                 +         12 -291.218 611.7  23.63  0.000\n6 2.444 -0.02848          -0.2204  3 -371.770 749.9 161.80  0.000\n2 2.328 -0.02844                   2 -382.278 768.7 180.63  0.000\n5 1.918                   -0.2192  2 -411.086 826.3 238.25  0.000\n1 1.804                            1 -421.538 845.1 257.04  0.000\nModels ranked by AICc(x) \n\n\nWe might also try using model selection to help us decide whether to use quantitative or categorical month and/or day…\n\nduck.logr2 &lt;- update(duck.logr2, formula= . ~ . + month + factor(day), na.action='na.fail')\ndredge(duck.logr2, rank='BIC')\n\nFixed term is \"(Intercept)\"\n\n\nGlobal model call: glm(formula = cbind(successes, failures) ~ day + factor(month) + \n    impact + month + factor(day), family = binomial(link = \"logit\"), \n    data = pd, na.action = \"na.fail\")\n---\nModel selection table \n   (Int)      day fct(day) fct(mnt)     imp      mnt df   logLik   BIC  delta\n14 2.618 -0.01173                 + -0.2236          14 -276.365 612.6   0.00\n30 2.618 -0.01173                 + -0.2236          14 -276.365 612.6   0.00\n13 2.430                          + -0.2235          13 -280.560 616.7   4.11\n29 2.430                          + -0.2235          13 -280.560 616.7   4.11\n26 2.770 -0.01173                   -0.2226 -0.09155  4 -299.926 617.0   4.36\n15 2.313                 +        + -0.2250          33 -238.023 617.2   4.57\n16 2.342 -0.01464        +        + -0.2250          33 -238.023 617.2   4.57\n31 2.313                 +        + -0.2250          33 -238.023 617.2   4.57\n32 2.342 -0.01464        +        + -0.2250          33 -238.023 617.2   4.57\n25 2.631                            -0.2224 -0.10250  3 -305.371 623.6  10.97\n6  2.501 -0.01172                 +                  13 -287.029 629.7  17.05\n22 2.501 -0.01172                 +                  13 -287.029 629.7  17.05\n5  2.314                          +                  12 -291.218 633.8  21.15\n21 2.314                          +                  12 -291.218 633.8  21.15\n18 2.653 -0.01171                           -0.09141  3 -310.543 633.9  21.31\n7  2.195                 +        +                  32 -248.752 634.4  21.75\n23 2.195                 +        +                  32 -248.752 634.4  21.75\n8  2.224 -0.01459        +        +                  32 -248.752 634.4  21.75\n24 2.224 -0.01459        +        +                  32 -248.752 634.4  21.75\n27 2.884                 +          -0.2237 -0.11950 24 -267.701 638.0  25.44\n28 2.911 -0.01337        +          -0.2237 -0.11950 24 -267.701 638.0  25.44\n17 2.514                                    -0.10230  2 -315.978 640.5  27.91\n20 2.793 -0.01335        +                  -0.11930 23 -278.371 655.1  42.50\n19 2.766                 +                  -0.11930 23 -278.371 655.1  42.50\n10 2.444 -0.02848                   -0.2204           3 -371.770 756.4 143.77\n11 2.285                 +          -0.2216          23 -335.678 769.7 157.12\n12 2.347 -0.03102        +          -0.2216          23 -335.678 769.7 157.12\n2  2.328 -0.02844                                     2 -382.278 773.1 160.51\n3  2.170                 +                           22 -346.244 786.6 173.97\n4  2.232 -0.03097        +                           22 -346.244 786.6 173.97\n9  1.918                            -0.2192           2 -411.086 830.7 218.12\n1  1.804                                              1 -421.538 847.4 234.75\n   weight\n14  0.360\n30  0.360\n13  0.046\n29  0.046\n26  0.041\n15  0.037\n16  0.037\n31  0.037\n32  0.037\n25  0.001\n6   0.000\n22  0.000\n5   0.000\n21  0.000\n18  0.000\n7   0.000\n23  0.000\n8   0.000\n24  0.000\n27  0.000\n28  0.000\n17  0.000\n20  0.000\n19  0.000\n10  0.000\n11  0.000\n12  0.000\n2   0.000\n3   0.000\n4   0.000\n9   0.000\n1   0.000\nModels ranked by BIC(x) \n\n\nHere it looks like day as quantitative and month as categorical works best."
  },
  {
    "objectID": "collinearity.html#graphical-checks",
    "href": "collinearity.html#graphical-checks",
    "title": "10  Collinearity and Multicollinearity",
    "section": "10.1 Graphical Checks",
    "text": "10.1 Graphical Checks\nThere are two functions in the package GGally that we can use to visualize the level of correlation between predictor variables.\nAs a simple example, let’s consider a small dataset on blood pressure and other variables for 20 adults with high blood pressure. It contains variables:\n\npatient number (Pt)\nblood pressure (BP in mm Hg)\nAge in years\nWeight, in kg\nbody surface area (BSA, in \\(m^2\\))\nduration of hypertension (Dur, in years)\nbasal pulse (Pulse, in beats per minute)\nstress index (Stress)\n\n\nbp &lt;- read.table('http://sldr.netlify.com/data/bloodpress.txt', \n                 header=TRUE)\n\nImagine we want to predict BP as a function of the other variables in the dataset. How can we use a plot to check whether any of the predictors at (too) correlated with each other?\n\n10.1.1 Preferred option: Correlation Scatter Plot\n\nlibrary(GGally)\nbp2 &lt;- bp |&gt; dplyr::select(Age, Weight, BSA, Dur, Pulse, Stress)\nggpairs(bp2)\n\n\n\n\n\n\n10.1.2 Another option: Heat map of correlation coefficients\n\n# label=TRUE input adds numeric corr coef labels\nggcorr(bp2, label=TRUE)\n\n\n\n\n\n\n10.1.3 How to use this information\nBut how do we know how much correlation is too much? Some people suggest excluding predictors from a model if their pairwise correlation is above about 0.8, but that is a very arbitrary rule; sometime model estimate works just fine in the face of that much correlation. In other cases correlation of each pair of variables is low, but there is multicollinearity (a group of variables that predicts one other variable very well). Having a look at the data is always useful, but what else can we do to decide which predictors to include or exclude?\nSo, one solution is to decide if there is “too much” collinearity or multicollinearity, and if there is “too much” for a given predictor, remove that one from the model.\nThere are also other options - model fitting techinques that get around the problem in other ways. (Key words in this arena are things like “lasso” and “neural net regression”). We will likely not have time to consider these in our class."
  },
  {
    "objectID": "collinearity.html#variance-inflation-factors",
    "href": "collinearity.html#variance-inflation-factors",
    "title": "10  Collinearity and Multicollinearity",
    "section": "10.2 Variance Inflation Factors",
    "text": "10.2 Variance Inflation Factors\nA more precise way to check for collinearity and multicollinearity is to use VIFs. To get them, we will use the function vif() from package car (Companion to Applied Regression).\nBut…what are the actually measuring?\n\\[VIF = \\frac{1}{1 - R^2_i}\\]\nWhere\n\\[R^2_i\\] is the \\(R^2\\) value for a regression model with the \\(i\\)th predictor as the response variable, and all the other predictors as a predictor. So \\(R^2_i\\) measures how well values of predictor \\(i\\) can be estimated based on the other predictors. High \\(R^2_i\\) indicates a predictor that is redundant, providing the same information as other predictors.\nSince \\(R^2_i\\) is in the denominator of the \\(VIF\\) expression, larger VIFs indicate more problematic correlations between predictors.\nNote: an alternate interpretation of VIFs is that they measure the factor by which variance of model coefficient estimates is “inflated” due to collinearity between predictors. Big is still bad, because big VIF means inflated (high) uncertainty.\n\n10.2.1 Quantitative predictors (VIFs)\nFor our blood pressure example:\n\nlibrary(car)\nm1 &lt;- lm(BP ~ Age + Weight + BSA + Dur + Pulse + Stress, data=bp)\nvif(m1)\n\n     Age   Weight      BSA      Dur    Pulse   Stress \n1.762807 8.417035 5.328751 1.237309 4.413575 1.834845 \n\nm2 &lt;- lm(BP ~ Age + BSA + Dur + Pulse + Stress, data=bp)\nvif(m2)\n\n     Age      BSA      Dur    Pulse   Stress \n1.703115 1.428349 1.237151 2.360939 1.502936 \n\n\n\n\n10.2.2 (Some) Categorical Predictors (GVIFs)\nWhat if some predictors are categorical? In this case, vif() will report generalized VIFs (GVIFs) and scaled GVIFs (\\(GVIF^{\\frac{1}{2DF}}\\)).\n\n\n10.2.3 Rules of Thumb\nIf a VIF (or squared scaled GVIF) is 1, that means there is no added uncertainty in model estimates because of collinearity. If VIF (or squared scaled GVIF) is greater than 4, then there’s a problem and you should probably try to fix it; if VIF (or squared scaled GVIF) is more than 10, then something definitely must be done to correct the problem.\nAs suggested above…To use these rules of thumb with scaled GVIFs (\\(GVIF^{\\frac{1}{2DF}}\\)), square the scaled GVIF value before applying the rule.\nLet’s look at an example from the built-in R dataset Duncan, which has data from 1950 on characteristics of different professions (type ?Duncan in R if you want more info on the data).\nIf we only include quantiative predictors, we get VIFs as before:\n\nm2 &lt;- lm(prestige ~ income + education, data=Duncan)\nvif(m2)\n\n   income education \n   2.1049    2.1049 \n\n\nIf we include a categorical predictor, then we get GVIFs:\n\nm3 &lt;- lm(prestige ~ income + education + type, data=Duncan)\nvif(m3)\n\n              GVIF Df GVIF^(1/(2*Df))\nincome    2.209178  1        1.486330\neducation 5.297584  1        2.301648\ntype      5.098592  2        1.502666\n\n\nIn this case, it seems that the only apparent problem is with the education variable in the three-predictor model. We might try to correct this by excluding the education predictor. If we do, we see that the \\(R^2\\) value of the model will not decrease by very much (since most of the information encoded in the education variable was already present in the others):\n\nsummary(m3)$r.squared\n\n[1] 0.9130657\n\nm4 &lt;- update(m3, . ~ . -education)\nsummary(m4)$r.squared\n\n[1] 0.8929864"
  },
  {
    "objectID": "collinearity.html#was-it-worth-it",
    "href": "collinearity.html#was-it-worth-it",
    "title": "10  Collinearity and Multicollinearity",
    "section": "10.3 Was it worth it?",
    "text": "10.3 Was it worth it?\n(How would we have done at sorting this out using graphical methods only? Check by creating a pairwise scatterplot using the Duncan data on your own.)"
  },
  {
    "objectID": "zero-inflation.html#reference-material",
    "href": "zero-inflation.html#reference-material",
    "title": "11  Zero-Inflation",
    "section": "11.1 Reference material",
    "text": "11.1 Reference material\nOn the topic of zero-inflated data (as on many other topics), your book is …terse! I have several copies of Mixed models and extensions for ecology in R if you want to borrow – You may want to check out Chapter 11 in that book."
  },
  {
    "objectID": "zero-inflation.html#data-for-example",
    "href": "zero-inflation.html#data-for-example",
    "title": "11  Zero-Inflation",
    "section": "11.2 Data for Example",
    "text": "11.2 Data for Example\nThe state wildlife biologists want to model how many fish are being caught by fishermen at a state park. Visitors are asked how long they stayed, how many people were in the group, were there children in the group and how many fish were caught. Some visitors do not fish, but there is no data on whether a person fished or not. Some visitors who did fish did not catch any fish so there are excess zeros in the data because of the people that did not fish.\nWe have data on 250 groups that went to a park. Each group was questioned about how many fish they caught (count), how many children were in the group (child), how many people were in the group (persons), and whether or not they brought a camper to the park (camper).\nIn addition to predicting the number of fish caught, there is interest in predicting the existence of excess zeros, i.e., the probability that a group caught zero fish. We will use the variables child, persons, and camper in our model. Let’s look at the data.\n\nzinb &lt;- read.csv(\"https://stats.idre.ucla.edu/stat/data/fish.csv\")\nhead(zinb)\n\n  nofish livebait camper persons child         xb         zg count\n1      1        0      0       1     0 -0.8963146  3.0504048     0\n2      0        1      1       1     0 -0.5583450  1.7461489     0\n3      0        1      0       1     0 -0.4017310  0.2799389     0\n4      0        1      1       2     1 -0.9562981 -0.6015257     0\n5      0        1      0       1     0  0.4368910  0.5277091     1\n6      0        1      1       4     2  1.3944855 -0.7075348     0\n\nzinb &lt;- zinb |&gt; mutate(nofish=factor(nofish),\n                        livebait=factor(livebait),\n                        livebait = ifelse(livebait==0, 'No', 'Yes'),\n                        camper=factor(camper),\n                        camper = ifelse(camper ==0, 'No', 'Yes'),\n                        child_cat = factor(child))\nhead(zinb)\n\n  nofish livebait camper persons child         xb         zg count child_cat\n1      1       No     No       1     0 -0.8963146  3.0504048     0         0\n2      0      Yes    Yes       1     0 -0.5583450  1.7461489     0         0\n3      0      Yes     No       1     0 -0.4017310  0.2799389     0         0\n4      0      Yes    Yes       2     1 -0.9562981 -0.6015257     0         1\n5      0      Yes     No       1     0  0.4368910  0.5277091     1         0\n6      0      Yes    Yes       4     2  1.3944855 -0.7075348     0         2"
  },
  {
    "objectID": "zero-inflation.html#visualization",
    "href": "zero-inflation.html#visualization",
    "title": "11  Zero-Inflation",
    "section": "11.3 Visualization?",
    "text": "11.3 Visualization?\nIn class, we created several figures for data exploration. They are not printed here.\nOne idea was to make a pairs plot to look at possible collinearity. Good idea! But all our predictors are either yes/no or have very small ranges of integers as their values, so it doesn’t prove very informative with our data - feel free to try for yourself to illustrate:\n\nlibrary(GGally)\nggpairs(zinb[, c('livebait', 'camper', 'persons', 'child')])\n\nAnother idea would be to plot response as a function of possible predictors, to check the linearity conditions of the parts of the model. This could be a good idea, but on the other hand we don’t have any predictors that are unequivocally numerical. (If we had one, we could plot log(count) as a function of the predictor.)"
  },
  {
    "objectID": "zero-inflation.html#collinearitymulticollinearity",
    "href": "zero-inflation.html#collinearitymulticollinearity",
    "title": "11  Zero-Inflation",
    "section": "11.4 Collinearity/Multicollinearity?",
    "text": "11.4 Collinearity/Multicollinearity?\nWe might want to check VIFs, but we can’t compute them directly for a ZIP model (fitted later) using vif() from the car package.\nHowever, note: VIFs really depend mainly on the covariates, not the model fitted. We can check them effectively by fitting the Poisson model (and the logistic regression, if we had more than one predictor in that part of the model) and checking its VIFs:\n\nP &lt;- glm(count ~ camper + child + persons, data=zinb, family=poisson)\nlibrary(car)\nvif(P)\n\n  camper    child  persons \n1.008616 1.011875 1.008656 \n\n\nNo evidence of any problem there!"
  },
  {
    "objectID": "zero-inflation.html#fitting-models",
    "href": "zero-inflation.html#fitting-models",
    "title": "11  Zero-Inflation",
    "section": "11.5 Fitting models",
    "text": "11.5 Fitting models\n\n11.5.1 Zero-inflated Poisson\n\nlibrary(pscl)\nZIP &lt;- zeroinfl(count ~ child + camper + persons | 1, data = zinb,\n                dist = 'poisson')\nsummary(ZIP)\n\n\nCall:\nzeroinfl(formula = count ~ child + camper + persons | 1, data = zinb, \n    dist = \"poisson\")\n\nPearson residuals:\n     Min       1Q   Median       3Q      Max \n-1.08778 -0.77677 -0.57757 -0.09894 27.97095 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.99805    0.17557  -5.685 1.31e-08 ***\nchild       -1.36091    0.09410 -14.462  &lt; 2e-16 ***\ncamperYes    0.79561    0.09411   8.454  &lt; 2e-16 ***\npersons      0.87214    0.04496  19.398  &lt; 2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.3659     0.1868  -1.958   0.0502 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -776.5 on 5 Df\n\n\n\n\n11.5.2 Zero-inflated NB: one way\nWe can also use this function to fit a NB model. BUT BEWARE: this is a different parameterization of the NB model than the NB1 and NB2 models we use in glmmTMB() and may be closer to type 1 or type 2 depending on the dataset. You could compare all three NB versions via IC.\n\nZINBx &lt;- zeroinfl(count ~ child + camper + persons | 1, data = zinb,\n                  dist='negbin')\nsummary(ZINBx)\n\n\nCall:\nzeroinfl(formula = count ~ child + camper + persons | 1, data = zinb, \n    dist = \"negbin\")\n\nPearson residuals:\n     Min       1Q   Median       3Q      Max \n-0.66364 -0.54174 -0.42090 -0.03142 20.44992 \n\nCount model coefficients (negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.6250     0.3294  -4.933 8.09e-07 ***\nchild        -1.7805     0.1920  -9.272  &lt; 2e-16 ***\ncamperYes     0.6211     0.2358   2.634  0.00844 ** \npersons       1.0608     0.1175   9.030  &lt; 2e-16 ***\nLog(theta)   -0.7689     0.1539  -4.998 5.80e-07 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   -13.63     217.63  -0.063     0.95\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 0.4635 \nNumber of iterations in BFGS optimization: 40 \nLog-likelihood: -405.2 on 6 Df\n\n\n\n11.5.2.1 What do the summary() outputs mean?\nThe second part of the output (labelled “Zero Inflation model coefficients”) is from a logistic regression with one predictor (persons) to predict the probability of an observation being a “true zero” - a group that was not in the park to fish. (We can’t really tell from thsi output whether being there to fish, or not, is considered a true zero by the model…I figured it out by reading the help file for zeroinfl and looking at prediction plots below). The first part of the out put (labelled “Count model coefficients”) is a Poisson regression fitted to the count data for the proportion of the groups in the data set who were not true zeros (and were thus actually there to fish). Both parts of the model can have predictors included; to have no predictors in the logistic regression part, you would use a formula like y ~ x1 + x2 … + xn | 1, and to include the same predictors for both the Poisson and logistic parts, you exclude the |… part, for example ** y ~ x1+x2** is the same as y ~ x1 + x2 | x1 + x2.\n\n\n\n11.5.3 Zero-inflated negative binomial (other way)\n\nlibrary(glmmTMB)\nZINB1 &lt;- glmmTMB(count ~ child + camper + persons, ziformula = ~1, data=zinb, family=nbinom1(link='log'))\nZINB2 &lt;- glmmTMB(count ~ child + camper + persons, ziformula = ~1, data=zinb, family=nbinom2(link='log'))\nsummary(ZINB1)\n\n Family: nbinom1  ( log )\nFormula:          count ~ child + camper + persons\nZero inflation:         ~1\nData: zinb\n\n     AIC      BIC   logLik deviance df.resid \n   823.7    844.8   -405.8    811.7      244 \n\n\nDispersion parameter for nbinom1 family (): 9.21 \n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.82828    0.32013  -2.587 0.009673 ** \nchild       -1.42884    0.16569  -8.624  &lt; 2e-16 ***\ncamperYes    0.72240    0.19303   3.742 0.000182 ***\npersons      0.77145    0.08394   9.190  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nZero-inflation model:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -20.1     4603.4  -0.004    0.997\n\n\nNote the difference from the zeroinfl() model formula and function call. There is an separate input, ziformula, to add zero inflation to a glmmTMB() model. (The default is no zero inflation, if you do not include a ziformula). The zero-inflation model always uses a logit link.\n\n\n11.5.4 Tweedie Model\nAnother option for this type of data might be to try a Tweedie model, because it also has a probability mass at 0 (in other words, allows for zero inflation). The Tweedie distributions we are considering are not exactly perfect for count data since the positive part of the distribution is like a gamma distribution (continuous, not counts), but the shape and support of the distribution are pretty close to our data. Let’s try:\n\nZIT &lt;- glmmTMB(count ~ child + camper + persons, data=zinb, family=tweedie(link = 'log'))\nsummary(ZIT)\n\n Family: tweedie  ( log )\nFormula:          count ~ child + camper + persons\nData: zinb\n\n     AIC      BIC   logLik deviance df.resid \n   857.6    878.8   -422.8    845.6      244 \n\n\nDispersion parameter for tweedie family (): 3.95 \n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.75575    0.32878  -5.340 9.29e-08 ***\nchild       -1.75659    0.18369  -9.563  &lt; 2e-16 ***\ncamperYes    0.76704    0.22200   3.455  0.00055 ***\npersons      1.06495    0.09683  10.998  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "zero-inflation.html#model-assessment",
    "href": "zero-inflation.html#model-assessment",
    "title": "11  Zero-Inflation",
    "section": "11.6 Model Assessment",
    "text": "11.6 Model Assessment\nWe can do model assessment “like” we do for other count data models…more or less. (It is much harder to get something like Pearson residuals for these models, though…) Please stay tuned!"
  },
  {
    "objectID": "zero-inflation.html#model-selection",
    "href": "zero-inflation.html#model-selection",
    "title": "11  Zero-Inflation",
    "section": "11.7 Model Selection?",
    "text": "11.7 Model Selection?\nOne thing to understand is: is the ZIP model a better fit to the data than just a Poisson model? We can think of ZI as a different way of modelling overdispersion, an alternative to NB or quasi-Poisson models when we know there are true zeros in the data set.\n\nP &lt;- glm(count ~ camper + child + persons, data=zinb, family=poisson)\nNB1 &lt;- glmmTMB(count ~ camper + child + persons, data=zinb,\n              family=nbinom1(link='log'))\nNB2 &lt;- glmmTMB(count ~ camper + child + persons, data=zinb,\n              family=nbinom2(link='log'))\nAIC(ZIP,P, NB1, NB2)\n\n    df       AIC\nZIP  5 1562.9324\nP    4 1682.1450\nNB1  5  821.6926\nNB2  5  820.4440\n\n\nWell, yes, the ZIP model is much better than the Poisson! But, the NB1 and NB2 models are much better than both.\nAmong the different families and types of models that we have tried, which seems to be best for this data set?\n\nBIC(P, ZIP, ZINB1, ZINB2, ZINBx, ZIT)\n\n      df       BIC\nP      4 1696.2308\nZIP    5 1580.5397\nZINB1  6  844.8214\nZINB2  6  843.5728\nZINBx  6  843.5728\nZIT    6  878.7701\n\n\nFor these data, the zero-inflated models are very similar to the other NB models, in terms of the AIC. If we used BIC instead, the non-zero inflated models would look quite a bit better! The improvement in goodness of fit with the zero-inflation is not really enough to make us prefer these ZI models to simpler models that also allow overdispersion, like the NB models.\nThe NB models and the ZI models (with whatever “family” you choose) are two different ways of dealing with overdispersion caused by an abundance of zeros in the data. I’d recommend choosing ZI models if:\n\nThere are true zeros and other zeros in your data, and you want to model this explicitly; or if\nThe ZI models fit the data much better than a simpler non-ZI NB (or perhaps Tweedie) model.\n\nThe zero-inflated negative binomial models seem to be best, among the ZI models. However, there is still a lot more work that could be done here! For example…\n\n11.7.1 Zero inflation covariates\nwhat if the Logistic regression part of the model also depends on covariates? That seems reasonable – maybe the number of people present, whether they are camping, and the number of kids present would help you to predict whether a group was not going to go fishing at all. To check that, we would need to change the ziformula (in NB models) or the part of the formula after the \\(\\vert\\) in ZIP models. It is possible this change might make the ZI models competitive with, or better than, the non-ZI NB models in IC terms.\n\n\n11.7.2 Interaction terms\nFor this dataset, it may also make sense to consider some more predictors, and interaction terms. For example, maybe live bait use is a good predictor, and maybe the relationship between live bait use and number of fish caught changes depending on the number of children present – for example, we might try something like:\n\nZINB1b &lt;- glmmTMB(count ~ camper + child*livebait + persons, ziformula= ~1, \n                  data=zinb, family=nbinom1(link='log'))\nsummary(ZINB1b)\n\n Family: nbinom1  ( log )\nFormula:          count ~ camper + child * livebait + persons\nZero inflation:         ~1\nData: zinb\n\n     AIC      BIC   logLik deviance df.resid \n   818.0    846.2   -401.0    802.0      242 \n\n\nDispersion parameter for nbinom1 family (): 8.47 \n\nConditional model:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.648964   0.457499  -3.604 0.000313 ***\ncamperYes          0.649637   0.191185   3.398 0.000679 ***\nchild             -1.469266   0.643409  -2.284 0.022397 *  \nlivebaitYes        0.900232   0.352927   2.551 0.010749 *  \npersons            0.790689   0.082140   9.626  &lt; 2e-16 ***\nchild:livebaitYes -0.008494   0.663504  -0.013 0.989786    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nZero-inflation model:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   -19.73    4306.92  -0.005    0.996\n\nAIC(ZINB1, ZINB1b)\n\n       df      AIC\nZINB1   6 823.6926\nZINB1b  8 817.9976\n\n\nHmmm…it looks like we made an improvement. But do you think the improvement is because of the interaction term?\nLooking at the p-values, we might suspect that while livebait helps predict fish caught, there isn’t a clear interaction between effects of live bait use and the number of kids present.\nIn this case, it is hard to fathom all the model selection options because there are predictors in both parts of the model.\nIf you continue by hand, one thing to remember is: You can never include an interaction term in a model without also including the corresponding “main effect” term. For example, you can not have an interaction between child and livebait without also having both child and livebait as individual predictors in the model. (In fact, I have intentionally not taught you how to fit the “wrong” model…as long as you use the * to specify an interaction in your model formula, you should be fine!)\n\n\n11.7.3 Dredge\nWe can also use dredge(), but:\n\nIt will only put covariates in/out of the model, not compare zero-inflated with non-zero-inflated models.\nIt will put covariates in and out of both the zero-inflation and regular parts of the model.\n\nSo to do a comprehensive comparison, you would need to start with several full models – one for each “family” you want to try, with and without ZI, and with all possible predictors included for both the count and ZI parts of the models – and compare dredge results for all of them.\ndredge() is also nice and smart about interaction terms – it will never include an interaction unless the main effect terms are also present.\n\nlibrary(MuMIn)\nZINB1c &lt;- glmmTMB(count ~ child_cat*livebait + camper + persons, \n                  ziformula = ~ child_cat + camper + persons, \n                  data=zinb, family=nbinom1(link='log'), na.action = 'na.fail')\nms &lt;- dredge(ZINB1c, rank='AIC')\nhead(ms)\n\nGlobal model call: glmmTMB(formula = count ~ child_cat * livebait + camper + persons, \n    data = zinb, family = nbinom1(link = \"log\"), ziformula = ~child_cat + \n        camper + persons, na.action = \"na.fail\", dispformula = ~1)\n---\nModel selection table \n    cnd((Int)) zi((Int)) dsp((Int)) cnd(cmp) cnd(chl_cat) cnd(lvb) cnd(prs)\n16      -1.655    -20.13          +        +            +        +   0.7894\n144     -1.655    -14.99          +        +            +        +   0.7894\n48      -1.655    -19.44          +        +            +        +   0.7894\n176     -1.655    -25.77          +        +            +        +   0.7894\n32      -1.685    -24.09          +        +            +        +   0.7890\n80      -1.655    -23.88          +        +            +        +   0.7894\n    cnd(chl_cat:lvb) zi(cmp) zi(chl_cat) zi(prs) df   logLik   AIC delta weight\n16                                                9 -399.843 817.7  0.00  0.504\n144                                       -3.018 10 -399.843 819.7  2.00  0.186\n48                         +                     10 -399.843 819.7  2.00  0.186\n176                        +              -4.369 11 -399.843 821.7  4.00  0.068\n32                 +                             12 -399.634 823.3  5.58  0.031\n80                                     +         12 -399.843 823.7  6.00  0.025\nModels ranked by AIC(x) \n\n\nIn this case, which model do you think is best? What else might you try?"
  },
  {
    "objectID": "zero-inflation.html#prediction-plots",
    "href": "zero-inflation.html#prediction-plots",
    "title": "11  Zero-Inflation",
    "section": "11.8 Prediction Plots",
    "text": "11.8 Prediction Plots\nWe can also use prediction plots as usual. Here, they will help us clarify what “success” is in the logistic regression part of the model – according to the help file for the zeroinfl() function, we should be able to see that a “success” is to be a “true zero”.\nLet’s try an example - making prediction plots for our ZIP model for different numbers of persons in the group.\nFirst let’s check the range of the data for different variables:\n\nsummary(zinb[,c('camper', 'child', 'persons')])\n\n    camper              child          persons     \n Length:250         Min.   :0.000   Min.   :1.000  \n Class :character   1st Qu.:0.000   1st Qu.:2.000  \n Mode  :character   Median :0.000   Median :2.000  \n                    Mean   :0.684   Mean   :2.528  \n                    3rd Qu.:1.000   3rd Qu.:4.000  \n                    Max.   :3.000   Max.   :4.000  \n\n\nNow, the prediction plots:\n\nnew_data &lt;- data.frame(persons=c(1,2,3,4),\n                       camper='Yes',\n                       child=0)\npred &lt;- predict(ZIP, newdata=new_data)\ngf_point(pred~persons, data=new_data) |&gt;\n  gf_labs(x='People in Group', y='Number of Fish Caught')\n\n\n\n\n\n\n\n\nThe number of fish caught goes up as the number of people in the group goes up. But persons was a predictor for the logistic regression (zero-inflation part) of the model! And its coefficient was estimated to have a negative value. So according to our model, the more persons in the group, the less likely you are to be a “true zero” (and the more likely you are to be in the park to fish). Therefore, the number of fish your group catches goes up.\nHowever, notice that unfortunately, the predict() function for zerofinl() ZIP models does not return standard errors on the fitted values. Shucks! We would have to compute these ourselves to show uncertainty on the prediction plot. We could use something called a parametric or nonparametric bootstrap to do it. We may learn how to do this later on in the course, but for now we don’t have a tool to do it.\nIf you use one of the glmmTMB() models, though, you should be able to make prediction plots with uncertainty as usual."
  },
  {
    "objectID": "zero-inflation.html#acknowledgements",
    "href": "zero-inflation.html#acknowledgements",
    "title": "11  Zero-Inflation",
    "section": "11.9 Acknowledgements",
    "text": "11.9 Acknowledgements\nMany thanks to: https://stats.idre.ucla.edu/r/dae/zip/, a source of the data set and information used in preparing these notes."
  },
  {
    "objectID": "ncv.html",
    "href": "ncv.html",
    "title": "12  Non-constant variance (and other unresolved problems)",
    "section": "",
    "text": "So far, we have worked to assemble a tool-kit that allows us to fit appropriate regression models for a variety of types of response and predictor variables. If we fit an appropriate model (conditions are met) to an appropriate dataset (representative sample of the population of interest), we should be able to draw valid conclusions – but as we have seen, if conditions are not met, then our conclusions (model predictions, judgements about which predictors are important, etc.) may all be unreliable.\nSo what can we do if we’ve fit the best model we know how to fit, and it’s still not quite right? We have seen a number of examples so far where, even after we fit a model with what seem to be the right family and sensible predictors, conditions are not met. The most common problems are non-constant variance of residuals (often seen with non-normal residuals), and non-independent residuals. Solutions for non-independent residuals are coming soon, in future sections. Here we’ll review options (some already in our tool-kit, and some new) to try to improve the situation when non-constant variance (NCV) is present.\nWhat can we try if a model seems to fit data well, except for the fact that the constant variance condition doesn’t hold (for linear regression) or the mean-variance relationship is not as expected (GLMs etc.)? (Note: this problem is also often accompanied by right skew in the distribution of the residuals.) Possible solutions are presented approximately in order of desirability…\n\n12.0.1 Already in Our Tool Box: Make sure the model is “right”\nImproving the model specification sometimes corrects a problem with non-constant variance. Make sure you have considered:\n\nIs the right “family” being used for the data type being modelled?\nAre all the variables that you think are important predictors (and are available to you in the data) included in the model? \n\n\n\n12.0.2 Already in Our Tool Box: Models that estimate dispersion parameters\nNegative binomial (and quasi-Poisson) models include estimation of a dispersion parameter that helps to account for over- or under-disperion (that is - the variance is larger, or smaller, than the mean value). If one of these models is being used (or is appropriate for the data) - NCV should be accounted for! (We can also verify this: If all is working well, the Pearson residuals will have constant variance.)\n\n\n12.0.3 Gamma GLMs\nIf you are fitting a linear regression and:\n\nThe response variable of interest is non-negative, and\nThere is right skew in residuals and/or\nThere is non-constant variance of residuals…\n\nYou may want to try fitting a model using the Gamma family (link = 'log' or link = 'inverse' link functions are the most common). If needed you can use model assessment plots and/or model selection criteria (AIC, BIC…) to decide between link functions. For example:\n\ngamma.mod &lt;- glm(resp ~ pred1 + pred2, data=dataset,\n                 family=Gamma(link='log')) # or can use link='inverse'\n\n\n\n12.0.4 Beta GLMs\nIf your response variable happens to be bounded between 0-1 (but NOT a probability, so that a binary data regression would not be appropriate), you can also try a Beta regression (using the beta distribution). Since the shape of the distribution is extremely flexible, it will help with residuals whose distribution is not as expected. The variance of the beta distribution also does depend on its mean, so it is likely to be better than a linear model at addressing NCV. A code example is below. You must use glmmTMB() to fit this model.\n\nbeta_model &lt;- glmmTMB(response ~ predictor1 + predictor2,\n                      data=my_data,\n                      family=beta_family(link='logit'))\n\n\n\n12.0.5 Transformations\nIn some cases, a logarithmic or square-root transformation of the response variable can correct a problem with the variance of the residuals. This is most sensible as a solution if the log(variable) or sqrt(variable) makes some “sense” to a human…for example, if the response variable is wages in dollars, then log10(wages) is kind of the order of magnitude of the salary, which makes some sense as a measure of income. Another example: sound pressure is measured in \\(\\mu Pa\\), but we perceive sound logarithmically, so that a sound that has 10 times greater pressure “sounds” about twice as loud. So log10(sound_pressure) could be a sensible metric.\nWhy does this work? These transformations don’t affect the magnitude of small residual values very much, but they make large residuals get a lot smaller. The result is that the histogram of residuals has less right skew and the large-magnitude residuals (the ones causing the “flare” of the “trumpet” in the residuals vs. fitted plot) get much smaller.\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.0.6 Modelling non-constant variance\nIf your model is a linear regression, then there are a few specialized fitting functions that can allow you to fit models with non-constant variance of the residuals.\nFor multiple linear regression (original fit with lm()), you can use gls() from package nlme and add input weights=varPower() – inputs are otherwise the same as for lm().\n\nlibrary(nlme)\nnls(response ~ pred1 + pred2, data=dataset,\n    weights=varPower())\n\n\n\n\n\n\n\n\n\n\n\nIf you’re interested in learning more about this method (and other options for specification of the form of the non-constant variance), see the references below in R (you will not be held responsible for the information contained in these help files for STAT 245 - just provided for further reference.)\n\n?nlme::nls\n?nlme::varClasses"
  },
  {
    "objectID": "random-effects.html#dataset",
    "href": "random-effects.html#dataset",
    "title": "13  Random Effects",
    "section": "13.1 Dataset",
    "text": "13.1 Dataset\nFrom: Falcone et al. 2017, http://rsos.royalsocietypublishing.org/content/royopensci/4/8/170629.full.pdf\nSatellite tags were used to record dive data and movements of 16 Cuvier’s beaked whales for up to 88 days each. The whales were incidentally exposed to different types of naval sonar exercises during the study period. How did characteristics of their dives change during sonar exposure? We will look specifically at shallow dive duration as a response variable.\n\nd &lt;- read.csv('http://sldr.netlify.com/data/zshal.csv')\nd$SonarA &lt;- factor(d$SonarA)\nd$SonarB &lt;- factor(d$SonarB)"
  },
  {
    "objectID": "random-effects.html#data-exploration",
    "href": "random-effects.html#data-exploration",
    "title": "13  Random Effects",
    "section": "13.2 Data Exploration",
    "text": "13.2 Data Exploration\nFor these data, we are especially interested in how dive duration depends on sonar exposure. We also need to control for effects of other variables like depth and time of day.\n\ngf_boxplot(DurAvg ~ factor(SonarA), data=d) |&gt;\n  gf_labs(x='Sonar A Presence', y='Dive Duration (min.)')\ngf_boxplot(DurAvg ~ factor(SonarB), data=d) |&gt;\n  gf_labs(x='Sonar B Presence', y='Dive Duration (min.)')\ngf_boxplot(DurAvg ~ TransClass, data=d) |&gt;\n  gf_labs(x='Time of Day', y='Dive Duration (min.)')\ngf_point(DurAvg ~ DepthAvg, data=d, alpha=0.5) |&gt;\n  gf_labs(x='Max. Depth (m)', y='Dive Duration (min.)')\ngf_point(DurAvg ~ SonarAPercOL.fill, data=d, alpha=0.5) |&gt;\n  gf_labs(x='Percent Sonar A Overlap', y='Dive Duration (min.)')\ngf_point(DurAvg ~ SonarBPercOL.fill, data=d, alpha=0.5) |&gt;\n  gf_labs(x='Percent Sonar B Overlap', y='Dive Duration (min.)')"
  },
  {
    "objectID": "random-effects.html#a-base-linear-model",
    "href": "random-effects.html#a-base-linear-model",
    "title": "13  Random Effects",
    "section": "13.3 A Base Linear Model",
    "text": "13.3 A Base Linear Model\nA starting point for these data would be a basic linear regression, because the response variable is continuous, and we don’t have strong indication of nonlinear predictor-response relationships.\n\nbase.model &lt;- lm(DurAvg ~ DepthAvg + TransClass +   SonarA +\n                   SonarB +SonarAPercOL.fill +\n                   SonarBPercOL.fill, data=d)\nsummary(base.model)\n\n\nCall:\nlm(formula = DurAvg ~ DepthAvg + TransClass + SonarA + SonarB + \n    SonarAPercOL.fill + SonarBPercOL.fill, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.344  -3.174  -0.148   2.891  40.732 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       11.0774181  0.3056179  36.246  &lt; 2e-16 ***\nDepthAvg           0.0384292  0.0005743  66.920  &lt; 2e-16 ***\nTransClassDay     -0.8195877  0.2718055  -3.015  0.00258 ** \nTransClassDusk    -2.1080411  0.3536873  -5.960 2.66e-09 ***\nTransClassNight   -2.4488002  0.2708857  -9.040  &lt; 2e-16 ***\nSonarA1            2.6646627  1.8160219   1.467  0.14234    \nSonarB1            5.1562595  0.8556973   6.026 1.78e-09 ***\nSonarAPercOL.fill  0.7555749  2.1077356   0.358  0.72000    \nSonarBPercOL.fill  0.8554161  2.2865382   0.374  0.70834    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.242 on 6174 degrees of freedom\nMultiple R-squared:  0.4844,    Adjusted R-squared:  0.4837 \nF-statistic: 725.1 on 8 and 6174 DF,  p-value: &lt; 2.2e-16\n\n\n\n13.3.1 Model assessment\nLet’s take a look right away at the model assessment plot that we suspect will be problematic for time-series data like ours. As we fear…\n\nacf(resid(base.model), main='Residual ACF for base lm')"
  },
  {
    "objectID": "random-effects.html#a-random-effects-model",
    "href": "random-effects.html#a-random-effects-model",
    "title": "13  Random Effects",
    "section": "13.4 A Random Effects model",
    "text": "13.4 A Random Effects model\nThis time we will try to account for the correlation over time within individuals using something called a random effect model (also known as a mixed effects model, multilevel level, among others). How does this model change our regression equation?\nRecall that the form of a base linear model (with just 2 predictors) would be:\n\\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon\\]\nWhere \\(\\epsilon \\sim N(0,\\sigma)\\) are the normally distributed residuals with mean 0.\nNow… \n\n13.4.1 The Formula\nThe function to fit a linear random effect model is lmer(). For a Poisson or Logistic regression with random effects, it’s glmer(). Both are from the package lme4. We add random effects to the model formula with:\n\\[ + (1|variable)\\]\nor nested:\n\\[ + (1|variable1/variable2)\\]\nLet’s try a random effect of individual whale first. We have:\n\nrem1 &lt;- lmer(DurAvg ~ DepthAvg + TransClass +\n  SonarA + SonarB +SonarAPercOL.fill+ \n    SonarBPercOL.fill + (1|TagID), \n  data=d)\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\n\nWhy yes - we should consider rescaling…what/why/how? \n\nd$SonarAPercScale &lt;- scale(d$SonarAPercOL.fill)\nd$SonarBPercScale &lt;- scale(d$SonarBPercOL.fill)\nrem2 &lt;- lmer(DurAvg ~ DepthAvg + TransClass +\n  SonarA + SonarB + SonarAPercScale + \n    SonarBPercScale + (1|TagID), \n  data=d)\n\n\n\n13.4.2 The Results\n\nsummary(rem2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: DurAvg ~ DepthAvg + TransClass + SonarA + SonarB + SonarAPercScale +  \n    SonarBPercScale + (1 | TagID)\n   Data: d\n\nREML criterion at convergence: 36982.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.1028 -0.5632 -0.0081  0.5297  7.8183 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n TagID    (Intercept)  3.401   1.844   \n Residual             22.928   4.788   \nNumber of obs: 6183, groups:  TagID, 15\n\nFixed effects:\n                  Estimate Std. Error t value\n(Intercept)     11.0158506  0.5547014  19.859\nDepthAvg         0.0391448  0.0005327  73.490\nTransClassDay   -0.6416909  0.2490852  -2.576\nTransClassDusk  -1.9171752  0.3235698  -5.925\nTransClassNight -2.3456311  0.2479312  -9.461\nSonarA1          3.4520183  1.6608961   2.078\nSonarB1          4.5271312  0.7842742   5.772\nSonarAPercScale  0.0362979  0.1716890   0.211\nSonarBPercScale  0.1119528  0.0991365   1.129\n\nCorrelation of Fixed Effects:\n            (Intr) DpthAv TrnsClssDy TrnsClssDs TrnsCN SonrA1 SonrB1 SnrAPS\nDepthAvg    -0.282                                                         \nTransClssDy -0.365 -0.055                                                  \nTrnsClssDsk -0.308  0.051  0.650                                           \nTrnsClssNgh -0.410  0.097  0.844      0.658                                \nSonarA1     -0.029 -0.011 -0.011      0.004     -0.001                     \nSonarB1     -0.006 -0.026 -0.037     -0.026     -0.006 -0.032              \nSonrAPrcScl  0.029  0.009  0.001      0.001      0.001 -0.933 -0.002       \nSonrBPrcScl  0.017  0.005 -0.005      0.011      0.003  0.020 -0.785  0.000\n\n\nHow does this model compare to the original linear regression model? (Coefficient estimates? SEs? Additional stuff in the summary output?)\n\n\n13.4.3 Model Assessment\nHow have the model assessment plots changed? Here we’ll focus mainly on the problem ACF.\n\ngf_point(resid(rem2)~fitted(rem2), alpha=0.5)\n\n\n\n\n\nacf(resid(rem2))\n\n\n\n\n\n\n13.4.4 Refinement\nWhat can we try next? \n\nhead(d, 3)\n\n  TagID DurAvg           StartTime DepthAvg TransClass SonarA SonarB\n1    14  17.58 2011-01-06 20:45:30    335.5        Day      0      0\n2    14  19.71 2011-01-06 22:13:23    351.5        Day      0      0\n3    14  18.11 2011-01-06 22:34:48    287.5        Day      0      0\n  SonarAMinKm.fill SonarBMinKm.fill SonarAPercOL.fill SonarBPercOL.fill\n1              500              500                 0                 0\n2              500              500                 0                 0\n3              500              500                 0                 0\n      TagDay  Period       TagDayPeriod SonarAPercScale SonarBPercScale\n1 2011-01-06 (18,20] 2011-01-06.(18,20]     -0.09784411      -0.1020368\n2 2011-01-06 (20,22] 2011-01-06.(20,22]     -0.09784411      -0.1020368\n3 2011-01-06 (20,22] 2011-01-06.(20,22]     -0.09784411      -0.1020368\n\nrem3 &lt;- lmer(DurAvg ~ DepthAvg + TransClass + SonarA +\n               SonarB +SonarAPercScale + SonarBPercScale +\n               (1|TagID/TagDayPeriod), data=d)\n\n\nacf(resid(rem3))"
  },
  {
    "objectID": "random-effects.html#model-selection-for-mixed-models",
    "href": "random-effects.html#model-selection-for-mixed-models",
    "title": "13  Random Effects",
    "section": "13.5 Model Selection for Mixed Models",
    "text": "13.5 Model Selection for Mixed Models\nCan we use our standard likelihood-based model selection criteria with random effects models?\nWell…yes, and no.\n\n13.5.1 REML or ML?\nThere are two different ways to fit these models to data:\n\nby maximizing the likelihood (ML, as we learned about earlier in the course). Unfortunately, it turns out that in this case, the ML estimates of the variance components (the random effects) is biased, toward underestimating variance, when sample size is small.\nby maximizing the restricted maximum likelihood (REML), which separates the likelhood into two parts (one with the fixed effects and one with the variance components). Maximizing parameters with respect to the second part only yields the REML estimators, which are unbiased and so preferred for smaller sample sizes. BUT there’s a catch…REML values can be used to compare models with different error and random effects structures, but not to determine which predictor variables should remain in a best model.\n\nHere, we do have a large sample size, so if we ensure our model is fitted by ML we can try using AIC or BIC for model selection. The default of lmer() and glmer() is to use REML, so if we want ML we have to add the input REML=FALSE to our call.\n\nrem4 &lt;- lmer(DurAvg ~ DepthAvg + TransClass + SonarA + SonarB +\n               SonarAPercScale +  SonarBPercScale +\n               (1|TagID/TagDayPeriod), data=d,\n             na.action='na.fail', REML=FALSE)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00309987 (tol = 0.002, component 1)\n\n\nIn doing model selection for random effects models, dredge() knows to keep the random effects terms present in all models, so we don’t have to specify them as fixed terms.\n\nlibrary(MuMIn)\nrem4_sel &lt;- dredge(rem4, rank='BIC')\n\nFixed term is \"(Intercept)\"\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00309987 (tol = 0.002, component 1)\n\nhead(rem4_sel)\n\nGlobal model call: lmer(formula = DurAvg ~ DepthAvg + TransClass + SonarA + SonarB + \n    SonarAPercScale + SonarBPercScale + (1 | TagID/TagDayPeriod), \n    data = d, REML = FALSE, na.action = \"na.fail\")\n---\nModel selection table \n   (Intrc)   DpthA SonrA  SnAPS SonrB   SnBPS TrnsC df    logLik     BIC delta\n44   10.66 0.03985     +            +             + 10 -18081.06 36249.4  0.00\n46   10.69 0.03986       0.2679     +             + 10 -18081.52 36250.3  0.92\n42   10.68 0.03987                  +             +  9 -18088.50 36255.6  6.15\n48   10.67 0.03985     + 0.1051     +             + 11 -18080.83 36257.7  8.28\n60   10.66 0.03985     +            + 0.02154     + 11 -18081.03 36258.1  8.67\n62   10.69 0.03986       0.2688     + 0.02007     + 11 -18081.50 36259.0  9.60\n   weight\n44  0.583\n46  0.368\n42  0.027\n48  0.009\n60  0.008\n62  0.005\nModels ranked by BIC(x) \nRandom terms (all models): \n  1 | TagID/TagDayPeriod\n\n\n\n\n13.5.2 Best model so far:\n\nrem5 &lt;- lmer(DurAvg ~ DepthAvg + TransClass + SonarA + SonarB +\n               (1|TagID/TagDayPeriod), data=d,\n             na.action='na.fail', REML=FALSE)"
  },
  {
    "objectID": "random-effects.html#random-slopes",
    "href": "random-effects.html#random-slopes",
    "title": "13  Random Effects",
    "section": "13.6 Random Slopes?",
    "text": "13.6 Random Slopes?\nWhat we just practiced and called a “random effect” is sometimes also called a “random intercept” model because, although we allowed for an offset between the overall average predicted response value and that of an individual, we did not allow the slope of the relationship with any of the predictor variables to vary randomly with individual. It is possible to do this, although in my experience it often makes interpretation difficult.\nBefore you do it, think to yourself: do you really think that there is random variation in the relationship of the predictor with the response? One case where random slopes will work well is where there is a strong, clear overall effect and small variations in its magnitude between individuals. Another might be where the relationship with a certain predictor has very strong and very different slopes for different individuals, and you want to account for the added variability this adds to the model.\nIn the (g)lmer() formula, a model with a random slope and intercept in relation to a particular predictor is specified with the form:\n\\[ ... + (PredictorVariable | GroupingVariable)\\] or equivalently \\[ ... + (1 + PredictorVariable | GroupingVariable)\\]\nIf you want to have a random slope for a certain predictor without the corresponding random intercept ( I can’t think of an example where this would be a good idea but you can do it), then use:\n\\[ ... + (0 + PredictorVariable | GroupingVariable)\\]"
  },
  {
    "objectID": "random-effects.html#prediction-plots",
    "href": "random-effects.html#prediction-plots",
    "title": "13  Random Effects",
    "section": "13.7 Prediction Plots",
    "text": "13.7 Prediction Plots\nThere is a bit of added work involved in making prediction plots for some random effects models.\nUnlike GEEs, which provide marginal predictions (predictions of the population average value for any combination of predictor variable values), random effects models provide predictions for an average individual. For a linear regression model (or any model with the identity link function, that is, no link function), the predicted values for the population average and average individual are the same. But with a link function in the mix, it’s different. Consider a (hypothetical) example of a logistic regression modelling probability of passing a test as a function of hours of instruction spent before the test.\n\n\n\n\n\n\n\n\n\n13.7.1 Parametric bootstrap to the rescue!\nHow can we get around this problem? We can make predictions from our model for many, many (simulated) individuals to get a ``population” of predictions. Then, we can take a point-wise average over all those individuals (and also use them to find a CI), to get population average predictions and confidence intervals.\nWe can do this with help from the function bootMer() from the lme4 package.\nTo make this work, we first need a function that makes predictions from our model.\n\n# function to make predictions from a fitted model\nlibrary(s245)\n# predict_rem4 &lt;- function(model){\n#   orig_dat &lt;- model@frame\n#   fixed_vals &lt;- get_fixed(orig_dat[,c(2:ncol(orig_dat))])\n#   new_dat &lt;- get_new_data(orig_dat, predictor='SonarA', fixed_vals)\n#   return(predict(model, newdata = new_dat, \n#                  type = \"response\", allow.new.levels=TRUE))\n# }\n\nbootMer() does parametric bootstrap simulations and each time, computes some function of the fitted model (here, predictions.) We can then examine the quantiles of these bootstrap predictions (the median or mean is our estimate or best-guess predicted value, and the 2.5 and 97.5 percentiles are the bounds of a 95 percent CI).\n\n# boot_rem4 &lt;- bootMer(rem4, FUN = predict_rem4, nsim = 1000, \n#                      type = \"parametric\", use.u = FALSE)\n\n\n# glimpse(boot_rem4$t )\n\n\n# orig_dat &lt;- rem4@frame\n# fixed_vals &lt;- get_fixed(orig_dat[,c(2:ncol(orig_dat))])\n# new_dat &lt;- get_new_data(orig_dat, predictor='SonarA',\n#                           fixed_vals)\n# new_dat &lt;- new_dat |&gt;\n#   mutate(pred = apply(boot_rem4$t, 2, mean),\n#          CIlow = apply(boot_rem4$t, 2, quantile, probs=0.025),\n#          CIhigh = apply(boot_rem4$t, 2, quantile, probs=0.975)\n#          )\n# \n# gf_point(pred ~ SonarA, data=new_dat) |&gt;\n#   gf_labs(x='Sonar A Presence', y='Dive Duration (min.)') |&gt;\n#   gf_errorbar(CIlow + CIhigh ~ SonarA, data=new_dat, width=0.3)\n\n(Because…)\n\n# pred_plot(rem4, 'SonarA')"
  },
  {
    "objectID": "random-effects2.html#model-for-whale-dive-duration",
    "href": "random-effects2.html#model-for-whale-dive-duration",
    "title": "14  Random effects with glmmTMB and standardized residuals",
    "section": "14.1 Model for whale dive duration",
    "text": "14.1 Model for whale dive duration\nWe start with our previous model for whale dive duration.\n\nrem5 &lt;- lmer(DurAvg ~ DepthAvg + TransClass + SonarA + SonarB +\n               (1|TagID/TagDayPeriod), data=d,\n             na.action='na.fail', REML=FALSE)"
  },
  {
    "objectID": "random-effects2.html#glmmtmb",
    "href": "random-effects2.html#glmmtmb",
    "title": "14  Random effects with glmmTMB and standardized residuals",
    "section": "14.2 glmmTMB",
    "text": "14.2 glmmTMB\nWe fitted these models using lmer() and glmer() from package lme4, which is probably the most commonly used R package to fit these models. But we’ve also used glmmTMB in this course, and it can also fit random effects models. The syntax to add random effects to a glmmTMB() model is exactly the same as for the lme4 functions.\nOne difference is that for glmmTMB() REML = FALSE is the default.\nglmmTMB may be faster in some cases.\n\nlibrary(glmmTMB)\nrem6 &lt;- glmmTMB(DurAvg ~ DepthAvg + TransClass + SonarA + SonarB +\n               (1|TagID/TagDayPeriod), data=d,\n             na.action='na.fail', REML=FALSE)\n\nIf you view the summaries, you will see that rem5 and rem6 are basically identical (in terms of model coefficients and variance estimates)."
  },
  {
    "objectID": "random-effects2.html#model-assessment-with-scaled-residuals",
    "href": "random-effects2.html#model-assessment-with-scaled-residuals",
    "title": "14  Random effects with glmmTMB and standardized residuals",
    "section": "14.3 Model assessment with scaled residuals",
    "text": "14.3 Model assessment with scaled residuals\nFor models where Pearson residuals are not helpful/can not be computed, one option is scaled residuals via simulation.\n\nSimulate many replicates from the fitted model corresponding to each data observation\nExamine the distribution of the simulated data, and use it to scale the observed value to get a “scaled residual” such that a residual of 0 means that all simulated values are larger than the observed value, and a residual of 0.5 means half of the simulated values are larger than the observed value, and a residual of 1 means that the observed value is larger than all the simulated values.\n\nThese scaled residuals should be uniformly distributed between 0 and 1, if the model is correct.\nThere is a much more detailed explanation in the DHARMa package documentation online.\nExample: computing scaled residuals, creating 1000 simulated datasets (don’t lower it much below this to get good estimates). Note: use R chunk setting “message = FALSE” to print messages during simulation to your screen, rather than your knitted file.\n\nlibrary(DHARMa)\nsims &lt;- simulateResiduals(fittedModel = rem5, n = 1000)\nd &lt;- d |&gt;\n  mutate(scaled_resids = sims$scaledResiduals,\n         fitted = fitted(rem5))\n\nIf our model is perfectly correct, we would expect:\n\nHistogram of scaled residuals looks uniform between 0 and 1\nPlot of scaled residual vs. fitted, or vs. any predictor, look uniform (evenly spread, no trends, constant in variance).\n\n\ngf_histogram(~scaled_resids, data =d)\ngf_point(scaled_resids ~ fitted, data = d, alpha = 0.5)\n\n\n\n\n\n\n\n\n14.3.1 glmmTMB version\n\nsims2 &lt;- simulateResiduals(fittedModel = rem6, n = 1000)\nd &lt;- d |&gt;\n  mutate(scaled_resids_TMB = sims2$scaledResiduals,\n         fitted_TMB = fitted(rem6))\n\n\ngf_point(scaled_resids_TMB ~ fitted_TMB, data = d, alpha = 0.5)\n\n\n\n\n\n\n14.3.2 You now have the power!\nYou may use these scaled residuals for any of the models considered so far this semester. It is basically an alternative method of scaling residuals so that we know “how they should look” in residual plots."
  },
  {
    "objectID": "gee-motivation.html#data-source",
    "href": "gee-motivation.html#data-source",
    "title": "15  GEEs",
    "section": "15.1 Data Source",
    "text": "15.1 Data Source\nThe dataset used here is industry data from a skin care company. It contains data from experiments with 20 subjects. Each person tested 6 different skin moisturizers, and the hydration level of their skin was measured every 2 hours for 24 hours following application of each product. The variables are:\n\nSubjects Numeric code identifying the person\nCorneoDiff The hydration CorneoDiff\nTime Time in hours since product application\nProduct Which product was used\n\nThe data file can be accessed online at:\nhttp://sldr.netlify.com/data/hydrationData.csv"
  },
  {
    "objectID": "gee-motivation.html#data-exploration",
    "href": "gee-motivation.html#data-exploration",
    "title": "15  GEEs",
    "section": "15.2 Data Exploration",
    "text": "15.2 Data Exploration\nWe would like to model the hydration, CorneoDiff, over time and as a function of product.\n\ngf_point(CorneoDiff ~ Time | Product, data=hyd.sm) |&gt;\n  gf_lims(x=c(0,24)) \n\n\n\n\n\ngf_point(CorneoDiff ~ Time | Subjects, color=~Product, data=hyd.sm) |&gt;\n  gf_line(CorneoDiff ~ Time | Subjects, color=~Product, data=hyd.sm) |&gt;\n  gf_lims(x=c(0,24))"
  },
  {
    "objectID": "gee-motivation.html#linear-regression",
    "href": "gee-motivation.html#linear-regression",
    "title": "15  GEEs",
    "section": "15.3 Linear Regression",
    "text": "15.3 Linear Regression\nWe could try just fitting a linear regression. What do you expect?\n\nlm1 &lt;- lm(CorneoDiff ~ Time + Product, data=hyd.sm)\nsummary(lm1)\n\n\nCall:\nlm(formula = CorneoDiff ~ Time + Product, data = hyd.sm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.791  -5.349  -1.164   4.415  29.691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.79072    0.87716  11.162  &lt; 2e-16 ***\nTime        -0.05726    0.02720  -2.105  0.03577 *  \nProduct2    -1.90238    1.10665  -1.719  0.08623 .  \nProduct3    -2.80079    1.10665  -2.531  0.01169 *  \nProduct4    -2.98016    1.10665  -2.693  0.00732 ** \nProduct5    -3.02659    1.10665  -2.735  0.00646 ** \nProduct6    -0.43929    1.10665  -0.397  0.69157    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.172 on 497 degrees of freedom\nMultiple R-squared:  0.03701,   Adjusted R-squared:  0.02538 \nF-statistic: 3.183 on 6 and 497 DF,  p-value: 0.004487\n\n\n\ngf_point(CorneoDiff ~ Time | Subjects, color=~Product, data=hyd.sm) |&gt;\n  gf_line(CorneoDiff ~ Time | Subjects, color=~Product, data=hyd.sm) |&gt;\n  gf_lims(x=c(0,24))  |&gt;\n  gf_abline(intercept=7.86, slope=0.05608)"
  },
  {
    "objectID": "gee-motivation.html#model-assessment",
    "href": "gee-motivation.html#model-assessment",
    "title": "15  GEEs",
    "section": "15.4 Model Assessment",
    "text": "15.4 Model Assessment\nFor the linear regression:\n\nacf(resid(lm1))\n\n\n\n\n\nhyd.sm &lt;- arrange(hyd.sm,Time, Subjects)\nlm1 &lt;- lm(CorneoDiff ~ Time + Product, data=hyd.sm)\nacf(resid(lm1))\n\n\n\n\nAs we expected, things do not look good…"
  },
  {
    "objectID": "gee-motivation.html#linear-regression-1",
    "href": "gee-motivation.html#linear-regression-1",
    "title": "15  GEEs",
    "section": "15.5 Linear Regression",
    "text": "15.5 Linear Regression\nWe tried a linear regression and encountered two problems:\n\nThe residuals are not independent. There seems to be correlation over time within subjects.\nWe can’t account for inter-person differences unless we include person as a predictor, but we don’t want to do that, because if we do we can not make predictions from the fitted model without specifying which of these exact people we want to predict for. That’s not ideal - we want predictions for all people, or at least averaged over all people."
  },
  {
    "objectID": "gee-motivation.html#generalized-estimating-equations-gees",
    "href": "gee-motivation.html#generalized-estimating-equations-gees",
    "title": "15  GEEs",
    "section": "15.6 Generalized Estimating Equations (GEEs)",
    "text": "15.6 Generalized Estimating Equations (GEEs)\nA potential solution we will invesitgate today is to use a generalized estimating equation (GEE) instead of a GLM. GEEs:\n\nAre a ``PA” model:\nWork by changing…    \n\nWhat residual correlation structures can be accomodated in this framework?\n\nIndependence (corstr=independence’`)\nExchangeable = Block Diagonal (corstr=exchangeable’`)\nAR1 (first-order auto-regressive) (corstr=ar1’`)\nUnstructured (CAUTION!) (corstr=unstructured’`)\n\n\n15.6.1 Fitting GEEs with different correlation structures\n\nlibrary(geepack)\n\n\nAttaching package: 'geepack'\n\n\nThe following object is masked from 'package:MuMIn':\n\n    QIC\n\nhyd &lt;- arrange(hyd, Subjects, Time)\nlm1 &lt;- lm(CorneoDiff ~ Time + Product, data=hyd)\nsummary(lm1)\n\n\nCall:\nlm(formula = CorneoDiff ~ Time + Product, data = hyd)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.5776  -5.3165  -0.0275   5.1061  28.0872 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.42313    0.48032  23.782  &lt; 2e-16 ***\nTime        -0.09552    0.01489  -6.413 1.85e-10 ***\nProduct2    -2.08655    0.60598  -3.443 0.000589 ***\nProduct3    -2.49940    0.60598  -4.125 3.90e-05 ***\nProduct4    -1.96107    0.60598  -3.236 0.001235 ** \nProduct5    -2.37238    0.60598  -3.915 9.41e-05 ***\nProduct6    -0.50833    0.60598  -0.839 0.401669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.17 on 1673 degrees of freedom\nMultiple R-squared:  0.04082,   Adjusted R-squared:  0.03738 \nF-statistic: 11.87 on 6 and 1673 DF,  p-value: 4.489e-13\n\ngee.ind &lt;- geeglm(CorneoDiff ~ Time + Product, data=hyd, \n                  id = Subjects, corstr='independence')\nsummary(gee.ind)\n\n\nCall:\ngeeglm(formula = CorneoDiff ~ Time + Product, data = hyd, id = Subjects, \n    corstr = \"independence\")\n\n Coefficients:\n            Estimate  Std.err   Wald Pr(&gt;|W|)    \n(Intercept) 11.42313  1.20102 90.462  &lt; 2e-16 ***\nTime        -0.09552  0.02342 16.638 4.52e-05 ***\nProduct2    -2.08655  0.98810  4.459   0.0347 *  \nProduct3    -2.49940  1.03782  5.800   0.0160 *  \nProduct4    -1.96107  1.02207  3.681   0.0550 .  \nProduct5    -2.37238  1.32981  3.183   0.0744 .  \nProduct6    -0.50833  0.95744  0.282   0.5955    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation structure = independence \nEstimated Scale Parameters:\n\n            Estimate Std.err\n(Intercept)     51.2   4.017\nNumber of clusters:   20  Maximum cluster size: 84 \n\ngee.ar1 &lt;- geeglm(CorneoDiff ~ Time + Product, data=hyd,\n                  id = Subjects, corstr='ar1')\nsummary(gee.ar1)\n\n\nCall:\ngeeglm(formula = CorneoDiff ~ Time + Product, data = hyd, id = Subjects, \n    corstr = \"ar1\")\n\n Coefficients:\n            Estimate Std.err  Wald Pr(&gt;|W|)    \n(Intercept)  11.4005  1.3968 66.61  3.3e-16 ***\nTime         -0.2045  0.0322 40.27  2.2e-10 ***\nProduct2     -2.2329  0.9941  5.04   0.0247 *  \nProduct3     -2.7965  1.0486  7.11   0.0077 ** \nProduct4     -2.4137  1.0402  5.38   0.0203 *  \nProduct5     -2.9857  1.3513  4.88   0.0271 *  \nProduct6     -1.2880  0.9735  1.75   0.1858    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation structure = ar1 \nEstimated Scale Parameters:\n\n            Estimate Std.err\n(Intercept)     56.9    4.48\n  Link = identity \n\nEstimated Correlation Parameters:\n      Estimate Std.err\nalpha    0.951 0.00905\nNumber of clusters:   20  Maximum cluster size: 84 \n\ngee.exch &lt;- geeglm(CorneoDiff ~ Time + Product, data=hyd, \n                   id = Subjects, corstr='exchangeable')\n\nWhat is the same (or similar) and what is very different between the models?\n\n\n15.6.2 Comparing different correlation structures\nWe can use a specific variance of QIC, \\(QIC_{R}\\), to compare models with different correlation structures:\n\nlibrary(MuMIn)\n# QIC(gee.ind, gee.exch, gee.ar1, typeR=TRUE)\n\nHow can we interpret this result?"
  },
  {
    "objectID": "gee-motivation.html#gee-model-assessment",
    "href": "gee-motivation.html#gee-model-assessment",
    "title": "15  GEEs",
    "section": "15.7 GEE model assessment",
    "text": "15.7 GEE model assessment\nModel assessment for a GEE is mostly the same as for the corresponding linear regression or GLM (Poisson, Logistic, etc.)\nWe were using GEEs to try to correct for issues with non-independent residuals. How does the residual plot change for a GEE relative to the corresponding (g)lm? Does it change? Should it?\n\nacf(resid(lm1), main='LM ACF')\nacf(resid(gee.ind), main='GEE ACF')\n\n\n\n\n\n\n\nWhat is going on here?"
  },
  {
    "objectID": "gee-motivation.html#model-selection---which-variables",
    "href": "gee-motivation.html#model-selection---which-variables",
    "title": "15  GEEs",
    "section": "15.8 Model Selection - Which variables?",
    "text": "15.8 Model Selection - Which variables?\nWe can use another variant of the QIC to do model selection to determine which variables are important to retain in a GEE model.\n\ngee.ind &lt;- update(gee.ind, na.action='na.fail')\ndredge(gee.ind, rank='QIC', typeR=FALSE)\n\nFixed term is \"(Intercept)\"\n\n\nGlobal model call: geeglm(formula = CorneoDiff ~ Time + Product, data = hyd, na.action = \"na.fail\", \n    id = Subjects, corstr = \"independence\")\n---\nModel selection table \n  (Intrc) Prdct    Time qLik  QIC delta weight\n1    8.46               -840 1729  0.00  0.954\n3    9.85       -0.0955 -840 1735  6.06  0.046\n2   10.03     +         -840 1756 26.88  0.000\n4   11.42     + -0.0955 -840 1763 33.71  0.000\nModels ranked by QIC(x, typeR = FALSE) \n\n\nHow would you interpret these results and present them to the cosmetics company that collected the data?"
  },
  {
    "objectID": "gee-motivation.html#prediction-plots",
    "href": "gee-motivation.html#prediction-plots",
    "title": "15  GEEs",
    "section": "15.9 Prediction Plots",
    "text": "15.9 Prediction Plots\nAs for models we studied previously, we can make prediction plots to visualize the relationships a model specifies between the predictor and response variables.\nHowever, we can not use predict() to get model predictions with standard errors from a GEE.\npred_plot() works, though; for example:\n\ns245::pred_plot(gee.ind, 'Product') |&gt;\n  gf_labs(y = 'CorneoDiff')\n\n\n\n\nOnce again we’re grateful for the parametric bootstrap! (This time, pred_plot() is silently doing the work for us.)"
  },
  {
    "objectID": "corr-struct.html",
    "href": "corr-struct.html",
    "title": "16  Correlation Structures",
    "section": "",
    "text": "This section is an appendix to the GEE chapter, to illustrate and discuss the different correlation structures that we can specify when fitting a GEE.\n\n16.0.1 Variance/Covariance or Correlation?\nStatisticians often talk about a model’s variance-covariance matrix (which gives residual variance on the diagonal and covariance between residuals in its off-diagonal elements). Here we will consider correlation matrices instead. This will give us a simplified way of looking at similar ideas. In a correlation matrix, the diagonal entries will be all 1s (the correlation of something with itself is 1), and off-diagonal elements will show correlation between residuals.\n\n\n16.0.2 Example Case\nLet’s consider an example for a dataset with 9 observations; three observations for each of three individuals. (Side note: The groupings causing non-independence in the residuals don’t have to be “individuals” – it could be some other relationship, like being from the same school or town or country or ethnic group – any single categorical variable that defines the groups of interest and induces non-independence withing the groups, could work. Here we are just calling them “individuals” for convenience, and because when time-series data are collected on multiple individuals, this kind of model often works well.)\nEach of the correlation matrices below has one row and one column for each observation.\nThe diagonal entries will all, always, be 1.\nThe off-diagonal entries indicate how the different residuals depend on each other – in other words, they describe mathematically exactly how each residual is correlated with the others.\nWe will use the letter \\(\\rho\\) for correlations.\n\n\n16.0.3 Independence\nIf the residuals are all independent of each other, then all the correlations between them will be zero:\n\\[\n\\begin{vmatrix}\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n\\end{vmatrix}\n\\]\nThis corresponds to all of the models we have considered previously in class (which all have a condition that residuals should be independent).\n\n\n16.0.4 ACF Example\nThe residual ACF from a model fitted to data that perfectly embody this structure with 5 observations per individual might look like:\n\n\n\n\n\n\n\n16.0.5 Exchangeable = Block Diagonal\nIn an exchangeable or block diagonal structure, all residuals for one individual are equally correlated (correlation = \\(\\rho\\)). All residuals from different individuals are independent (correlation = 1).\n\\[\n\\begin{vmatrix}\n1 & \\rho & \\rho & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\rho & 1 & \\rho & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\rho & \\rho & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\rho &\\rho & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\rho & 1 & \\rho & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\rho & \\rho & 1 & 0& 0& 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & \\rho & \\rho\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\rho & 1 & \\rho\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\rho & \\rho & 1\\\\\n\\end{vmatrix}\n\\]\n\n16.0.5.1 ACF Example\nThe residual ACF from a model fitted to data that perfectly embody this structure would show non-independence in the residual ACF, out to about the number of lags that there are observations per individual, but the autocorrelation coefficients would not decline at exactly the rate predicted by an AR(1) process (see below).\n\n\n\n16.0.6 AR1 (first-order autoregressive process)\nIn this structure, we again assume that residuals are uncorrelated (independent) between individuals.\nWithin an individual, we assume measurements that are closer to one another are more correlated. More precisely, we say that observations 1 lag apart have correlation \\(\\rho\\); observations 2 lags apart have correlation \\(\\rho^2\\); three lags apart \\(\\rho^3\\), and so on.\nTo illustrate this, I show below a correlation structure for a model with 10 observations: 5 observations from each of 2 individuals.\n\\[\n\\begin{vmatrix}\n1 & \\rho & \\rho^2 & \\rho^3 & \\rho^4 & 0 & 0 & 0 & 0 & 0\\\\\n\\rho & 1 & \\rho &\\rho^2 & \\rho^3 & 0 & 0 & 0 & 0& 0\\\\\n\\rho^2 & \\rho & 1 & \\rho & \\rho^2 & 0 & 0 & 0 & 0& 0\\\\\n\\rho^3 & \\rho^2 & \\rho & 1 & \\rho & 0 & 0 & 0 & 0& 0\\\\\n\\rho^4 & \\rho^3 & \\rho^2 & \\rho & 1 & 0 & 0 & 0 & 0& 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & \\rho & \\rho^2 & \\rho^3 & \\rho^4\\\\\n0 & 0 & 0 & 0 & 0 & \\rho & 1 & \\rho &\\rho^2 & \\rho^3\\\\\n0 & 0 & 0 & 0 & 0 & \\rho^2 & \\rho & 1 & \\rho & \\rho^2 \\\\\n0 & 0 & 0 & 0 & 0 & \\rho^3 & \\rho^2 & \\rho & 1 & \\rho\\\\\n0 & 0 & 0 & 0 & 0 & \\rho^4 & \\rho^3 & \\rho^2 & \\rho & 1\\\\\n\\end{vmatrix}\n\\]\n\n16.0.6.1 ACF Example\nThe residual ACF from a model fitted to data that perfectly embody this structure with \\(\\rho=0.99\\) and 5 observations per individual, might look like:\n\n\n\n\n\n\n\n\n16.0.7 Unstructured\nThis one is not of practical use because it is so hard to estimate.\n\\[\n\\begin{vmatrix}\n1 & \\rho_{1,2} & \\rho_{1,3} & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\rho_{1,2} & 1 & \\rho_{2,3} & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\rho_{1,3} & \\rho_{2,3} & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\rho_{1,2} & \\rho_{1,3} & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\rho_{1,2} & 1 & \\rho_{2,3}& 0 & 0 & 0\\\\\n0 & 0 & 0 & \\rho_{1,3} & \\rho_{2,3} & 1&0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & \\rho_{1,2} & \\rho_{1,3}\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\rho_{1,2} & 1 & \\rho_{2,3}\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\rho_{1,3} & \\rho_{2,3} & 1\\\\\n\\end{vmatrix}\n\\]"
  },
  {
    "objectID": "anova-plus.html#backward-selection",
    "href": "anova-plus.html#backward-selection",
    "title": "17  Other Model Selection Approaches",
    "section": "17.1 Backward selection",
    "text": "17.1 Backward selection\nHow do we use p-value-based selection to arrive at a best model? There are many options and much controversy about different approaches; here I’ll suggest one. None of these methods are guaranteed to arrive at a model that is theoretically “best” in some specific way, but they do give a framework to guide decision-making and are computationally quick. The premise is that we’d like a simple algorithm to implement, and we will begin with a full model including all the predictors that we think should or could reasonably be important (not just throwing in everything possible).\n\n17.1.1 Algorithm\n\nObtain p-values for all predictors in full model\nRemove the predictor with the largest p-value that you judge to be “not small” or “not significant”\nRe-compute p-values for the new, smaller model\nRepeat until all p-values are “significant”\n\n\n\n17.1.2 Example\nLet’s consider a logistic regression to predict whether a person in substance-abuse treatment is homeless.\n\nhome_mod0 &lt;- glm(homeless ~ sex + substance + i1 + cesd + \n                  racegrp + age,\n                data = HELPrct, family = binomial(link = 'logit'))\nAnova(home_mod0)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: homeless\n          LR Chisq Df Pr(&gt;Chisq)   \nsex         3.3837  1   0.065846 . \nsubstance   2.2326  2   0.327483   \ni1         10.6261  1   0.001115 **\ncesd        1.1751  1   0.278351   \nracegrp     3.1811  3   0.364541   \nage         0.4392  1   0.507491   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRemoving age:\n\nhome_mod &lt;- update(home_mod0, .~. - age)\nAnova(home_mod)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: homeless\n          LR Chisq Df Pr(&gt;Chisq)    \nsex         3.2184  1   0.072817 .  \nsubstance   2.6874  2   0.260877    \ni1         11.0836  1   0.000871 ***\ncesd        1.1300  1   0.287773    \nracegrp     3.1561  3   0.368180    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRemoving racegrp\n\nhome_mod &lt;- update(home_mod, .~. - racegrp)\nAnova(home_mod)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: homeless\n          LR Chisq Df Pr(&gt;Chisq)    \nsex         3.5883  1  0.0581886 .  \nsubstance   3.6342  2  0.1624971    \ni1         11.0174  1  0.0009026 ***\ncesd        1.6055  1  0.2051242    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRemove cesd (a score indicating depression level)\n\nhome_mod &lt;- update(home_mod, .~. - cesd)\nAnova(home_mod)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: homeless\n          LR Chisq Df Pr(&gt;Chisq)    \nsex         2.7855  1   0.095118 .  \nsubstance   3.6743  2   0.159272    \ni1         13.2405  1   0.000274 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRemove substance\n\nhome_mod &lt;- update(home_mod, .~. - substance)\nAnova(home_mod)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: homeless\n    LR Chisq Df Pr(&gt;Chisq)    \nsex   2.9647  1     0.0851 .  \ni1   25.7861  1  3.814e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRemove sex\n\nhome_mod &lt;- update(home_mod, .~. - sex)\nAnova(home_mod)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: homeless\n   LR Chisq Df Pr(&gt;Chisq)    \ni1   27.187  1  1.847e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n17.1.3 Can’t this be automated?\nStrangely…functions are not widely available.\n\n\n17.1.4 Stepwise IC-based selection\nAnother option may be to use backward stepwise selection (same algorithm as above), but using AIC or BIC as the criterion at each stage instead of p-values. If the IC value is better (by any amount) without a variable, it gets dropped. Variables are dropped one by one until no further IC improvement is possible.\nThis evaluates many fewer models than dredge so should be much faster, but may not find the best of all possible models.\nFor example, for our model using AIC (note: this may or may not work for all model types.):\n\nlibrary(MASS)\nstepAIC(home_mod0)\n\nStart:  AIC=606.26\nhomeless ~ sex + substance + i1 + cesd + racegrp + age\n\n            Df Deviance    AIC\n- racegrp    3   589.44 603.44\n- substance  2   588.49 604.49\n- age        1   586.70 604.70\n- cesd       1   587.43 605.43\n&lt;none&gt;           586.26 606.26\n- sex        1   589.64 607.64\n- i1         1   596.88 614.88\n\nStep:  AIC=603.44\nhomeless ~ sex + substance + i1 + cesd + age\n\n            Df Deviance    AIC\n- age        1   589.85 601.85\n- substance  2   592.45 602.45\n- cesd       1   591.10 603.10\n&lt;none&gt;           589.44 603.44\n- sex        1   593.20 605.20\n- i1         1   600.00 612.00\n\nStep:  AIC=601.85\nhomeless ~ sex + substance + i1 + cesd\n\n            Df Deviance    AIC\n- cesd       1   591.46 601.46\n- substance  2   593.49 601.49\n&lt;none&gt;           589.85 601.85\n- sex        1   593.44 603.44\n- i1         1   600.87 610.87\n\nStep:  AIC=601.46\nhomeless ~ sex + substance + i1\n\n            Df Deviance    AIC\n- substance  2   595.13 601.13\n&lt;none&gt;           591.46 601.46\n- sex        1   594.24 602.24\n- i1         1   604.70 612.70\n\nStep:  AIC=601.13\nhomeless ~ sex + i1\n\n       Df Deviance    AIC\n&lt;none&gt;      595.13 601.13\n- sex   1   598.10 602.10\n- i1    1   620.92 624.92\n\n\n\nCall:  glm(formula = homeless ~ sex + i1, family = binomial(link = \"logit\"), \n    data = HELPrct)\n\nCoefficients:\n(Intercept)      sexmale           i1  \n    0.92969     -0.39976     -0.02657  \n\nDegrees of Freedom: 452 Total (i.e. Null);  450 Residual\nNull Deviance:      625.3 \nResidual Deviance: 595.1    AIC: 601.1\n\n\nNote that we might want to still remove one more variable than stepAIC() does! Above, you see that if you were to remove age, the AIC would only go up by about 1 unit. So according to our \\(\\Delta AIC \\sim 3\\) threshold, we would take age out too.\nUsing BIC instead, we need to specify the input k = log(nrow(data)) (the BIC penalty multiplier):\n\nstepAIC(home_mod0, k = log10(nrow(HELPrct)))\n\nStart:  AIC=612.82\nhomeless ~ sex + substance + i1 + cesd + racegrp + age\n\n            Df Deviance    AIC\n- racegrp    3   589.44 608.03\n- substance  2   588.49 609.74\n- age        1   586.70 610.60\n- cesd       1   587.43 611.34\n&lt;none&gt;           586.26 612.82\n- sex        1   589.64 613.55\n- i1         1   596.88 620.79\n\nStep:  AIC=608.03\nhomeless ~ sex + substance + i1 + cesd + age\n\n            Df Deviance    AIC\n- substance  2   592.45 605.73\n- age        1   589.85 605.79\n- cesd       1   591.10 607.04\n&lt;none&gt;           589.44 608.03\n- sex        1   593.20 609.14\n- i1         1   600.00 615.94\n\nStep:  AIC=605.73\nhomeless ~ sex + i1 + cesd + age\n\n       Df Deviance    AIC\n- age   1   593.49 604.11\n- cesd  1   594.20 604.82\n&lt;none&gt;      592.45 605.73\n- sex   1   596.48 607.11\n- i1    1   611.94 622.57\n\nStep:  AIC=604.11\nhomeless ~ sex + i1 + cesd\n\n       Df Deviance    AIC\n- cesd  1   595.13 603.10\n&lt;none&gt;      593.49 604.11\n- sex   1   597.25 605.22\n- i1    1   615.70 623.66\n\nStep:  AIC=603.1\nhomeless ~ sex + i1\n\n       Df Deviance    AIC\n&lt;none&gt;      595.13 603.10\n- sex   1   598.10 603.41\n- i1    1   620.92 626.23\n\n\n\nCall:  glm(formula = homeless ~ sex + i1, family = binomial(link = \"logit\"), \n    data = HELPrct)\n\nCoefficients:\n(Intercept)      sexmale           i1  \n    0.92969     -0.39976     -0.02657  \n\nDegrees of Freedom: 452 Total (i.e. Null);  450 Residual\nNull Deviance:      625.3 \nResidual Deviance: 595.1    AIC: 601.1\n\n\nTo get less verbose output, set trace = 0 – but then you won’t know whether it would make sense to perhaps remove additional variables…\n\nstepAIC(home_mod0, k = log10(nrow(HELPrct)), trace = 0)\n\n\nCall:  glm(formula = homeless ~ sex + i1, family = binomial(link = \"logit\"), \n    data = HELPrct)\n\nCoefficients:\n(Intercept)      sexmale           i1  \n    0.92969     -0.39976     -0.02657  \n\nDegrees of Freedom: 452 Total (i.e. Null);  450 Residual\nNull Deviance:      625.3 \nResidual Deviance: 595.1    AIC: 601.1"
  },
  {
    "objectID": "anova-plus.html#summary-tables",
    "href": "anova-plus.html#summary-tables",
    "title": "17  Other Model Selection Approaches",
    "section": "17.2 Summary tables",
    "text": "17.2 Summary tables\nYou may want to compute and display summary tables for your projects. Here are a few examples of how to do it.\n\n17.2.1 Mean (or sd, median, IQR, etc.) by groups\nCompute the mean and sd (could use any other summary stats you want, though) for several quantitative variables, by groups.\nExample: find mean and sd of iris flower Petal.Length and Petal.Width by Species and display results in a pretty table. The dataset is called iris.\nMake a little one-row table for each variable being summarized, then stick them together.\n\nlibrary(knitr)\n\nlength_stats &lt;- iris |&gt; \n  df_stats(Petal.Length ~ Species, mean, sd, long_names = FALSE) |&gt;\n  mutate(variable = 'Petal Length')\n\nwidth_stats &lt;- iris |&gt; \n  df_stats(Petal.Width ~ Species, mean, sd, long_names = FALSE) |&gt;\n  mutate(variable = 'Petal Width')\n\nmy_table &lt;- bind_rows(length_stats, width_stats)\n\nkable(my_table)\n\n\n\n\nresponse\nSpecies\nmean\nsd\nvariable\n\n\n\n\nPetal.Length\nsetosa\n1.462\n0.1736640\nPetal Length\n\n\nPetal.Length\nversicolor\n4.260\n0.4699110\nPetal Length\n\n\nPetal.Length\nvirginica\n5.552\n0.5518947\nPetal Length\n\n\nPetal.Width\nsetosa\n0.246\n0.1053856\nPetal Width\n\n\nPetal.Width\nversicolor\n1.326\n0.1977527\nPetal Width\n\n\nPetal.Width\nvirginica\n2.026\n0.2746501\nPetal Width\n\n\n\n\n\nWhat if we want to round all table entries to 2 digits after the decimal?\n\nkable(my_table, digits = 2)\n\n\n\n\nresponse\nSpecies\nmean\nsd\nvariable\n\n\n\n\nPetal.Length\nsetosa\n1.46\n0.17\nPetal Length\n\n\nPetal.Length\nversicolor\n4.26\n0.47\nPetal Length\n\n\nPetal.Length\nvirginica\n5.55\n0.55\nPetal Length\n\n\nPetal.Width\nsetosa\n0.25\n0.11\nPetal Width\n\n\nPetal.Width\nversicolor\n1.33\n0.20\nPetal Width\n\n\nPetal.Width\nvirginica\n2.03\n0.27\nPetal Width\n\n\n\n\n\nWhat if we want the column order to be Variable, Species, mean, sd, and sort by Species and then Variable?\n\nmy_table &lt;- my_table |&gt;\n  dplyr::select(variable, Species, mean, sd) |&gt;\n  arrange(Species, variable)\nkable(my_table, digits = 2)\n\n\n\n\nvariable\nSpecies\nmean\nsd\n\n\n\n\nPetal Length\nsetosa\n1.46\n0.17\n\n\nPetal Width\nsetosa\n0.25\n0.11\n\n\nPetal Length\nversicolor\n4.26\n0.47\n\n\nPetal Width\nversicolor\n1.33\n0.20\n\n\nPetal Length\nvirginica\n5.55\n0.55\n\n\nPetal Width\nvirginica\n2.03\n0.27\n\n\n\n\n\nWhat if we actually want a column for mean length, sd length, etc. and one row per species?\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n✔ readr     2.1.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ mosaic::count() masks dplyr::count()\n✖ purrr::cross()  masks mosaic::cross()\n✖ mosaic::do()    masks dplyr::do()\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ car::recode()   masks dplyr::recode()\n✖ MASS::select()  masks dplyr::select()\n✖ purrr::some()   masks car::some()\n✖ mosaic::stat()  masks ggplot2::stat()\n✖ mosaic::tally() masks dplyr::tally()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmy_table2 &lt;- my_table |&gt;\n  pivot_wider(names_from = variable, \n              values_from = c(\"mean\", \"sd\"),\n              names_sep = ' ')\nkable(my_table2, digits = 2, align = 'c')\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nmean Petal Length\nmean Petal Width\nsd Petal Length\nsd Petal Width\n\n\n\n\nsetosa\n1.46\n0.25\n0.17\n0.11\n\n\nversicolor\n4.26\n1.33\n0.47\n0.20\n\n\nvirginica\n5.55\n2.03\n0.55\n0.27\n\n\n\n\n\n\n\n17.2.2 Proportions in categories by groups\nYou may also want to make a table of proportion observations in each category by groups, potentially for many variables.\nFor just one variable, we can use tally:\n\ntally(~substance | sex, data = HELPrct, format = 'prop') |&gt;\n  kable(caption = 'Proportion using each substance', digits = 2)\n\n\nProportion using each substance\n\n\n\nfemale\nmale\n\n\n\n\nalcohol\n0.34\n0.41\n\n\ncocaine\n0.38\n0.32\n\n\nheroin\n0.28\n0.27\n\n\n\n\n\nFor many variables we can use a loop. For example, we might want to know the proportion homeless and housed and proportion using each substance, both by sex, from the HELPrct dataset. Above we were using the function knitr::kable() to make tables, but we can use pander::pander() too:\n\n# select only variables needed for the table\n# make the first variable the groups one\ncat_data &lt;- HELPrct |&gt; dplyr::select(sex, substance, homeless) \n\nfor (i in c(2:ncol(cat_data))){\ntally(~cat_data[,i] | cat_data[,1], format = 'prop') |&gt; \n    pander::pander(caption = paste('Proportion in each ',\n                                   names(cat_data)[i]))\n  # can rename variables in cat_data if you want better captions\n  }"
  },
  {
    "objectID": "anova-plus.html#figures",
    "href": "anova-plus.html#figures",
    "title": "17  Other Model Selection Approaches",
    "section": "17.3 Figures",
    "text": "17.3 Figures\nWe’ve made a lot of figures in this class, and almost all have been kind of mediocre. To aim for awesome, here are a couple of great references for inspiration, ideas, and best practices:\n\nFundamentals of Data Visualization by Claus Wilke. https://serialmentor.com/dataviz/\nhttps://infogram.com/blog/20-best-data-visualizations-of-2018/\nvisualizingdata.com blog\n\nhttps://www.visualisingdata.com/2019/08/10-significant-visualisation-developments-january-to-june-2019/\nhttps://www.visualisingdata.com/2016/03/little-visualisation-design/"
  },
  {
    "objectID": "gams.html#non-linear-non-monotonic-relationships",
    "href": "gams.html#non-linear-non-monotonic-relationships",
    "title": "18  GAMs: Generalized Additive Models",
    "section": "18.1 Non-linear, non-monotonic relationships",
    "text": "18.1 Non-linear, non-monotonic relationships\nIt’s not true that all interesting predictor-response relationships are linear or monotonic. One example is in some data on bat migration: how does the probability of bats leaving on their migratory journey depend on air humidity?\n\nbats &lt;- read.csv('https://ndownloader.figshare.com/files/9348010')\n\n\n\n\n\n\n\n\n\nAnother dataset (our example for the day) – ozone levels as a function of temperature, day, and humidity:\n\nozone &lt;- read_csv('https://raw.githubusercontent.com/selva86/datasets/master/ozone.csv') |&gt;\n  mutate(Date = lubridate::mdy(paste(Month, Day_of_month, '2018')),\n         Day = lubridate::yday(Date))\ngf_point(ozone_reading ~ Temperature_Sandburg, \n         data=ozone, alpha=0.4, ylab = 'Ozone')\ngf_point(ozone_reading ~ Day, \n         data=ozone, alpha=0.4, ylab = 'Ozone')\ngf_point(ozone_reading ~ Humidity, \n         data=ozone, alpha=0.4, ylab = 'Ozone')"
  },
  {
    "objectID": "gams.html#smooth-terms",
    "href": "gams.html#smooth-terms",
    "title": "18  GAMs: Generalized Additive Models",
    "section": "18.2 Smooth terms",
    "text": "18.2 Smooth terms\nWe can fit a model where the relationship between the response and the predictor is a ``smooth” – no linearity or monotonicity requirement.\n\n18.2.1 Basis functions\n\nA smooth term is constructed as the sum of several parts, or basis functions. Each of these functions has a relatively simple shape, but scaled and added together, they can produce nearly any ``wiggly” shape.\nIncreasing the dimension of the basis (more functions added together) can allow more wiggliness.\nGoal: allow enough wiggliness to fit the data well, without overfitting (smooth goes through every point in the data, or follows ``trends” that are spurious)\n\nWe will fit smooth models to data using the function gam() from the package mgcv. It includes many options for basis functions (types of smooths) - see ?mgcv::gam or [https://rsconnect.calvin.edu:3939/content/28/] for details."
  },
  {
    "objectID": "gams.html#fitting-gams",
    "href": "gams.html#fitting-gams",
    "title": "18  GAMs: Generalized Additive Models",
    "section": "18.3 Fitting GAMs",
    "text": "18.3 Fitting GAMs\nAn excellent resource: https://converged.yt/mgcv-workshop/.\n\n18.3.1 Choosing model formulation\nWhich terms should be modelled as smooth terms? Explore the data!\n\nPros:\nCons:\n\n\n\n18.3.2 Model formula\nLet’s fit a simple GAM for the ozone data as a function of radiation, temperature and wind. Note the s() function for specifying a smooth, which takes as input:\n\na variable name (or more than one, for advanced users)\nk\nbs\n\nHow do we choose? For some exploration, see: [https://rsconnect.calvin.edu:3939/content/28/].\nWe can also fit the model and smooths by different methods and with options:\n\nmethod = 'GCV.Cp'\nmethod = 'REML'\nmethod = 'ML'\nselect = TRUE (or FALSE)\n\n\nlibrary(mgcv)\noz.gam &lt;- gam(ozone_reading ~ s(Day, k = 7, bs = 'cc') +\n                s(Wind_speed, k = 5, bs = 'tp') +\n                s(Temperature_Sandburg, \n                  k = 5, bs = 'tp'),\n              data = ozone,\n              method = 'ML', \n              select = TRUE)\nsummary(oz.gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nozone_reading ~ s(Day, k = 7, bs = \"cc\") + s(Wind_speed, k = 5, \n    bs = \"tp\") + s(Temperature_Sandburg, k = 5, bs = \"tp\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.3740     0.3163   35.96   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                          edf Ref.df     F p-value    \ns(Day)                  3.641      5  4.93 2.1e-05 ***\ns(Wind_speed)           1.180      4  0.77  0.0696 .  \ns(Temperature_Sandburg) 3.283      4 45.87 &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.697   Deviance explained =   71%\n-ML =  604.6  Scale est. = 20.309    n = 203"
  },
  {
    "objectID": "gams.html#model-assessment",
    "href": "gams.html#model-assessment",
    "title": "18  GAMs: Generalized Additive Models",
    "section": "18.4 Model Assessment",
    "text": "18.4 Model Assessment\nIn addition to what you already know (…which all still holds, except linearity expectation!) mgcv has some nice model checking functions built in.\n\npar(mar=c(4,4,2,2))\ngam.check(oz.gam)\n\n\nMethod: ML   Optimizer: outer newton\nfull convergence after 10 iterations.\nGradient range [-0.000846257,0.001371855]\n(score 604.6047 & scale 20.3091).\nHessian positive definite, eigenvalue range [0.0003470333,101.5511].\nModel rank =  14 / 14 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n                          k'  edf k-index p-value   \ns(Day)                  5.00 3.64    0.86    0.01 **\ns(Wind_speed)           4.00 1.18    1.14    0.97   \ns(Temperature_Sandburg) 4.00 3.28    1.10    0.91   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n18.4.1 Concurvity\nLike collinearity and multicollinearity, but for smooths…values of 0 indicate no problem, and 1 a huge problem (total lack of identifiability – same information in multiple predictors).\nOverall, does the model have problems with concurvity?\n\nconcurvity(oz.gam, full=TRUE)\n\n                 para    s(Day) s(Wind_speed) s(Temperature_Sandburg)\nworst    2.191674e-20 0.6717847     0.2733692               0.6505342\nobserved 2.191674e-20 0.1609172     0.1601073               0.6453322\nestimate 2.191674e-20 0.2421204     0.2119838               0.5270197\n\n\nOr alternatively, which specific pairs of terms cause problems?\n\nconcurvity(oz.gam, full=FALSE)\n\n$worst\n                                para       s(Day) s(Wind_speed)\npara                    1.000000e+00 6.612020e-32  1.595828e-29\ns(Day)                  1.243525e-31 1.000000e+00  2.476773e-01\ns(Wind_speed)           2.467911e-29 2.476773e-01  1.000000e+00\ns(Temperature_Sandburg) 2.081406e-20 6.398944e-01  2.054144e-01\n                        s(Temperature_Sandburg)\npara                               2.081412e-20\ns(Day)                             6.398944e-01\ns(Wind_speed)                      2.054144e-01\ns(Temperature_Sandburg)            1.000000e+00\n\n$observed\n                                para       s(Day) s(Wind_speed)\npara                    1.000000e+00 1.710467e-32  1.312129e-30\ns(Day)                  1.243525e-31 1.000000e+00  8.351370e-02\ns(Wind_speed)           2.467911e-29 9.567723e-02  1.000000e+00\ns(Temperature_Sandburg) 2.081406e-20 6.230468e-02  7.925646e-02\n                        s(Temperature_Sandburg)\npara                               7.645727e-23\ns(Day)                             6.353482e-01\ns(Wind_speed)                      8.030772e-02\ns(Temperature_Sandburg)            1.000000e+00\n\n$estimate\n                                para       s(Day) s(Wind_speed)\npara                    1.000000e+00 1.411658e-32  9.541351e-31\ns(Day)                  1.243525e-31 1.000000e+00  1.685925e-01\ns(Wind_speed)           2.467911e-29 7.801012e-02  1.000000e+00\ns(Temperature_Sandburg) 2.081406e-20 1.970558e-01  8.331840e-02\n                        s(Temperature_Sandburg)\npara                               1.703578e-22\ns(Day)                             4.902313e-01\ns(Wind_speed)                      9.916508e-02\ns(Temperature_Sandburg)            1.000000e+00"
  },
  {
    "objectID": "gams.html#model-selection",
    "href": "gams.html#model-selection",
    "title": "18  GAMs: Generalized Additive Models",
    "section": "18.5 Model Selection",
    "text": "18.5 Model Selection\n\n18.5.1 Shrinkage and Penalties\nWith GAMs, in a sense, some model selection is (or can be) done during model fitting - what smooth is best? Or is the relationship a line? A flat line? Using shrinkage basis or including select=TRUE allows for this.\n\n\n18.5.2 P-value selection\nCautions: p-values are approximate! Successfulness of the procedure best when fitting method is: ML (1st choice), REML (2nd choice).\n\nanova(oz.gam)\n\nInterpretation as usual. Note that anova() (not Anova()) works here - especially for GAMs, it does not do sequential tests; and Anova() doesn’t handle GAMs.\n\n\n18.5.3 Information criteria\n\nConditional/Approximate - bias\nFitting method:\n\nREML-based IC scores can be used to compare models with different random effects but not different predictors. (IF select = TRUE and using a shrinkage basis.)\nML-based IC scores can be used to compare models with different fixed effects (regular predictors) and different family, but not different random effects\n\n\n\nlibrary(MuMIn)\noz.ml &lt;- update(oz.gam, method='ML', na.action='na.fail')\nhead(dredge(oz.ml, rank='AIC'),2)\n\nGlobal model call: gam(formula = ozone_reading ~ s(Day, k = 7, bs = \"cc\") + s(Wind_speed, \n    k = 5, bs = \"tp\") + s(Temperature_Sandburg, k = 5, bs = \"tp\"), \n    data = ozone, na.action = \"na.fail\", method = \"ML\", select = TRUE)\n---\nModel selection table \n  (Int) s(Day,7,\"cc\") s(Tmp_Snd,5,\"tp\") s(Wnd_spd,5,\"tp\") df   logLik    AIC\n8 11.37             +                 +                 + 11 -589.011 1201.6\n4 11.37             +                 +                   10 -591.448 1203.0\n  delta weight\n8  0.00  0.666\n4  1.38  0.334\nModels ranked by AIC(x)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "r if (knitr:::is_html_output()) ' '"
  }
]